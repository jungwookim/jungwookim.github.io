<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jungwoo Kim</title>
  <link href="https://github.com/borkdude/quickblog/atom.xml" rel="self"/>
  <link href="https://github.com/borkdude/quickblog"/>
  <updated>2023-04-30T02:14:40+00:00</updated>
  <id>https://github.com/borkdude/quickblog</id>
  <author>
    <name>Quick Blogger</name>
  </author>
  <entry>
    <id>https://github.com/borkdude/quickblog/2023-04-16-gerrit-102.html</id>
    <link href="https://github.com/borkdude/quickblog/2023-04-16-gerrit-102.html"/>
    <title>Github 사용자를 위한 Gerrit (2)</title>
    <updated>2023-04-16T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>제목이 이제 적절치 않을 수 있겠다. 비교보다는 이제 새로운 관점에 대한 내용이 많은 것 같다. 1편에서 다루지 못했던 내용들에 대해서 몇가지 알아보자. <a href='https://trunkbaseddevelopment.com/'>Trunk Based Development</a>라는 개념이 Google에서 개발 방법론으로 많이 택하는 것인데, 이것이 Gerrit의 철학과도 맞지 않나 생각이 든다. Github은 버전 관리를 Git을 통해서 할 수 있게 해주는 어플리케이션일 뿐이라 사용법이 아주 다양한데, Gerrit은 그 사용법을 TBD에 더 적합하게 만들어주는 것이 아닌가라는 생각이 든다.</p><h2>Goal</h2><ul><li>Trunk based development를 지향한다.</li><li>짧은 생명 주기를 가진 CL이 항상 main branch에 머지된다.</li><li>리니어한 커밋 히스토리를 가지면 좋다.</li></ul><p>위 3개를 다 만족하기 위한 방식으로 Gerrit이 주는 장점이 여럿 있다.</p><h2>Topic</h2><p>(설정하기 따라 다르겠지만) 로컬에서 feature 브랜치를 따서 작업을 하더라도 원격 저장소에서는 그것이 브랜치가 되어 main 브랜치에 Pull Request가 되는 개념이 아니다. 로컬에서 feature 브랜치를 <code>git review</code>하게 된다면 해당 feature라는 <code>Topic</code>이 CL에 붙어서 1편에서 설명한 그대로 Commit 자체가 CL이 되어 리뷰 대상이 된다. 즉, 항상 main branch로 commit되는 CL만 있는 것이다. 참 재밌는 것 같다.</p><h2>중요한 점</h2><p>반복되는 점이지만 그만큼 중요한 점인데 CL의 단위는 커밋 단위이고 크기는 작고 생명 주기는 짧다. 그리고 항상 배포 가능한 master에 병합이 된다. 이점은 전체 코드의 안정성과 개발의 생산성을 높여준다.</p><h2>Tip</h2><p>그렇다면 우리는 Commit을 조금 더 신중하게 해야하고 (Commit과 Save는 다르고 Save 개념으로 사용하고 싶다면 Stash를 사용할 것을 권장), 그렇다면 잘 commit하는 것도 중요하다. 그리고 rebase를 잘 활용해서 main branch의 커밋을 땡겨오도록 한다.</p><h3>git add -p</h3><p><code>p tag</code>를 사용한다면 hunk 단위로 commit을 조절할 수 있다. 가끔 vscode에서 한 파일 내의 변경사항을 부분적으로 commit하고 싶을 때 종종 사용했는데 Gerrit을 사용하는 환경에서는 이것들이 더 중요하게 느껴진다.</p><h3>git rebase</h3><p>git rebase가 더 중요해졌다. 사실 Rebase하지 않고 Merge Commit을 만들고, Squash Merge를 하게 된다면 위의 시나리오와 아주 큰 차이점은 없기도 하다. 그렇지만 이는 약간 CL을 바라보는 관점이 Github과 Gerrit이 다르다보니 다르게 문제를 해결한 것 같다. 그래서 rebase를 적절히 잘 해주는게 중요하고, 이 때 여러 commit에 대해서 다루어야하는 일도 있다보니 <code>git rebase -i</code>를 이용하는 경우가 종종 있다.</p><h2>Attention Set</h2><p>리뷰를 하다보면 리뷰어와 리뷰이 중에 해당 CL에 대해서 주의를 기울이고 있는 차례인지 헷갈리는 때가 있지 않은가? 그런 혼란을 피할 수 있는 개념이 Attention Set이다. 마치 Turn제 게임 같은 것이다. 이 CL을 봐야하는 추구의 차례인지 지정되는 룰이 있고 룰을 만들 수도, 그리고 그냥 명시할 수도 있다. 그렇게 되면 불필요하게 리뷰하러 들어가서 이거 반영이 되었나?라고 확인하는 시간을 소모하거나 할 필요가 없다.</p><h2>마무리</h2><p>Github과 Gerrit을 다 아는 것은 아니지만 그래도 초심자의 경험은 중요하니 이정도로 기록하는 것도 의미가 있지 않을까 생각이 든다. 조금씩 알게 되면 더 기록해나가도록 하겠다.</p><h2>Reference</h2><ul><li><a href='https://www.gerritcodereview.com/'>Official Site</a></li><li><a href='https://gerrit-review.googlesource.com/Documentation/'>Documentation</a></li><li><a href='https://google.github.io/eng-practices/'>Google Code Review</a></li><li><a href='https://docs.opendev.org/opendev/git-review/latest/'>git review</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2023-04-08-gerrit-101.html</id>
    <link href="https://github.com/borkdude/quickblog/2023-04-08-gerrit-101.html"/>
    <title>Github 사용자를 위한 Gerrit (1)</title>
    <updated>2023-04-08T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h2>Introduction</h2><p><a href='https://www.gerritcodereview.com/'>Gerrit</a>을 이해하는데 필요한 내용을 Github 사용자들에게 알기 쉽게 설명해보자.</p><h2>Disclaimer</h2><p>Git 전략 - Flow, Branch, Commit 관리 - 는 다 다를 수 있음을 유의할 것.</p><h2>Github Workflow</h2><p><code>Github</code>을 이용하는 흔한 Workflow를 생각해보자. Local Machine에서 새로운 작업 브랜치를 만들고 해당 브랜치에서 작업을 한다. 그리고 의미 단위로 보통 commit을 한다. 그리고나서 push하고 Pull Request를 생성해서 <strong>리뷰를 받고</strong> master branch에 Merge가 된다. 제일 많은 상황에서의 워크플로우이다. 즉, Github에서는 Branch의 변경사항 전체를 ChangeList라고 고려해서 Review를 한다.</p><h3>Github Workflow Examples</h3><p>Branch 생성 및 Checkout<pre><code class="lang-zsh">git branch &lt;branch-name&gt;
git checkout &lt;branch-name&gt;
</code></pre></p><p>Unstage -> Stage Files<pre><code class="lang-zsh">git add .
</code></pre></p><p>Commit<pre><code class="lang-zsh">git commit -m &quot;commit message&quot;
</code></pre></p><p>Remote 저장소로 Push<pre><code class="lang-zsh">git push
</code></pre></p><p>위처럼 작업하면 대부분의 상황에서는 물 흐르듯이 흘러갈 것이다. Conflicts나 rebase/pull 하는 것도 종종 발생하니 이것은 뒷부분에 부록으로 다루어보자.</p><h2>Gerrit Workflow</h2><p><code>Gerrit</code>과 Github에서의 Workflow의 가장 큰 차이는 ChangeList, 즉 Review의 Unit(단위)가 Pull Request가 아니라 Commit이라는 점이다. 즉, Commit 단위로 리뷰를 한다는 것이다! 이렇게 되면 commit 메시지나 commit 단위를 잘 쪼개는게 중요하다. 그러다보니 <a href='https://www.atlassian.com/git/tutorials/rewriting-history#git-commit--amend'><code>git commit --amend</code></a>를 자주 사용하게 된다.</p><p>하나의 브랜치에 리뷰 단위가 Commit 단위로 쪼개지면서 리뷰를 받을 수 있는 것이다. 이는 ChangeList가 작아지게 되고 그에 따른 이득은 <a href='https://google.github.io/eng-practices/'>구글 코드 리뷰</a>에서 많이 나오니 생략한다.</p><h3>Gerrit Workflow Examples</h3><p>Commit 전까지의 플로우는 거의 같다.</p><p><code>amend</code> tag를 더 자주 사용하게 된다.<pre><code class="lang-zsh">git commit --amend
</code></pre></p><p>Review 요청<pre><code class="lang-zsh">git rewiew
</code></pre></p><h2>Outro</h2><p><a href='https://google.github.io/eng-practices/'>구글 코드 리뷰</a> 문서를 보면서 ChangeList에 대한 내용이 아주 상세히 다루어질 때마다 Pull Request를 매번 작게 만드는게 중요하지만 번거로운 일이라고 생각을 했었는데, 더 작은 단위로 할 수 있는 도구들을 구글에서는 사용하고 있다보니 이번에 더 잘 이해하게 된 것 같다.</p><h2>Reference</h2><ul><li><a href='https://www.gerritcodereview.com/'>Official Site</a></li><li><a href='https://gerrit-review.googlesource.com/Documentation/'>Documentation</a></li><li><a href='https://google.github.io/eng-practices/'>Google Code Review</a></li><li><a href='https://docs.opendev.org/opendev/git-review/latest/'>git review</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2023-03-25-stackoverflow_2022_survey.html</id>
    <link href="https://github.com/borkdude/quickblog/2023-03-25-stackoverflow_2022_survey.html"/>
    <title>2022 stackoverflow survey</title>
    <updated>2023-03-25T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>2022년 스택오버플로우 설문 조사 결과</p><p>개발자 프로필부터, 프로그래밍 기술(Language, Database, Tool 등), 커뮤니티 등등 아주 많은 범위에서 나온 결과.</p><p><a href='https://survey.stackoverflow.co/2022/#top-paying-technologies'>Top paying tech</a> Clojure 개발자가 무려 1등! 상위 언어들은 함수형 언어(F#, Scala, Elixir, OCaml)도 보이고 Rust, Go 같은 동시성이나 안정성, 퍼포먼스와 관련되어 강조된 언어도 보인다. 언어는 표현수단일 뿐이지만, 그래도 개발자들한테 흥미로운 자극을 주는 것은 사실.</p><p><a href='https://survey.stackoverflow.co/2022/#programming-scripting-and-markup-languages'>Loved vs Dreaded Programming</a> Rust, Elixir, Clojure. 올해엔 Elixir를 꼭 사용해봐야겠다.</p><p><a href='https://survey.stackoverflow.co/2022/#web-frameworks-and-technologies'>Loved Dreaded Web Frameworks</a> 앞의 내용과 비슷하게 올해엔 Elixir와 함께 Pheonix를 사용해볼 일이 있으면 좋겠다. 그리고 재밌는 건, Clojure는 프레임워크라는게 없기(?) 때문에 데이터가 없는데 뭔가 당연하기도 하고 다행이다 싶다. Frameworks은 Frame에 맞게 Works를 하는 것이기 때문에...</p><p>재미로 읽는 설문조사이긴 하지만, 여기 나온 조사의 결과들과 한국에서의 인기는 상관관계가 없는 것 같다. 한편으로 아쉽기도 하고 이게 현실인 것을 뭐 어쩌하리.</p><p>개인 프로젝트에 사용하거나, 회사에서 스터디를 통해 장점을 알아가는 방법을 찾아야할 듯하다.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2023-03-14-layoffs.html</id>
    <link href="https://github.com/borkdude/quickblog/2023-03-14-layoffs.html"/>
    <title>1년 전에 1700억을 투자 받은 회사가 하루 아침에 망했다</title>
    <updated>2023-03-14T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>보름 전까지 근무 했던 회사가 (거의) 망했다. 재정적으로 망해버렸다. 회사가 어렵지 않다고 말한 것은 1월 초. 근데 대뜸 회사가 어렵다고 말한 것은 2023년 1월 31일. 그리고 2주 후에 희망퇴직을 선언했고, 2월 말에 대거 퇴사자가 나왔다. 이제 나는 회사 직원이 아니므로 건너 들은 이야기이지만, 4월에 추가 정리해고도 있을 것 같다고 한다. 회사는 수백명의 사람들이 동시에 나갔지만 더 나가야하는 상황이다.</p><p>누구나 그리고 어떠한 일이나 벤처, 사업, 프로젝트는 실패할 수 있다. 그치만 그 과정에서 교훈이 있어야하고 회복 가능한 수준이어야한다. 하지만 회사의 상황은 그렇지 못했다. 결과론적인 이야기가 아니라 너무 무리한 일을 근거 없이 진행했다. 이를 막을 수 있었을 것이다. 경영진, 임원, 그리고 실무자들끼리 나오는 위험성이나 대화 내용에 조금만 더 귀를 귀울였다면 어땠을까.</p><p>로또에 당첨되고 되려 인생이 망한 사람들도 많다고 하지 않나. 현재 상황이 딱 그렇다. 큰 돈이 하늘에서 떨어졌고, 투자자들의 압박은 있을테니 숫자 만들기에 급급해서 가짜 숫자만 찍어내고 외형적 성장에 집중했었다.</p><p>재밌게도 이런 회사의 엎어짐은 회사의 주주들보다 내게 더 큰 배움을 줬다. 투자금을 내 돈인 마냥 함부로 쓰지 말 것. 보여주기식 성장을 하지 말 것. 직원을 지나치게 뽑지말 것. 회사를 선택할 때 경영진의 커리어와 윤리성을 무시하지말 것. 마지막으로 다른 사람을 비난하지 말 것.</p><p>나는 결국 떠났다. 내가 남아서 기여할 수 있는 것과 별개로 설레임이 없었다. 아마 회사는 1년이고 3년이고 망하지 않고 버티긴 할 것 같다. 하지만 회사가 풀고자 하는 문제가 뾰족하지 못하다면 이는 식물회사 같은 상태일 수 있으니 빨리 다시 살아나길 희망한다.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2023-02-20-clojure-materials.html</id>
    <link href="https://github.com/borkdude/quickblog/2023-02-20-clojure-materials.html"/>
    <title>Clojure materials</title>
    <updated>2023-02-20T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h2>Videos</h2><ul><li><a href='https://www.youtube.com/watch?v=wASCH_gPnDw'>Inside Clojure</a></li><li><a href='https://www.youtube.com/watch?v=LKtk3HCgTa8&t=41s'>Simple Made Easy</a></li><li><a href='https://ericnormand.me/programmer-profiles/rich-hickey'>Rich Hickey's talks, interviews, and articles</a></li><li><a href='https://lambdaisland.com/blog/2022-04-25-making-lambda-island-free'>Clojure Free Courses from lambda island</a></li><li><a href='https://www.jacekschae.com/'>One of Clojure Course</a></li></ul><h2>Reads</h2><ul><li><a href='https://chreke.com/little-languages.html'>Little Languages</a></li><li><a href='https://objectcomputing.com/resources/publications/sett/march-2009-clojure-functional-programming-for-the-jvm'>Basic Clojure</a></li><li><a href='https://gist.github.com/baumgardner/c147450a5bac9b955d9c'>Clojure is for type b personalities</a></li><li><a href='https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/'>Parse, don't validate</a></li><li><a href='https://guide.clojure.style/'>Clojure Style Guide</a></li><li><a href='https://shadow-cljs.github.io/docs/UsersGuide.html'>Shadow CLJS</a></li><li><a href='https://www.brainonfire.net/files/seqs-and-colls/main.html'>Collections and Sequences in Clojure</a></li></ul><h2>Programming</h2><ul><li><a href='https://www.youtube.com/watch?v=8Ab3ArE8W3s'>"Stop Writing Dead Programs" by Jack Rusher</a></li><li><a href='https://medium.com/@metabase/why-we-picked-clojure-448bf759dc83'>Why we picked clojure from metabase</a></li><li><a href='https://www.youtube.com/watch?v=8pDqJVdNa44'>React.js</a></li><li><a href='https://roadmap.sh/backend'>Backend roadmap</a></li><li><a href='https://ericnormand.me/article/arguments-against-frameworks'>Arguments against frameworks</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-12-12-dynamodb-tips.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-12-12-dynamodb-tips.html"/>
    <title>DynamoDB Tips</title>
    <updated>2022-12-12T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h2>배경</h2><p>회사에서 소셜미디어 개발에 DynamoDB를 사용하기로 했다. 속도와 스케일을 위한 선택이었다. 키 설계에 대한 생각보다는 비즈니스적 + 성능적 판단으로 DynamoDB를 선택했고, 실제로 PK / SK 설계 단계에는 처음부터 같이 하지는 못했었지만, 다른 작업 후에 한 2~3주 늦게 작업을 같이 하게 되면서 설계도 같이 보게 되었다.</p><h2>AWS SA 세션</h2><p>AWS SA 2 + @명이 오셔서 만들고 있는 서비스에 대해서 요구사항을 같이 보고 키 설계를 같이 했다. 재밌었다. 그럼 그 세션을 통해 얻은 기록해두면 좋은 것들을 나열해보자.</p><h2>기억해둘 것</h2><p>편한 언어로 썼음을 유의하자.</p><h3>General</h3><ul><li><code>PK</code>는 무조건 <code>String</code>만 써라.</li><li>날짜는 <code>PK</code>로 가능은 하지만 <code>cardinality</code>가 너무 낮아서 안티 패턴이다.</li><li><code>PK</code>만 있어도 조회 패턴이 가능하더라도 <code>SK</code>에 <code>&quot;&quot;</code>(empty string)이라도 있는게 좋다.</li><li><code>LSI</code>는 쓰지 마라. <code>LSI</code>는 최초 테이블 생성시에만 만들 수 있고 중간에 없애는 것도 불가능하다.</li><li>반면에, <code>GSI</code>는 테이블을 새로 만드는 것과 같아서 중간에 없앨 수도 있다.</li><li><code>GSI</code>는 새로운 Access Pattern을 만들 때 사용한다.</li><li><code>GSI</code>는 <code>Eventually consistency</code>만 제공하고 <code>Async</code>하다. 딱 하나, <code>getItem</code>을 못쓰는데 일반적으로 <code>query</code>만 사용한다.</li><li><code>GSI</code> 구성은 <code>PK/SK</code>는 어차피 항상 들어가고, 다른 <code>attributes</code>는 선택할 수 있다.</li><li><code>PK</code>는 <code>hash</code>값을 이용한 조회이고, <code>O&#40;1&#41;</code>으로 조회한다.</li><li><code>SK</code>는 <code>Range Query</code>에 많이 사용되고, <code>redis</code>에서 <code>Sorted Set</code>과 유사하다.</li><li><code>PK/SK</code>를 잘 설계하는 것은 <code>DynamoDB</code>의 전부나 다름 없다.</li><li>Key-Value Database 설계시 거쳐야하는 4단계는 아래와 같고, 이 과정을 거치지 않으면 안된다.<ol><li>User case from business</li><li>Draw entity relation diagram</li><li>Write down access patterns</li><li>Start thinking key design</li></ol></li><li>Scalable을 고려한 키 설계를 해야한다. 셀럽이 나오는 경우를 대비하자.</li><li>Strongly consistency는 비싸고 굳이? <code>Eventually consistency</code>가 낫다.</li></ul><h3>Data tiering</h3><ul><li>자주 바뀌는 데이터와 자주 바뀌지 않는 데이터를 잘 구분하자. ex. user profile과 user count는 따로 관리하자.</li><li>Access Pattern에 호출 빈도도 하나의 요소이다.</li><li>조회수, 팔로워수 이런 것들은 in-memory workload로 빼서 구성할 수도 있다.</li><li>Data Tier를 잘 나누는 것이 중요하다. (이것은 소프트웨어 개발에 중요한 요소)</li><li>구분자는 #을 써도 되고 다른 것을 써도 된다.</li><li>게시글ID 같은 것은 점점 커져야한다.(sequencial)</li><li>2tiers data 구성 - redis에 좋아요수 같은 것을 구성해놓고 람다를 이용해서 ddb에 업데이트 한다.</li><li>queue를 두는 것도 좋다.</li><li>User case에 따라서 다양한 데이터베이스를 사용해보자.<ul><li>following/follower 등 추천: Neptune</li><li>Ranking: ES</li><li>집계(통계/분석): Redis</li></ul></li><li>(소프트웨어 개발에서) Data를 Tiering하는 개념은 중요하다.<ul><li>흡수</li><li>변형</li><li>표현</li></ul></li><li>redis -> (consumer/puller) -> database. 구조. 즉, consumer 구성을 어케 하느냐만 다른 설계.</li><li>msk -> by topic -> ddb 설계도 있음.</li><li>DAX는 내부적으로 redis랑 똑같은데, instance기반으로 관리해야해서 사실 비추한다.</li></ul><h3>ETC</h3><ul><li>읽기, 쓰기를 tradeoff 해야하는데, 읽기를 잘하는 것에 설계에 집중하자.</li><li>쓰기가 읽기보다 산술적으로 40배 비싸다.</li><li>차라리 여러번 읽자.</li><li><code>ttl</code> attribute는 데이터를 알아서 없애줌.</li><li><code>CAS</code> 신경 안쓴다. 마지막에 쓴 사람이 임자다.</li><li><code>hot key</code>가 발생할 것이고, 그래서 키 디자인이 중요하다.</li><li>vip(예, 트위터의 트럼프)가 있으면 테이블을 분리해야할 수도 있다.</li><li>Stream을 이용해서 vip 트리거 포인트를 만들어둬야할 수도 있다.</li><li>DB Client는 <code>Nosql Workbench</code>정도 쓴다.</li><li>데이터엔지니어링에서 CDC할 때에는, ddb stream써서 cdc하면 된다. kinesis + firehorse로 구성 가능. 구현에 따라 다른데 consumer가 firehose에 던져주는게 좋음.</li></ul><h3>Monitoring</h3><ul><li>모니터링에서 cloudwatch에 <code>contribute insight</code>를 사용하면 좋다. popular item을 찾을 수 있다.</li><li>1주일치 모니터링을 해서 1주일 최고 피크보다 2배 이상 피크칠 것 같으면 대비해야한다.</li></ul><h3>Single Table</h3><ul><li>하나의 큰 테이블이 가진 장점이 많다.</li><li>파티션 키가 많은 것과 비용은 상관 없다.</li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-10-30-feature-flags-release-toggles.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-10-30-feature-flags-release-toggles.html"/>
    <title>Feature Flags - Release toggles</title>
    <updated>2022-10-30T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>Posted on <a href='https://green-labs.github.io/feature-flags-1'>Greenlabs's blog</a>.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-10-11-Debugging_clojure_with_trace_and_portal.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-10-11-Debugging_clojure_with_trace_and_portal.html"/>
    <title>Debugging Clojure with trace and portal</title>
    <updated>2022-10-11T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>Since I started with Debugging clojure, I have used to use <code>prn</code> to see results. I also used <code>#dbg</code> in calva, which looks like a traditional debug mode and good as well. However, I'd like to improve my debugging with REPL driven development and be more productive. I have searched for things and end up with finding nice combinations, <a href='trace'>trace</a> + <a href='portal'>portal</a>.</p><h2>Prerequisite</h2><p>Understanding <a href='tap'>tap></a></p><h2>Trace</h2><p><a href='trace'>trace</a> is written by <a href='https://github.com/hlship'>hlship</a> and a README contains motivations. I find it strong to leave trace codes in production because the debug is traced when the set-up is on. The other point is it let us know where it is traced. If you experienced with <code>tap&gt;</code>, it's hard to trace a bunch of values in a tap.</p><h2>Portal</h2><p>[portal] is written by <a href='https://github.com/djblue'>djblue</a>. It connects to REPL and can navitate data. Obviously, it's related with UI stuff and navigation tool.</p><h3>1. Using prn</h3><p>Easy approach to debug.<pre><code class="lang-clojure">&#40;defn inspector
  &#91;x&#93;
  &#40;prn x&#41;
  x&#41;

&#40;defn calls
  &#91;&#93;
  &#40;inspector &quot;called&quot;&#41;&#41;

&#40;defn calls&gt;
  &#91;&#93;
  &#40;-&gt; {:value 1}
      &#40;update :value inc&#41;
      inspector
      &#40;assoc :after true&#41;&#41;&#41;

&#40;defn calls&gt;&gt;
  &#91;&#93;
  &#40;-&gt;&gt; &#40;range 10&#41;
       &#40;map inc&#41;
       inspector
       &#40;partition 2&#41;&#41;&#41;

&#40;comment
  &#40;calls&#41;
  ;; &quot;called&quot;
  &#40;calls&gt;&#41;
  ;; {:value 2}
  &#40;calls&gt;&gt;&#41;
  ;; &#40;1 2 3 4 5 6 7 8 9 10&#41;
  &#41;
</code></pre></p><h3>2. Using tap></h3><p><code>tap&gt;</code> is always good enough but a little bit hard to label.<pre><code class="lang-clojure">&#40;def debug-atom &#40;atom &#91;&#93;&#41;&#41;
&#40;add-tap &#40;fn &#91;value&#93; &#40;swap! debug-atom #&#40;conj % value&#41;&#41;&#41;&#41;

&#40;comment
  &#40;tap&gt; 1&#41;
  @debug-atom
  ;; &#91;1&#93;

  &#40;tap&gt; {:age  31
         :name &quot;jungwoo&quot;}&#41;
  @debug-atom&#41;
  ;; &#91;1 {:age  31
         :name &quot;jungwoo&quot;}&#93;
</code></pre></p><h3>3. Using trace</h3><p>This sample code is from <a href='https://github.com/hlship/trace/blob/main/test/net/lewisship/trace_test.clj'>here</a>. I've forked this repo and changed just few line to make it easy threading macro.<pre><code class="lang-clojure">&#40;defn calls-trace
  &#91;&#93;
  &#40;trace :msg &quot;called&quot;&#41;&#41;

&#40;defn calls-trace&gt;
  &#91;&#93;
  &#40;-&gt; {:value 1}
      &#40;update :value inc&#41;
      &#40;trace&gt; :label :post-inc&#41;
      &#40;assoc :after true&#41;&#41;&#41;

&#40;defn calls-trace&gt;&gt;
  &#91;&#93;
  &#40;-&gt;&gt; &#40;range 10&#41;
       &#40;map inc&#41;
       &#40;trace&gt;&gt; :label :post-inc&#41;
       &#40;partition 2&#41;&#41;&#41;


&#40;comment
  &#40;calls-trace&#41;
;; no output
  &#40;trace/setup-default&#41;
;; Reload this NS to test the remainder:
  &#40;calls-trace&#41;
;;   {:in     my-only-trace/calls-trace
;;    :line   7
;;    :thread &quot;nREPL-session-90a76836-b2ce-4d07-86db-23db11474218&quot;
;;    :msg    &quot;called&quot;}
  &#40;calls-trace&gt;&#41;
;;   {:in      my-only-trace/calls-trace&gt;
;;    :line    13
;;    :thread  &quot;nREPL-session-90a76836-b2ce-4d07-86db-23db11474218&quot;
;;    :%value% {:value 2}
;;    :label   :post-inc}
  &#40;calls-trace&gt;&gt;&#41;
;;   {:in      my-only-trace/calls-trace&gt;&gt;
;;    :line    20
;;    :thread  &quot;nREPL-session-90a76836-b2ce-4d07-86db-23db11474218&quot;
;;    :%value% &#40;1 2 3 4 5 6 7 8 9 10&#41;
;;    :label   :post-inc}
  &#41;
</code></pre></p><h3>4. Using trace + portal</h3><p>As I mentioned, portal is the UI tool with <code>add-tap</code> so example code is basically same but we need to launch portal like below.<pre><code class="lang-clojure">&#40;comment
  &#40;calls-trace&#41;
  &#40;trace/setup-default&#41;
;; Reload this NS to test the remainder:
  &#40;do &#40;ns dev&#41;
      &#40;def portal &#40;&#40;requiring-resolve 'portal.api/open&#41;
                   {:launcher                     :vs-code
                    :portal.launcher/window-Title &#40;System/getProperty &quot;user.dir&quot;&#41;}&#41;&#41;
      &#40;add-tap &#40;requiring-resolve 'portal.api/submit&#41;&#41;&#41;
  &#40;calls-trace&#41;
  &#40;calls-trace&gt;&#41;
  &#40;calls-trace&gt;&gt;&#41;&#41;
</code></pre></p><p>I was on VS code so the new window is shown up! <img src="../img/portal.png" alt="portal-img" /></p><h2>What should I use?</h2><p>It really depends on cases. I use all of those. This is <a href='https://github.com/jungwookim/debugging-clojure-example'>an whole example code</a>.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-09-04-mad_camp.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-09-04-mad_camp.html"/>
    <title>MAD camp</title>
    <updated>2022-09-04T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>몇일 전 문득, 누가 나한테 경력이 얼마나 되었냐고 물어봤다. 월급 받으면서 엔지니어로 일한지는 만 5년정도 되었더라. 그리고 그 다음 든 생각은 2014년 1월에 진행한 매드캠프가 떠올랐다.</p><p>매드캠프는 <code>Mobile Application Developer CAMP</code>의 약자로 본엔젤스에서 2010년부터 시작된 캠프이다. 2010년 초반, 스마트폰이 보급되기 시작하면서 여름/겨울 방학 때 대학생들 약 10명정도 뽑아서 숙식/사무실을 제공해주고 일주일에 100시간 이상 개발하면서 개발을 - 그 당시에는 보통 안드로이드 개발 - 을 하는 캠프였다.</p><p>내가 7기였나? 2014년 1월에 했다. (현재는 장병규 의장님으로 불리지만 그 당시에는) 장병규 대표님이랑 매일 스탠드업 미팅을 했고 강석흔 이사님도 자주 피드백을 주었다. 일주일에 한번씩 팀을 바꿔가면서 2~3명에서 팀을 짜서 안드로이드 앱 서비스를 만들었다.</p><p>우리 이전의 선배 기수 매드캠프에서 탄생한 틱톡이 있었는데 그 서비스를 결국 SK 플래닛에 매각하면서 그 당시 학생 개발자들이 다들 꽤 많이 벌어서 그 때 학교 선배가 썰 풀어주던 생각도 나기도 한다. <a href='https://www.hankyung.com/it/article/201204029638g'>관련뉴스</a></p><p>매드캠프가 중간에 몇년 쉰 것으로 알지만 현재 매드캠프는 다른 이름의 <a href='https://madcamp.io/'>몰입캠프</a>라는 새로운 브랜딩으로 재탄생했고 더 공개되고 체계적인 캠프로 변경되었다. 재밌게도 domain은 madcamp이긴 하다.</p><p>장병규 대표님의 스타트업에 대한 생각과 후배, 사회로의 환원에 대한 생각은 여전한 것 같다. 2014년 2월 훈련소에서 가장 존경하는 인물 적어내라고 할 때 장병규 대표님 적어내고 그랬는데... 만나뵌지 너무 오래 지났지만 그 당시의 본엔젤스 식구들은 정말 다 따뜻했다.</p><p>직장을 다니면서 여러 회사에 면접을 봤지만 유일하게 면접비를 챙겨줬던 곳이 본엔젤스였고 크래프톤이었다. 성인 이후 봤던 인터뷰에서 가장 기뻤던 결과가 매드캠프였지 않았을까. 그리고 그곳에서 배운 <code>누구나 몰입하면 할 수 있다</code>를 경험한게 현재하는 일에서 중요하지 않나 생각한다.</p><p>가장 재밌게 개발했던 기억 중 하나가 떠올라서 옛날 생각을 조금 해봤다.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-07-11-repl-driven-development.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-07-11-repl-driven-development.html"/>
    <title>Clojure Live Coding</title>
    <updated>2022-07-11T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>Posted on <a href='https://www.youtube.com/watch?v=c3BOdHuqojI'>Greenlabs's youtube</a></p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-05-22-has_joined_for_six_months.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-05-22-has_joined_for_six_months.html"/>
    <title>Clojure 사용 6개월 후기</title>
    <updated>2022-05-22T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>지금 회사로 11월 15일에 옮겼으니 만으로 딱 6개월이 지났다. 시간이 꽤 빨리 흘렀다. 그 사이에 여러가지 많이 배운 것 같다.</p><p>우리 회사는 지금 다이나믹 타입을 가진 Lisp계열의 함수형언어인 <code>Clojure</code>를 사용 중인데 이를 6개월가량 써보면서 드는 생각들을 조금 정리해보도록 하자. 덤으로 사용하고 있는 스택들도 조금 정리해보도록 하자.</p><h1>인상</h1><p>부족하거나 막힘이 없는 언어라고 느꼈다. 비즈니스 로직에 필요한 구현을 해야할 때 코드가 장황해지지 않았다. 간결했고 가독성이 높았다. 적은 코드량으로 깨끗한 코드를 짤 수 있었고 입/출력만 있고 side effects가 없는 함수들로 파이프라인만 잘 구성하면 되었다. 또한 매크로를 사용해서 함수를 만드는 함수나 새로운 함수를 쉽게 정의할 수도 있었다. 없으면 만들면 되었다. 메타프로그래밍이 쉽고 재밌었다.</p><p>처음 약 3개월동안은 타입이 없다는 불편함이 발목을 잡지 않을까 했는데 딱히 없었다. 언어의 설계가 <code>hashmap</code>을 중심으로 하고, nil에 대한 처리를 잘 하고 있어서 보통의 개발자라면 의심하지 않으면 잘 사용할 것 같다.</p><p>클래스에 얽매이지 않다보니 코드 읽기도 수월하고 이만저만 좋은게 많다.</p><h1>Server Tech Stack</h1><p>대충 정리하니 아래와 같다. 다른 3rd 이용 API나 사소한 것들은 제외시켰다.</p><ul><li>RESTful API: <a href='https://github.com/metosin/reitit'>Reitit</a>. Data-driven router일 뿐이다. (파이썬의 장고나 자바의 스프링 같은게 아니다)</li><li>Data-driven schemas: <a href='https://github.com/metosin/malli'>malli</a></li><li>GraphQL API: <a href='https://lacinia.readthedocs.io/en/latest/'>Lacinia</a></li><li>Dataloader: <a href='https://github.com/oliyh/superlifter'>superlifter</a></li><li>Date and Time: <a href='https://github.com/dm3/clojure.java-time'>java-time</a></li><li>HTTP<ul><li><a href='https://github.com/ring-clojure/ring'>ring</a>: HTTP server abstraction</li><li><a href='https://github.com/dakrone/clj-http'>clj-http</a>: Apache HttpComponents client wrapper</li></ul></li><li>Database: <a href='https://github.com/seancorfield/next-jdbc'>next.jdbc</a></li><li>Connection pools: <a href='https://github.com/tomekw/hikari-cp'>hikari-cp</a></li><li>Structural Migrations: <a href='https://github.com/yogthos/migratus'>Migratus</a></li><li>DSL for SQL generation: <a href='https://github.com/jkk/honeysql'>honeysql</a></li><li>Security: <a href='https://github.com/funcool/buddy'>Buddy</a></li><li>Pattern Matching: <a href='https://github.com/clojure/core.match'>core.match</a></li><li>Testing: <a href='https://github.com/lambdaisland/kaocha'>kaocha</a></li><li>Code Analysis and Linter: <a href='https://github.com/borkdude/clj-kondo'>clj-kondo</a></li><li>Error utility: <a href='https://github.com/adambard/failjure'>failjure</a></li><li>Dependency injection(<em>Managed lifecycle of stateful objects</em>): <a href='https://github.com/weavejester/integrant'>integrant</a></li></ul><h1>더 알아나가야할 것들</h1><p>더 알아나가야할 것들을 안다는 것 자체가 이미 잘 알고 있다는 건데... 실제 코드에서는 core.async를 잘 사용하지 않게 되는데 지금과 같은 규모와 서비스 특성상 비동기적 처리를 할 일이 자주 있을까 싶다. 그리고 protocol 관련해서도 코드 작성을 꺼려하게 되는데 (의견이 분분) 이것도 조금 더 살펴보도록 해야겠다.</p><h1>데이터로서 프로그래밍 한다는 것(DSL관련)</h1><p>Terraform을 비롯하여 데이터로서 행동을 정의하고 규약한다는 것은 이미 프로그래밍 세계에서 많이 이루어졌다. Graphql Schema도 이 중 하나일테고. 회사 내부에서 지금 GraphQL을 적극 사용 중인데 DSL로 만들어서 사용할 수 있을 각을 보고 있다. 조금 더 디벨롭할 수 있는 시간을 쓰도록 해야겠다.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-03-13-system_design_interview.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-03-13-system_design_interview.html"/>
    <title>System Design interview 책 내용 중 기록 몇가지</title>
    <updated>2022-03-13T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>책을 읽다보니 정리할게 없다. 한줄한줄 중요한 내용만 다 담겨 있다. 끝.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-02-22-clojure_urania.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-02-22-clojure_urania.html"/>
    <title>Urania Notes</title>
    <updated>2022-02-22T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>소개</h1><p>종종 비즈니스 로직이 DB, Cache, web services 등 여러 소스로부터 받아보고 싶은 원격 데이터에 의존한다. 이를 다루는게 보통 쉬운 일이 아니다. Urania는 비즈니스 로직을 쉽게 관리하게 해주는데 아래 3가지를 효율적으로 다룬다.</p><ul><li>batch multiple requests to the same data source</li><li>request data from multiple data sources concurrently</li><li>cache previous requests</li></ul><h1>유저 가이드</h1><h2>원리</h2><p>모든 프로그램은 성능과 표현력에 대해서 항상 고민이다. 쉽게 표현하면 성능이 떨어지는 경우가 있기 때문. 아래의 예시를 보자.</p><pre><code class="lang-clojure">&#40;require '&#91;clojure.set :refer &#91;intersection&#93;&#93;&#41;

&#40;defn friends-of
  &#91;id&#93;
  ;; ...
  &#41;

&#40;defn count-common
  &#91;a b&#93;
  &#40;count &#40;intersection a b&#41;&#41;&#41;

&#40;defn count-common-friends
  &#91;x y&#93;
  &#40;count-common &#40;friends-of x&#41; &#40;friends-of y&#41;&#41;&#41;

&#40;count-common-friends 1 2&#41;
</code></pre><p><code>&#40;friends-of x&#41;</code>와 <code>&#40;friends-of y&#41;</code>는 서로 독립적이다. 동시에 가져오면 좋겠다. 게다가, x, y가 심지어 같은 사람이면 두번할 필요도 없을 것이다. 최적화 하면 어떻게 될까? caching과 batching을 혼합적으로 생각해야한다.</p><p><code>Urania</code>는 코드를 약간 바꾸고도 <strong>데이터</strong>를 동시에 가지고 올 수 있게 해준다. 대략 아래처럼.<pre><code class="lang-clojure">&#40;require '&#91;urania.core :as u&#93;&#41;

&#40;defn count-common-friends &#91;x y&#93;
  &#40;u/map count-common
         &#40;friends-of x&#41;
         &#40;friends-of y&#41;&#41;&#41;

&#40;u/run! &#40;count-common-friends 1 2&#41;&#41;
</code></pre></p><p>보면 알겠지만 데이터 가져오는 것과 실행을 분리했다. run할 때 <code>urania</code>는 아래와 같은 일을 한다.</p><ul><li>request data from multiple data sources concurrently</li><li>batch multiple requests to the same data source</li><li>cache repeated requests</li></ul><h2>원격에서 데이터 가져오기</h2><p>보통 원격에서 데이터 가져오는 건 비동기적이고(asynchronous) 혹은 이거나 에러가 날 수도 있다. 그래서 <code>urania</code>가 <a href='ttps://github.com/funcool/promesa'>Promesa</a>를 사용한다.</p><pre><code class="lang-clojure">&#40;require '&#91;promesa.core :as prom&#93;&#41;

&#40;defn remote-req &#91;id result&#93;
  &#40;prom/promise
    &#40;fn &#91;resolve reject&#93;
      &#40;let &#91;wait &#40;rand 1000&#41;&#93;
        &#40;println &#40;str &quot;--&gt;&#91; &quot; id &quot; &#93; waiting &quot; wait&#41;&#41;
        &#40;Thread/sleep wait&#41;
        &#40;println &#40;str &quot;&lt;--&#91; &quot; id &quot; &#93; finished, result: &quot; result&#41;&#41;
        &#40;resolve result&#41;&#41;&#41;&#41;&#41;
</code></pre><h2>Remote data sources</h2><p><code>Urania</code>의 <code>DataSource</code> protocol을 구현한 타입인 data sources를 정의해보자.</p><ul><li><strong><code>-identity</code></strong>, which returns an identifier for the resource (used for caching and deduplication).</li><li><strong><code>-fetch</code></strong>, which fetches the result from the remote data source returning a promise.</li></ul><p>대략 예시<pre><code class="lang-clojure">&#40;require '&#91;urania.core :as u&#93;&#41;

&#40;defrecord FriendsOf &#91;id&#93;
  u/DataSource
  &#40;-identity &#91;&#95;&#93; id&#41;
  &#40;-fetch &#91;&#95; &#95;&#93;
    &#40;remote-req id &#40;set &#40;range id&#41;&#41;&#41;&#41;&#41;

&#40;defn friends-of &#91;id&#93;
  &#40;FriendsOf. id&#41;&#41;
</code></pre></p><ul><li><code>&#40;u/run! &#40;friends-of 10&#41;&#41;</code> : return promise</li><li><code>&#40;deref &#40;u/run! &#40;friends-of 10&#41;&#41;&#41;</code>: deref를 이용한 block</li><li><code>&#40;u/run!! &#40;friends-of 10&#41;&#41;</code>: block하는 두번째 방법. clojure에서는 이게 더 나음</li></ul><h3>가져온 데이터 변환하기</h3><p><code>u/map</code>을 이용해서 결과를 변환할 수 있다<pre><code class="lang-clojure">&#40;u/run!!
  &#40;u/map dec &#40;u/map count &#40;friends-of 10&#41;&#41;&#41;&#41;
</code></pre></p><h3>결과들의 의존성</h3><p>fetch 먼저하고 계산하는 로직이 있다고 하자. <code>u/mapcat</code>을 사용하면 된다.</p><pre><code class="lang-clojure">&#40;defrecord ActivityScore &#91;id&#93;
  u/DataSource
  &#40;-identity &#91;&#95;&#93; id&#41;
  &#40;-fetch &#91;&#95; &#95;&#93;
    &#40;remote-req id &#40;inc id&#41;&#41;&#41;&#41;

&#40;defn activity
  &#91;id&#93;
  &#40;ActivityScore. id&#41;&#41;
</code></pre><p>첫번째 친구의 활동성만 찾아보자.<pre><code class="lang-clojure">&#40;defn first-friends-activity
  &#91;id&#93;
  &#40;u/mapcat &#40;fn &#91;friends&#93;
              &#40;activity &#40;first friends&#41;&#41;&#41;
            &#40;friends-of id&#41;&#41;&#41;

&#40;u/run!! &#40;first-friends-activity 10&#41;&#41;
;; --&gt;&#91; 10 &#93; waiting 575.5289747556875
;; &lt;--&#91; 10 &#93; finished, result: #{0 7 1 4 6 3 2 9 5 8}
;; --&gt;&#91; 0 &#93; waiting 63.24540090623976
;; &lt;--&#91; 0 &#93; finished, result: 1
;; =&gt; 1
</code></pre></p><p>모든 친구에 대해서 한다면?<pre><code class="lang-clojure">&#40;defn friends-activity
  &#91;id&#93;
  &#40;u/mapcat &#40;fn &#91;friends&#93;
              &#40;u/collect &#40;map activity friends&#41;&#41;&#41;
            &#40;friends-of id&#41;&#41;&#41;

&#40;u/run!! &#40;friends-activity 5&#41;&#41;
;; --&gt;&#91; 5 &#93; waiting 480.8846764476696
;; &lt;--&#91; 5 &#93; finished, result: #{0 1 4 3 2}
;; --&gt;&#91; 0 &#93; waiting 488.58045819535687
;; --&gt;&#91; 1 &#93; waiting 87.96784013662884
;; --&gt;&#91; 4 &#93; waiting 868.2747930486679
;; &lt;--&#91; 1 &#93; finished, result: 2
;; --&gt;&#91; 3 &#93; waiting 293.59429652774116
;; &lt;--&#91; 3 &#93; finished, result: 4
;; --&gt;&#91; 2 &#93; waiting 280.68098217346835
;; &lt;--&#91; 0 &#93; finished, result: 1
;; &lt;--&#91; 2 &#93; finished, result: 3
;; &lt;--&#91; 4 &#93; finished, result: 5
;; =&gt; &#91;1 2 5 4 3&#93;
</code></pre></p><p>동시에 실행되는 것을 위에서 확인할 수 있다. 게다가 알아서 중복 요청은 하지 않는다.<pre><code class="lang-clojure">&#40;u/run!! &#40;u/collect &#91;&#40;friends-of 1&#41; &#40;friends-of 2&#41; &#40;friends-of 2&#41;&#93;&#41;&#41;
;; --&gt;&#91; 2 &#93; waiting 634.8383950264134
;; --&gt;&#91; 1 &#93; waiting 924.8381446535985
;; &lt;--&#91; 2 &#93; finished, result: #{0 1}
;; &lt;--&#91; 1 &#93; finished, result: #{0}
;; =&gt; &#91;#{0} #{0 1} #{0 1}&#93;
</code></pre></p><p><code>collect</code> + <code>mapcat</code>은 <code>traverse</code>로 대체될 수 있긴 하다.<pre><code class="lang-clojure">&#40;defn friends-activity
  &#91;id&#93;
  &#40;u/traverse activity &#40;friends-of id&#41;&#41;&#41;
</code></pre></p><h3>배치 요청</h3><p>위의 예시들에서 <code>urania</code>가 중복요청들을 잘 없앴는데 여전히 개선 여지가 있다. 위 예시 <code>u/collect</code>에서 동일한 데이터 소스에 대한 요청이 동시에(concurrently) 실행되는 방법을 봤다. 이걸 batch로 만들어보자.</p><pre><code class="lang-clojure">&#40;extend-type ActivityScore
  u/BatchedSource
  &#40;-fetch-multi &#91;score scores &#95;&#93;
    &#40;let &#91;ids &#40;cons &#40;:id score&#41; &#40;map :id scores&#41;&#41;&#93;
      &#40;remote-req ids &#40;zipmap ids &#40;map inc ids&#41;&#41;&#41;&#41;&#41;&#41;
</code></pre><p>여기 보면 한번에 요청했다. n+1(6)번의 요청이 2번으로 줄었다.<pre><code class="lang-clojure">&#40;u/run!! &#40;friends-activity 5&#41;&#41;
;; --&gt;&#91; 5 &#93; waiting 123.11807342157954
;; &lt;--&#91; 5 &#93; finished, result: #{0 1 4 3 2}
;; --&gt;&#91; &#40;0 1 4 3 2&#41; &#93; waiting 97.95578032830765
;; &lt;--&#91; &#40;0 1 4 3 2&#41; &#93; finished, result: {0 1, 1 2, 4 5, 3 4, 2 3}
;; &#91;1 2 5 4 3&#93;
</code></pre></p><h1>응용</h1><p>Cache, Executor, Environment 관련된 건데 공식 문서보고 확인하자.</p><h1>Reference</h1><ul><li><a href='ttp://funcool.github.io/urania/latest/'>Document</a></li><li><a href='ttps://github.com/funcool/urania'>Github</a></li><li><a href='ttps://github.com/funcool/promesa'>Promesa</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-02-15-topology_sort.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-02-15-topology_sort.html"/>
    <title>위상정렬 Topology sort</title>
    <updated>2022-02-15T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>위상정렬에 대해서 알아보자</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-02-09-graphql_in_action.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-02-09-graphql_in_action.html"/>
    <title>GraphQL in Action Notes</title>
    <updated>2022-02-09T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>Part 1. Exploring GraphQL</h1><h2>Chapter 1. Introduction to GraphQL</h2><pre><code>This chapter covers
 Understanding GraphQL and the design concepts behind it
 How GraphQL differs from alternatives like REST APIs
 Understanding the language used by GraphQL clients and services
 Understanding the advantages and disadvantages of GraphQL
</code></pre><p>GraphQL은 Facebook에서 모바일 어플리케이션에서 발생하는 어떤 기술적 이슈들을 풀기 위해서 만들었다. 하지만 기술적 문제 뿐 아니라 커뮤니케이션 문제들 또한 해결하고 있다고 볼 수 있다.</p><h3>1.1 What is GraphQL?</h3><p>GraphQL은 Graph 데이터 구조가 현실 세계의 데이터를 표현하기 가장 좋다고 생각해서 만들어진 말이다. 보통 관계를 생각한다. On the backend, a GraphQL-based stack needs a runtime. That runtime provides a structure for servers to describe the data to be exposed in their APIs. This structure is what we call a schema in the GraphQL world. An API consumer can then use the GraphQL language to construct a text request representing their exact data needs.</p><h4>1.1.1 The big picture</h4><p>While most relational databases directly support SQL, GraphQL is its own thing. GraphQL needs a runtime service. You cannot just start querying databases using the GraphQL query language (at least, not yet). You need to use a service layer that supports GraphQL or implement one yourself.</p><p> GraphQL allows clients to ask for the exact data they need and makes it easier for servers to aggregate data from multiple data storage resources.</p><p>At the core of GraphQL is a strong type system that is used to describe data and organize APIs. This type system gives GraphQL many advantages on both the server and client sides.</p><h4>1.1.2 GraphQL is a specification</h4><h4>1.1.3 GraphQL is a language</h4><pre><code>GraphQL operations
Queries represent READ operations. Mutations represent WRITE-then-READ operations. You can think of mutations as queries that have side effects.
In addition to queries and mutations, GraphQL also supports a third request type called
a subscription, used for real-time data monitoring requests. Subscriptions represent
continuous READ operations. Mutations usually trigger events for subscriptions.
GraphQL subscriptions require the use of a data-transport channel that supports continuous pushing of data. That’s usually done with WebSockets for web applications.
</code></pre><h3>1.2 Why GraphQL?</h3><p>GraphQL provides comprehensive standards and structures to implement API features in maintainable and scalable ways. GraphQL makes it mandatory for data API servers to publish documentation (the schema) about their capabilities. That schema enables client applications to know everything available for them on these servers. The GraphQL standard schema has to be part of every GraphQL API. Clients can ask the service about its schema using the GraphQL language.</p><h4>1.2.1 What about REST APIs?</h4><p>REST APIs도 customizing을 잘하면 GraphQL처럼 쓸 수는 있지만 별로 안 좋음. over-fetching도 많고, endpoints들이 너무 많다. 다만 caching에서는 약간 유리한 면은 있기는 함.</p><h4>1.2.2 The GraphQL way</h4><ul><li>The tyed graph schema</li><li>The declarative language</li><li>The single endpoint and client lauguage</li><li>The simple versioning</li></ul><h3>1.2.3 REST APIs and GraphQL APIs in action</h3><p>Good examples!</p><h3>1.3 GraphQL problems</h3><h4>1.3.1 Security</h4><p>서버가 공격 당하기 쉬움. 이상한 쿼리 막 날리면... 근데 방어장치는 만들 수 있음. 데이터 limit을 걸 수 있음. allow-list로도 관리할 수 있음 (좀 더 광범위한 어뷰징에 방어)</p><h4>1.3.2 Caching and optimizing</h4><p>정확히는 client caching쪽임. 하지만 graph cache를 이용해 global unique id를 기록해 캐시할 수 있다. 그리고 N+1 SQL quaries 문제도 쉽게 언급된다. Dataloader로 해결할 수 있는데 batching과 caching으로 해결한다. join-based로 N+1을 해결할 수도 있는데 이게 ID-based 배칭 전략보다 더 효율적이지만 구현이 까다롭다.</p><h4>1.3.3 Learning curve</h4><p>하나의 언어 배우는 것만큼 많은 학습량이 든다.</p><ul><li>퀴즈1) GraphQL은 데이터베이스에만 요청할 수 있는 질의어이다. (O/X)</li><li>퀴즈2) GraphQL은 캐싱이 불가능하다 (O/X)</li></ul><h2>Chapter 2. Exploring GraphQL APIs</h2><pre><code>This chapter covers
 Using GraphQL’s in-browser IDE to test GraphQL requests
 Exploring the fundamentals of sending GraphQL data requests
 Exploring read and write example operations from the GitHub GraphQL API
 Exploring GraphQL’s introspective features
</code></pre><p>브라우저에서 사용해보면서 GraphQL이 뭔지 알아가보자.</p><h3>2.1 The GraphiQL editor</h3><h3>2.2 The Basics of the GraphQL language</h3><h4>2.2.1 Requests</h4><p>At the core of a GraphQL communication is a <code>request</code> object.</p><ul><li>Request<ul><li>Document<ul><li>Queries</li><li>Mutations</li><li>Subscriptions</li><li>Fragments</li></ul></li><li>Variables</li><li>Meta-information</li></ul></li></ul><p>3가지 타입의 operations가 있음.</p><ul><li>Query: read-only (읽기)</li><li>Mutation: a write followed by a fetch (쓰기 다음에 읽기)</li><li>Subscription: real-time data updates (구독)</li></ul><h4>2.2.2 Fields</h4><p>One of the core elements in the text of a GraphQL operations is the field.</p><ul><li><code>Scalar</code> fields: primitive leaf values - <code>Int</code>, <code>String</code>, <code>Float</code>, and <code>Boolean</code> + customized scalar fields</li><li><code>Object</code></li><li>Object이어도 nested의 마지막 필드(leaf)는 최종적으로 scalar type으로 다 return 되어야한다</li></ul><p>Note) I use the term <em>root field</em> to refer to the first-level fields in a GraphQL.</p><h3>2.3 Examples from the Github API</h3><p><code>requests</code>, <code>documents</code>, <code>operations</code>, and <code>fields</code>를 이제 알게 되었다. Github GraphiQL로 연습해보자.</p><h4>2.3.1 Reading data from Github</h4><pre><code>{
  repository&#40;owner: &quot;facebook&quot;, name: &quot;graphql&quot;&#41; {
    issues&#40;first: 10&#41; {
      nodes {
        Title
        createdAt
        author {
          login
        }
      }
    }
  }
}
</code></pre><h4>2.3.2 Updating data at Github</h4><h4>2.3.3 Introspective queries</h4><p> Introspective queries start with a root field that’s either <b>type or </b>schema, known as meta-fields. There is also another meta-field, __typename, which can be used to retrieve the name of any object type. Fields with names that begin with double underscore characters are reserved for introspection support. NOTE Meta-fields are implicit. They do not appear in the fields list of their types. The __schema field can be used to read information about the API schema, such as what types and directives it supports. We will explore directives in the next chapter.  Let’s ask the GitHub API schema what types it supports. Here is an introspective query to do that.</p><pre><code>{
  &#95;&#95;schema {
    types {
      name
      description
    }
  }
}
</code></pre><ul><li>퀴즈1) Scalar type은 Int, Boolean, Float, String 4개 뿐이다 (O/X)</li><li>퀴즈2) Mutation 후에 따로 Query를 불러주어서 write and read를 해야한다 (O/X)</li></ul><h2>Chapter 3. Customizing and organizing GraphQL operations</h2><pre><code>This chapter covers
 Using arguments to customize what a request field returns
 Customizing response property names with aliases
 Describing runtime executions with directives
 Reducing duplicated text with fragments
 Composing queries and separating data requirement responsibilities
</code></pre><h3>3.1 Customizing fields with arguments</h3><h4>3.1.1 Identifying a single record to return</h4><p>Identifier를 argument로서 잘 보내야한다. Examples of single-record fields are popular. Some GraphQL APIs even have a singlerecord field for every object in the system. This is commonly known in the GraphQL world as a <strong>Node <i>interface</i></strong>: a concept popularized by the Relay framework (which also originated at Facebook). With a Node interface, you can look up any node in the data graph by its unique global system-wide ID. Then, based on what that node is, you can use an inline fragment to specify the properties on that node that you are interested in seeing in the response.</p><h4>3.1.2 Limiting the number of records returned by a list field</h4><p>first, last 등을 사용해서 제한해야한다. By default, the GitHub API orders the repositories in ascending order by date of creation. You can customize that ordering logic with another field argument.</p><h4>3.1.3 Ordering records returned by a list field</h4><p>Example</p><pre><code>query OrgPopularRepos {
  organization&#40;login: &quot;jscomplete&quot;&#41; {
    repositories&#40;first: 10, orderBy: {field: STARGAZERS, direction: DESC}&#41; {
      nodes {
        name
      }
    }
  }
}
</code></pre><h4>3.1.4 Paginating through a list of records</h4><p>When you need to retrieve a page of records, in addition to specifying a limit, you need to specify an offset. In the GitHub API, you can use the field arguments after and before to offset the results returned by the arguments first and last, respectively.  To use these arguments, you need to work with node identifiers, which are different than database record identifiers. The pagination interface that the GitHub API uses is called the Connection interface (which originated from the Relay framework as well). In that interface, every record is identified by a node field (similar to the Node interface) using a cursor field. The cursor is basically the ID field for each node, and it is the field we use with the before and after arguments.  To work with every node’s cursor next to that node’s data, the Connection interface adds a new parent to the node concept called an edge. The edges field represents a list of paginated records.  Here is a query that includes cursor values through the edges field.</p><p>example) metadata 추가 요청</p><pre><code>query OrgReposMetaInfoExample {
  organization&#40;login: &quot;jscomplete&quot;&#41; {
    repositories&#40;
      first: 10
      after: &quot;Y3Vyc29yOnYyOpK5MjAxNy0wMS0yMVQwODo1NTo0My0wODowMM4Ev4A3&quot;
      orderBy: {field: STARGAZERS, direction: DESC}
    &#41; {
      totalCount
      pageInfo {
        hasNextPage
      }
      edges {
        cursor
        node {
          name
        }
      }
    }
  }
}
</code></pre><h4>3.1.5 Searching and filtering</h4><h4>3.1.6 Providing input for mutations</h4><p>The input value in that mutation is also a field argument. It is a required argument. You cannot perform a GitHub mutation operation without an input object. All GitHub API mutations use this single required input field argument that represents an object. To perform a mutation, you pass the various input values as key/value pairs on that input object.<h3>3.2 Renaming fields with aliases</h3> alias example)</p><pre><code>query ProfileInfoWithAlias {
  user&#40;login: &quot;samerbuna&quot;&#41; {
    name
    companyName: company
    bio
  }
}
</code></pre><h3>3.3 Customizing responses with directives</h3><p> A <i>directive</i> in a GraphQL request is a way to provide a GraphQL server with additional information about the execution and type validation behavior of a GraphQL document. It is essentially a more powerful version of field arguments: you can use directives to conditionally include or exclude an entire field. In addition to fields, directives can be used with fragments and top-level operations.  A directive is any string in a GraphQL document that begins with the @ character. Every GraphQL schema has three built-in directives: @include, @skip, and @deprecated. Some schemas have more directives. You can use this introspective query to see the list of directives supported by a schema.</p><h4>3.3.1 Variables and input values</h4><p>A <i>variable</i> is simply any name in the GraphQL document that begins with a $ sign: for example, $login or $showRepositories. The name after the $ sign can be anything. We use variables to make GraphQL operations generically reusable and avoid having to hardcode values and concatenate strings.</p><h4>3.3.2 The @include directive</h4><p><code>fieldName @include&#40;if: $someTest&#41;</code> @skip 반대.</p><h4>3.3.3 The @skip directive</h4><p><code>fieldName @skip&#40;if: $someTest&#41;</code> @include 반대.<h4>3.3.4 The @deprecated directive</h4></p><pre><code>type User {
  emailAddress: String
  email: String @deprecated&#40;reason: &quot;Use 'emailAddress'.&quot;&#41;
}
</code></pre><h3>3.4 GraphQL fragments</h3><h4>3.4.1 Why fragments?</h4><p> In GraphQL, fragments are the composition units of the language. They provide a way to split big GraphQL operations into smaller parts. A fragment in GraphQL is simply a reusable piece of any GraphQL operation.  I like to compare GraphQL fragments to UI components. Fragments, if you will, are the components of a GraphQL operation.  Splitting a big GraphQL document into smaller parts is the main advantage of GraphQL fragments. However, fragments can also be used to avoid duplicating a group of fields in a GraphQL operation. We will explore both benefits, but let’s first understand the syntax for defining and using fragments. <h4>3.4.2 Defining and using fragments</h4> example)</p><pre><code>fragment orgFields on Organization {
  name
  description
  websiteUrl
}
</code></pre><p> The on Organization part of the definition is called the type condition of the fragment. Since a fragment is essentially a selection set, you can only define fragments on object types. You cannot define a fragment on a scalar value. spread example)</p><pre><code>query OrgInfoWithFragment {
  organization&#40;login: &quot;jscomplete&quot;&#41; {
    ...orgFields
  }
}
</code></pre><h4>3.4.3 Fragments and DRY</h4><p>DRY가 Framents를 사용하면 좋아진다. 더 좋은 건 바로 다음 장에 나온다.<h4>3.4.4 Fragments and UI components</h4></p><h4>3.4.5 Inline fragments for Interfaces and unions</h4><p>Inline fragments can be used as a type condition when querying against an interface or a union. The bolded part in the query in listing 3.34 is an inline fragment on the Commit type within the target object interface; so, to understand the value of inline fragments, you first need to understand the concepts of unions and interfaces in GraphQL.  Interfaces and unions are abstract types in GraphQL. An interface defines a list of “shared” fields, and a union defines a list of possible object types. Object types in a GraphQL schema can implement an interface that guarantees that the implementing object type will have the list of fields defined by the implemented interface. Object types defined as unions guarantee that what they return will be one of the possible types of that union.</p><p>질문) Object types은 intercface를 구현한다 질문) inline fragment가 type condition을 표현한다는게 무슨 말인지 잘 모르겠다 -> 아 이제 알겠다. 해당 인터페이스가 있을 때 해당 값을 가져오는 거구나. 그게 동시에 union type이 될 수 있고.</p><p>example query)</p><pre><code>query RepoUnionExampleFull {
  repository&#40;owner: &quot;facebook&quot;, name: &quot;graphql&quot;&#41; {
    issueOrPullRequest&#40;number: 3&#41; {
      ... on PullRequest {
        merged
        mergedAt
      }
      ... on Issue {
        closed
        closedAt
      }
    }
  }
}
</code></pre><ul><li>퀴즈1) inline fragment는 언제 사용하는 것이 좋은지 1가지 이상 알려주시오.</li><li>퀴즈2) directives 중 하나인 @include는 언제 사용하는 것인지 설명하시오.</li></ul><h1>Part 2. Building GraphQL APIs</h1><h2>Chapter 4. Designing a GraphQL schema</h2><pre><code>This chapter covers
 Planning UI features and mapping them to API operations
 Coming up with schema language text based on planned operations
 Mapping API features to sources of data
</code></pre><h3>4.1 Why AZdev?</h3><h3>4.2 The API requirements for AZdev</h3><p>요구사항의 첫번째는 UI를 생각하는 것이다. We’ll use a relational database service to store transactional data and a document database service to store dynamic data.</p><p>하나의 Task는 여러 Approach를 가진다. -> PostgreSQL에 저장 Approach의 다른 요소들인 explanations, warnings, or general notes -> MongoDB에 저장</p><p>생각) schema 설계는 내가 아니라 frontend가 하는 것일까? 혹은 함께 하는 것일까?</p><h4>4.2.1 The core types</h4><p>The main entities in the API I’m envisioning for AZdev are User, Task, and Approach. Models are usually defined with upper camel-case (Pascal case), while fields are defined with lower camel-case (Dromedary case).</p><pre><code>type User {
  id: ID! # serialized as a String
  createdAt: String!
  username: String!
  name: String!
}

type Task {
  id: ID!
  createdAt: String!
  content: String!
}

type Approach {
  id: ID!
  createdAt: String!
  content: String!
}
</code></pre><p>!는 non-nullable을 의미한다. The id and createdAt fields are examples of how GraphQL schema types don’t have to exactly match the column types in your database. GraphQL gives you flexibility for casting one type from the database into a more useful type for the client. Try to spot other examples of this as we progress through the API.</p><h3>4.3 Queries</h3><h4>4.3.1 Listing the lastest Task records</h4><pre><code>query {
  taskMainList {
    id
    content
  }
}
</code></pre><p>Note that I named the root field taskMainList instead of a more natural name like mainTaskList. This is just a style preference, but it has an advantage: by putting the subject of the action (task, in this case) first, all actions on that subject will naturally be grouped alphabetically in file trees and API explorers.</p><pre><code>type Query {
  taskMainList: &#91;Task!&#93;
  # More query root fields
}
</code></pre><pre><code>Root field nullability
However, root fields are special because making them nullable has an important consequence. In GraphQL.js implementations, when an error
is thrown in any field’s resolver, the built in executor resolves that field with null.
When an error is thrown in a resolver for a field that is defined as non-null, the executor propagates the nullability to the field’s parent instead. If that parent field is also
non-null, the executor continues up the tree until it finds a nullable field.
This means if the root taskMainList field were to be made non-null, then when an
error is thrown in its resolver, the nullability will propagate to the Query type &#40;its parent&#41;. So the entire data response for a query asking for this field would be null, even
if the query had other root fields.
This is not ideal. One bad root field should not block the data response of other root
fields. When we start implementing this GraphQL API in the next chapter, we will see
an example.
This is why I made the taskMainList nullable, and it’s why I will make all root fields
nullable. The semantic meaning of this nullability is, in this case, “Something went
wrong in the resolver of this root field, and we’re allowing it so that a response can
still have partial data for other root fields.”
</code></pre><h4>4.3.2 Search and the union/interface types</h4><pre><code>union TaskOrApproach = Task | Approach

type Query {
  # ...
  search&#40;term: String!&#41;: &#91;TaskOrApproach!&#93;
}
</code></pre><h4>4.3.3 Using an interface type (내가 잘 모르던 것)</h4><pre><code>interface SearchResultItem {
  id: ID!
  content: String!
}

type Task implements SearchResultItem {
  # ...
  approachCount: Int!
}

type Approach implements SearchResultItem {
  # ...
  task: Task!
}

type Query {
  # ...
  search&#40;term: String!&#41;: &#91;SearchResultItem!&#93;
}
</code></pre><h4>4.3.4 The page for one Task record</h4><h4>4.3.5 Entity relationships</h4><h4>4.3.6 The Enum Type</h4><h4>4.3.7 List of scalar values</h4><h4>4.3.8 The page of a user's Task records</h4><p>me를 사용해서 중복된 이름으로 인한 혼란을 피할 수 있다.</p><h4>4.3.9 Authentication and authorization</h4><p>query arguments에 담지 말고 request header에 담던지 해서 처리하자.</p><h3>4.4 Mutations</h3><pre><code>mutation {
  userCreate {
    # input for a new user record
  } {
    # Fail/Success response
  }
}

mutation {
  userLogin {
    # input to a new user record
  } {
    # Fail/Success response
  }
}
</code></pre><pre><code>type UserError {
  message: String!
}
type UserPayload {
  errors: &#91;UserError!&#93;!
  user: User
  authToken: String
}

type Mutation {
  userCreate&#40;
    # Mutation Input
  &#41;: UserPayload!
  userLogin&#40;
    # Mutation Input
  &#41;: UserPayload!
  # More mutations
}
</code></pre><h4>4.4.1 Mutation input</h4><p>Input object types are basically a simplified version of output object types. Their fields cannot reference output object types (or interface/union types). They can only use scalar input types or other input object types.</p><h4>4.4.2 Deleting a user record</h4><h4>4.4.3 Creating a Task object</h4><h4>4.4.4 Creating and voting on Approach entries</h4><h3>4.5 Subscriptions</h3><ul><li>퀴즈1) query 이름 중 mainTaskList가 아니라 taskMainList라고 작성한 이유는?</li><li>질문: userLogin은 왜 mutation인가?</li><li>질문: input UserInput { ... } 이런 건 pseudo code인지 아니면 input object type이 있는지?</li><li>UserdEletePayload 예시에 왜 error가 non-nullable인지?</li><li>액션: error interface를 구현해서 이를 implements하는 방식으로 관리하면 좋을 듯</li></ul><h2>Chapter 5. Implementing schema resolvers</h2><pre><code>This chapter covers
 Using Node.js drivers for PostgreSQL and
MongoDB
 Using an interface to communicate with a
GraphQL service
 Making a GraphQL schema executable
 Creating custom object types and handling errors
</code></pre><h3>5.1 Running the development environment</h3><h4>5.2.2 Creating resolver functions</h4><h3>5.3 Communicating over HTTP</h3><h3>5.4 Building a schema using constructor objects</h3><h2>Chapter 6. Working with database models and relations</h2><pre><code>This chapter covers
 Creating object types for database models
 Defining a global context shared among all
resolvers
 Resolving fields from database models and
transforming their names and values
 Resolving one-to-one and one-to-many relations
 Working with database views and join statements
</code></pre><h2>Chapter 7. Optimizing data fetching</h2><pre><code>This chapter covers
 Caching and batching data-fetch operations
 Using the DataLoader library with primary keys and custom IDs
 Using GraphQL’s union type and field arguments
 Reading data from MongoDB
</code></pre><h3>7.1 Caching and Batching</h3><p> Caching — The least we can do is cache the response of any SQL statements issued and then use the cache the next time we need the exact same SQL statement. If we ask the database about user x, do not ask it again about user x; just use the previous response. Doing this in a single API request (from one consumer) is a no-brainer, but you can also use longer-term, multisession caching if you need to optimize things further. However, caching by itself is not enough. We also need to group queries asking for data from the same tables.</p><p> Batching — We can delay asking the database about a certain resource until we figure out the IDs of all the records that need to be resolved. Once these IDs are identified, we can use a single query that takes in a list of IDs and returns the list of records for them. This enables us to issue a SQL statement per table, and doing so will reduce the number of SQL statements required for the simple query in listing 7.2 to just two: one for the azdev.tasks table and one for the azdev.users table.</p><h2>Chapter 8. Implementing mutations</h2><pre><code>This chapter covers
 Implementing GraphQL’s mutation fields
 Authenticating users for mutation and query
operations
 Creating custom, user-friendly error messages
 Using powerful database features to optimize
mutations
</code></pre><h3>8.1 The mutators context object</h3><p>A mutation can contain multiple fields, resulting in the server executing multiple database WRITE/READ operations. However, unlike query fields, which are executed in parallel, mutation fields run in a series, one after the other. If an API consumer sends two mutation fields, the first is guaranteed to finish before the second begins. This is to ensure that a race condition does not happen, but it also complicates the task of something like DataLoader.</p><ul><li>퀴즈1) datatime return값은 datestring보다 timestamp를 권장한다. (책에서)</li><li>퀴즈2) Dataloader의 핵심은 cxxxx와 bxxxx를 사용하는 것이다.</li><li>퀴즈3) user-friendly error messages는 mutation response에 포함될 수 없다.<h1>Reference</h1></li><li><a href='ttps://graphql.org/'>GraphQL</a></li><li><a href='ttps://spec.graphql.org/June2018/'>GraphQL Spec</a></li><li><a href='ttps://www.manning.com/books/graphql-in-action'>GraphQL in Action</a></li><li><a href='ttps://graphql.org/swapi-graphql/'>swapi-graphql</a></li><li><a href='ttps://docs.github.com/en/graphql/overview/explorer'>Github-GraphQL</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-01-20-graphql_1+N.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-01-20-graphql_1+N.html"/>
    <title>GraphQL 기초와 1+N 문제</title>
    <updated>2022-01-20T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>GraphQL 기초</h1><ul><li>기존의 Rest API의 요청은 하나의 endpoint를 통해 얻을 수 있는 스키마는 한정되어있음</li><li><code>GraphQL</code>의 경우에는 <code>over-fetching</code>을 방지하고, <code>under-fetching</code>에 이점이 있음</li><li><code>api</code>를 요청하는 쪽에서 필요한 데이터를 정의하고 조회할 수 있음</li><li><code>DB</code> 뿐만 아니라 저장 환경을 가리지 않고 사용할 수 있음</li><li><code>query</code> - 데이터 조회(read)</li><li><code>mutation</code> - 데이터 조작(create, update, delete)</li><li><code>subscription</code> - websocket 등 소켓 연결해서 변경사항 구독</li><li>필요한 데이터를 위해 쿼리를 여러개 보낼 수도 있음</li><li>alias도 할 수 있음</li><li>필터링 하려면 인자를 넘기면 됨</li><li>필드는 <code>scalar</code>(like primitive type)과 <code>object</code>가 있음</li><li>중복되는 셀렉션 세트가 있을 때 <code>fragment</code> 사용 -> <code>relay</code> 구현에 중요한 개념</li><li><code>union</code> type - 리스트에 여러 타입을 받을 수 있게 함.</li><li><code>fragment</code>를 사용하거나 in-line fragment 로 사용할 수도 있음.</li><li><code>interface</code> - 여러개의 타입을 반환하게 함.</li><li><code>introspection</code> - api 세부사항에 대한 쿼리 작성 가능하게 해줌.</li></ul><p>간단한 GraphQL syntax<pre><code class="lang-graphql">query 쿼리명 {
  필드명 {
     …
  }
}


fragment 이름 on 필드 {
   …
}
</code></pre></p><h1>1+N 문제</h1><p><code>GraphQL</code>은 사실 문서 처음에 <code>1+N</code> 문제를 언급해줘야한다고 생각함. 1+N 문제가 뭔지 생각해보자.</p><pre><code>query 사업자조회 {
    id,
    name,
    사업장 {
        id,
        name
    }
}
</code></pre><p>같은 쿼리를 날린다고 하자. 그럼 보통의 <code>Graphql resolver</code>에서는 어떻게 동작하냐고 하면,<pre><code class="lang-sql">select &#42; from 사업자
</code></pre></p><p>의 결과로 <code>N</code>개의 사업자를 얻었을 때, <code>N</code>개의 사업자의 사업장 정보를 얻기 위해서<pre><code class="lang-sql">select &#42; from 사업장 where 사업장.ID = 앞쿼리의결과.사업장ID
</code></pre></p><p>같은 쿼리가 <code>N</code>번 날아간다. 이상하다.</p><p>만약에 이걸 <code>rest api</code>로 구현했으면 <code>endpoint</code>에서 DB 조회를<pre><code class="lang-sql">select &#42;
from 사업자
join 사업장 ON 사업장.ID = 사업자.사업장ID
</code></pre></p><p>의 결과로 쿼리 <code>1</code>회로 끝날텐데 말이다.</p><p>이건 데이터가 많아지면 심각한 문제인데, 이를 해결하기 위해 <code>batching</code>을 이용하여 <code>javascript</code>로 구현한 <a href='ttps://github.com/graphql/dataloader'><code>Dataloader</code></a>가 있고, <code>clojure 생태계</code>에서는 <a href='ttps://github.com/oliyh/superlifter'><code>superlifter</code></a>가 있는데 이들이 <code>1+N</code>문제를 어떻게 해결하고 있는지 살펴보자.</p><h1>Dataloader</h1><ul><li><code>batching</code>과 <code>caching</code>을 통해 데이터베이스나 웹 서비스 같은 원격 데이터 소스에 단순하고 일반된 API를 제공하기 위해 만들어진 것임</li><li><code>Node.js</code> 서비스용을 위한 <code>javascript</code>로 구현된 간단한 버전임</li><li>특히 <code>graphql-js</code> 서비스 구현에 좋음</li><li>이 라이브러리가 아니라 <strong><code>개념</code></strong>은 <code>nodejs</code>나 <code>javascript</code>에만 고유한 것이 아니라 다른 언어에서도 사용할 수 있는 메커니즘임</li><li>정말 간단하게 설명하면 N회의 쿼리를 1회로 줄여주는데 batchSize를 가지고 batch가 동작할 때까지 데이터를 모았다가 한번에 쿼리한다고 개념만 생각하면 됨</li><li>자세한 구현 <a href='ttps://github.com/graphql/dataloader'>Github</a>에서 보면 됨</li></ul><h1><a href='ttps://github.com/oliyh/superlifter'>Superlifter</a></h1><ul><li><code>Dataloader</code>의 Haskell 구현체인 <a href='https://github.com/facebook/Haxl'>Haxl</a>에 영감을 받아 구현</li><li><a href='https://github.com/oliyh/superlifter#lacinia-usage'><code>lacinia</code>에서 어떻게 사용하는지 살펴보자</a></li><li><code>enqueue</code> - <code>fetcher</code>를 큐에 넣는 과정. <code>queue</code> 이름이 없으면 default 세팅값으로</li><li><code>update-trigger</code> - <code>trigger</code> 내용을 업데이트하는 내용.</li><li><code>{:triggers {:queue-size {:threshold 10}}}</code> - 정해진 queue size</li><li><code>{:triggers {:elastic {:threshold 0}}}</code> - 탄력적으로 정의한 만큼의 queue size를 가지도록 하고 그 이후에는 다시 0으로 돌아감</li><li><code>{:triggers {:interval {:interval 100}}}</code> - 100ms 간격으로 쿼리</li><li>triggers 설정이 위의 예시처럼 된다면 <code>threshold</code>에 맞은 상태가 되면 <code>queue에 데이터</code>의 <code>query</code>를 한번에 함</li><li><code>1:N</code> 관계에서 <code>def-superfetcher</code>를 잘 정의하고 사용하면 됨</li><li>미들웨어에 잘 등록해놓으면 편리하게 사용할 수 있음</li></ul><h1>Reference</h1><ul><li><a href='ttps://graphql.org/'>GraphQL</a></li><li><a href='ttps://github.com/graphql/dataloader'>Dataloader</a></li><li><a href='ttps://github.com/oliyh/superlifter'>Superlifter</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-01-14-git_remote.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-01-14-git_remote.html"/>
    <title>git remote auth 실패시</title>
    <updated>2022-01-14T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>git remote 접근이 token 만료되고 업데이트하고 안되었다. 나는 분명히 auth 설정을 다 했는데도. 문제는 ssh로 기본 통신을 하도록 설정해놨는데 왜인지 remote 설정이 https로 되어있던 것. 그래서 간단하게 origin remote를 삭제하고 다시 만들었다.</p><pre><code class="lang-bash">❯ git remote -v
origin  https://github.com/jungwookim/jungwookim.github.io.git &#40;fetch&#41; # ssh로 설정해뒀기 때문에 https를 삭제하고 재설정 필요
origin  https://github.com/jungwookim/jungwookim.github.io.git &#40;push&#41;
❯ git remote remove origin


❯ git remote add origin git@github.com:jungwookim/jungwookim.github.io.git
❯ git fetch
From github.com:jungwookim/jungwookim.github.io
 &#42; &#91;new branch&#93;      main       -&gt; origin/main
</code></pre><p>git 초보일지</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-01-12-clojure_malli_introduction.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-01-12-clojure_malli_introduction.html"/>
    <title>Clojure Malli</title>
    <updated>2022-01-12T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>동기</h1><p>처음에는 type system을 일부 빌려오고 싶었다. 클로저가 동적 타입의 언어이다 보니 개발하는 사람은 데이터 스키마를 어느정도 머릿 속에 가지고 있고 디버깅도 하다보니 오히려 속도를 낼 수 있는 장점이 있지만 해당 코드를 개발하지 않은 사람이 봤을 때에는 추가 개발이 힘든 점이 있다고 느꼈다.</p><p>그래서 리서치를 하다가 Data-driven development를 알게 되었고 찾다보니 Malli를 썼을 때 장점이 있어 보였다.</p><h1>소개</h1><p>Malli는 Data-driven 개발을 위해 만들어진 clojure library이다. 새로운 validation과 specification을 제시한다. schema definition, validation, error, value and schema transformation, generation and registries 같은 것들을 포함하고 있다.</p><p>간단하게 어떻게 동작하는지 한번 살펴보자.</p><h1>Syntax와 simple validation</h1><p>Vector 방식으로 정의<pre><code class="lang-clojure">&#40;require '&#91;malli.core :as m&#93;&#41;

&#40;def non-empty-string
  &#40;m/schema &#91;:string {:min 1}&#93;&#41;&#41;

&#40;m/schema? non-empty-string&#41;
; =&gt; true

&#40;m/validate non-empty-string &quot;&quot;&#41;
; =&gt; false

&#40;m/validate non-empty-string &quot;kikka&quot;&#41;
; =&gt; true

&#40;m/form non-empty-string&#41;
; =&gt; &#91;:string {:min 1}&#93;
</code></pre></p><p>Map 방식으로 정의<pre><code class="lang-clojure">&#40;def non-empty-string
  &#40;m/from-ast {:type :string
               :properties {:min 1}}&#41;&#41;

&#40;m/schema? non-empty-string&#41;
; =&gt; true

&#40;m/validate non-empty-string &quot;&quot;&#41;
; =&gt; false

&#40;m/validate non-empty-string &quot;kikka&quot;&#41;
; =&gt; true

&#40;m/ast non-empty-string&#41;
; =&gt; {:type :string,
;     :properties {:min 1}}
</code></pre></p><p>Schema AST를 쓰는 방식이 훨씬 빠르다고 한다.</p><h1>간단한 schema definition</h1><pre><code class="lang-clojure">&#40;m/validate &#91;:sequential any?&#93; &#40;list &quot;this&quot; 'is :number 42&#41;&#41;
;; =&gt; true

&#40;m/validate &#91;:vector int?&#93; &#91;1 2 3&#93;&#41;
;; =&gt; true

&#40;m/validate &#91;:vector int?&#93; &#40;list 1 2 3&#41;&#41;
;; =&gt; false

; fixed length vector
&#40;m/validate &#91;:tuple keyword? string? number?&#93; &#91;:bing &quot;bang&quot; 42&#93;&#41;
;; =&gt; true


&#40;m/validate &#91;:repeat {:min 2, :max 4} int?&#93; &#91;1&#93;&#41; ; =&gt; false
&#40;m/validate &#91;:repeat {:min 2, :max 4} int?&#93; &#91;1 2&#93;&#41; ; =&gt; true
&#40;m/validate &#91;:repeat {:min 2, :max 4} int?&#93; &#91;1 2 3 4&#93;&#41; ; =&gt; true ; :max is inclusive
&#40;m/validate &#91;:repeat {:min 2, :max 4} int?&#93; &#91;1 2 3 4 5&#93;&#41; ; =&gt; false


; string schemas
&#40;m/validate string? &quot;kikka&quot;&#41; ; using a predicate

&#40;m/validate :string &quot;kikka&quot;&#41; ; using :string schema
;; =&gt; true

&#40;m/validate &#91;:string {:min 1, :max 4}&#93; &quot;&quot;&#41;
;; =&gt; false

; maybe schemas
&#40;m/validate &#91;:maybe string?&#93; &quot;bingo&quot;&#41;
;; =&gt; true

&#40;m/validate &#91;:maybe string?&#93; nil&#41;
;; =&gt; true

&#40;m/validate &#91;:maybe string?&#93; :bingo&#41;
;; =&gt; false

; fn schemas
&#40;def my-schema
  &#91;:and
   &#91;:map
    &#91;:x int?&#93;
    &#91;:y int?&#93;&#93;
   &#91;:fn &#40;fn &#91;{:keys &#91;x y&#93;}&#93; &#40;&gt; x y&#41;&#41;&#93;&#93;&#41;
   
&#40;m/validate my-schema {:x 1, :y 0}&#41;
; =&gt; true

&#40;m/validate my-schema {:x 1, :y 2}&#41;
; =&gt; false

&#40;def pants-schema
  &#91;:and
   &#91;:map
    &#91;:id int?&#93;
    &#91;:size {:optional true} &#91;:maybe :int&#93;&#93;
    &#91;:size-alphabet {:optional true} &#91;:maybe &#91;:enum &quot;S&quot; &quot;M&quot; &quot;L&quot;&#93;&#93;&#93;&#93;
   &#91;:fn {:error/message &quot;size and size alphabet should be nil-matched&quot;}
    '&#40;fn &#91;{:keys &#91;size size-alphabet&#93;}&#93;
       &#40;or &#40;and &#40;nil? size&#41; &#40;nil? size-alphabet&#41;&#41;
           &#40;and &#40;some? size&#41; &#40;some? size-alphabet&#41;&#41;&#41;&#41;&#93;&#93;&#41;

&#40;m/validate pants-schema {:id   1
                          :size nil
                          :size-alphabet &quot;S&quot;}&#41;
; =&gt; false
</code></pre><h1>함수에서 어떻게 사용하고 있는지</h1><pre><code class="lang-clojure">&#40;defn foo-meta
  &quot;schema via var metadataz&quot;
  {:malli/schema &#91;:=&gt; &#91;:cat :int&#93; :int&#93;}
  &#91;x&#93;
  &#40;inc x&#41;&#41;

&#40;m/=&gt; foo-declare &#91;:=&gt; &#91;:cat :int&#93; :int&#93;&#41;
&#40;defn foo-declare
  &quot;schema via separate declaration&quot;
  &#91;x&#93;
  &#40;inc x&#41;&#41;

&#40;foo-meta 1&#41; ; ok
&#40;foo-meta &quot;1&quot;&#41; ; clj-kondo가 빨간 줄 그어줌

&#40;foo-declare 1&#41; ; ok
&#40;foo-declare 1&#41; ; 역시 clj-kondo가 빨간 줄 그어줌
</code></pre><h1>Errors</h1><pre><code class="lang-clojure">; 바로 알아보기 힘들다
&#40;m/validate pants-schema {:id   1
                          :size nil
                          :size-alphabet &quot;S&quot;}&#41;

&#40;-&gt; pants-schema
    &#40;m/explain {:id 1}&#41;
    &#40;me/humanize&#41;&#41;
</code></pre><h1>Value transformation</h1><p>Default Transformers: <code>string-transformer</code>, <code>json-transformer</code>, <code>strip-extra-keys-transformer</code>, <code>default-value-transformer</code> and <code>key-transformer</code>.</p><pre><code class="lang-clojure">&#40;m/decode int? &quot;42&quot; mt/string-transformer&#41;
; 42

&#40;m/encode int? 42 mt/string-transformer&#41;
; &quot;42&quot;

&#40;m/decode
  Address
  {:id &quot;Lillan&quot;,
   :tags &#91;&quot;coffee&quot; &quot;artesan&quot; &quot;garden&quot;&#93;,
   :address {:street &quot;Ahlmanintie 29&quot;
             :city &quot;Tampere&quot;
             :zip 33100
             :lonlat &#91;61.4858322 23.7854658&#93;}}
  mt/json-transformer&#41;
;{:id &quot;Lillan&quot;,
; :tags #{:coffee :artesan :garden},
; :address {:street &quot;Ahlmanintie 29&quot;
;           :city &quot;Tampere&quot;
;           :zip 33100
;           :lonlat &#91;61.4858322 23.7854658&#93;}}

&#40;m/encode
  Address
  {:id &quot;Lillan&quot;,
   :tags &#91;&quot;coffee&quot; &quot;artesan&quot; &quot;garden&quot;&#93;,
   :address {:street &quot;Ahlmanintie 29&quot;
             :city &quot;Tampere&quot;
             :zip 33100
             :lonlat &#91;61.4858322 23.7854658&#93;}}
  &#40;mt/key-transformer {:encode name}&#41;&#41;
;{&quot;id&quot; &quot;Lillan&quot;,
; &quot;tags&quot; &#91;&quot;coffee&quot; &quot;artesan&quot; &quot;garden&quot;&#93;,
; &quot;address&quot; {&quot;street&quot; &quot;Ahlmanintie 29&quot;
;            &quot;city&quot; &quot;Tampere&quot;
;            &quot;zip&quot; 33100
;            &quot;lonlat&quot; &#91;61.4858322 23.7854658&#93;}}

&#40;def strict-json-transformer
  &#40;mt/transformer
    mt/strip-extra-keys-transformer
    mt/json-transformer&#41;&#41;

&#40;m/decode
  Address
  {:id &quot;Lillan&quot;,
   :EVIL &quot;LYN&quot;
   :tags &#91;&quot;coffee&quot; &quot;artesan&quot; &quot;garden&quot;&#93;,
   :address {:street &quot;Ahlmanintie 29&quot;
             :DARK &quot;ORKO&quot;
             :city &quot;Tampere&quot;
             :zip 33100
             :lonlat &#91;61.4858322 23.7854658&#93;}}
  strict-json-transformer&#41;
;{:id &quot;Lillan&quot;,
; :tags #{:coffee :artesan :garden},
; :address {:street &quot;Ahlmanintie 29&quot;
;           :city &quot;Tampere&quot;
;           :zip 33100
;           :lonlat &#91;61.4858322 23.7854658&#93;}}

; 종합 예제
&#40;m/encode
  &#91;:map {:default {}}
   &#91;:a &#91;int? {:default 1}&#93;&#93;
   &#91;:b &#91;:vector {:default &#91;1 2 3&#93;} int?&#93;&#93;
   &#91;:c &#91;:map {:default {}}
        &#91;:x &#91;int? {:default 42}&#93;&#93;
        &#91;:y int?&#93;&#93;&#93;
   &#91;:d &#91;:map
        &#91;:x &#91;int? {:default 42}&#93;&#93;
        &#91;:y int?&#93;&#93;&#93;
   &#91;:e int?&#93;&#93;
  nil
  &#40;mt/transformer
    mt/default-value-transformer
    mt/string-transformer&#41;&#41;
;{:a &quot;1&quot;
; :b &#91;&quot;1&quot; &quot;2&quot; &quot;3&quot;&#93;
; :c {:x &quot;42&quot;}}
</code></pre><h1>Schema generation</h1><p>Schema 추론<pre><code class="lang-clojure">&#40;require '&#91;malli.provider :as mp&#93;&#41;

&#40;def samples
  &#91;{:id &quot;Lillan&quot;
    :tags #{:artesan :coffee :hotel}
    :address {:street &quot;Ahlmanintie 29&quot;
              :city &quot;Tampere&quot;
              :zip 33100
              :lonlat &#91;61.4858322, 23.7854658&#93;}}
   {:id &quot;Huber&quot;,
    :description &quot;Beefy place&quot;
    :tags #{:beef :wine :beer}
    :address {:street &quot;Aleksis Kiven katu 13&quot;
              :city &quot;Tampere&quot;
              :zip 33200
              :lonlat &#91;61.4963599 23.7604916&#93;}}&#93;&#41;

&#40;mp/provide samples&#41;
;&#91;:map
; &#91;:id string?&#93;
; &#91;:tags &#91;:set keyword?&#93;&#93;
; &#91;:address
;  &#91;:map
;   &#91;:street string?&#93;
;   &#91;:city string?&#93;
;   &#91;:zip number?&#93;
;   &#91;:lonlat &#91;:vector double?&#93;&#93;&#93;&#93;
; &#91;:description {:optional true} string?&#93;&#93;
</code></pre></p><h1>Value generation</h1><pre><code class="lang-clojure">&#40;mg/generate pants-schema {:seed 2}&#41;
; =&gt; {:id -28, :size 8083038, :size-alphabet &quot;M&quot;}
</code></pre><h1>Performance</h1><p>ideomatic clojure보다 훨씬 빠르다고 한다. 직접 테스트는 안하고 결과만 공유한다.<pre><code class="lang-clojure">&#40;require '&#91;criterium.core :as cc&#93;&#41;

&#40;def valid {:x true, :y 1, :z &quot;zorro&quot;}&#41;

;; idomatic clojure 54ns
&#40;let &#91;valid? &#40;fn &#91;{:keys &#91;x y z&#93;}&#93;
               &#40;and &#40;boolean? x&#41;
                    &#40;if y &#40;int? y&#41; true&#41;
                    &#40;string? z&#41;&#41;&#41;&#93;
  &#40;assert &#40;valid? valid&#41;&#41;
  &#40;cc/quick-bench &#40;valid? valid&#41;&#41;&#41;

&#40;require '&#91;malli.core :as m&#93;&#41;

;; malli 39ns
&#40;let &#91;valid? &#40;m/validator
               &#91;:map
                &#91;:x :boolean&#93;
                &#91;:y {:optional true} :int&#93;
                &#91;:z :string&#93;&#93;&#41;&#93;
  &#40;assert &#40;valid? valid&#41;&#41;
  &#40;cc/quick-bench &#40;valid? valid&#41;&#41;&#41;
</code></pre></p><h1>우리 프로젝트에서의 활용 - 데이터베이스 입력/업데이트시 값 validation을 해보자</h1><pre><code class="lang-clojure">&#40;def subsidy-schema
  &#91;:and
   &#91;:map
    &#91;:id int?&#93;
    &#91;:area int?&#93;
    &#91;:area-ineqaulity &#91;:enum &quot;&gt;=&quot; &quot;&gt;&quot; &quot;&lt;&quot; &quot;&lt;=&quot;&#93;&#93;&#93;
   &#91;:fn {:error/message &quot;area and area-inequality는 nil match가 되어야함&quot;}
    &#40;fn &#91;{:keys &#91;area area-ineqaulity&#93;}&#93;
      &#40;or &#40;and &#40;nil? area&#41; &#40;nil? area-ineqaulity&#41;&#41;
          &#40;and &#40;some? area&#41; &#40;some? area-ineqaulity&#41;&#41;&#41;&#41;&#93;&#93;&#41;

&#40;defn insert-subsidy-info
  {:malli/schema &#91;:=&gt; &#91;:cat subsidy-schema&#93; :nil&#93;}
  &#91;subsidy&#93;
  &#40;prn subsidy&#41;&#41;
</code></pre><h1>Spec, Schema and Malli</h1><p><code>Schema</code>라는게 있는데 다 좋은데, serializing이랑 de-serializing할 때 non-trivial하다. <code>Spec</code>도 개쩌는데, <code>runtime transformation</code>을 지원하지 않는게 가장 큰 흠이다. <code>malli</code>라는 건 다이나믹 시스템 개발에서 모든 것을 다 지원하기 위해 등장했다. Spec은 runtime transformation이 없다. 그리고 Spec은 global registry 하나를 통해서 다 spec을 관리하는데 Malli는 그럴 필요는 없다. schema와 registry를 값처럼 프로그래밍을 할 수 있다. null이나 optional한 것들도 더 쉽게 처리할 수 있다. 또한 Spec은 schema를 표현하기 위해 macro를 사용한다. 하지만 malli는 vector와 map을 사용한 pure data 그 자체이다. Data가 곧 system이라고 했을 때 더 쉽게 받아들일 수 있다. Spec, Schema 그리고 Malli는 하나만 써야하는 것은 아니고 같이 사용될 수 있다는 것이 중요하다.</p><h1>Reference</h1><ul><li><a href='https://github.com/metosin/malli'>malli github</a></li><li><a href='https://www.metosin.fi/blog/malli/'>malli blog 1</a></li><li><a href='https://www.karimarttila.fi/clojure/2020/12/07/malli-clojure-library.html'>malli blog 2</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-01-11-swimming_is_fun.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-01-11-swimming_is_fun.html"/>
    <title>12년만의 수영 재시작</title>
    <updated>2022-01-11T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>수영을 다시 시작했다. 무려 12년 만에.</p><p>그동안 물이 차갑고 숨을 잘 못쉰다는 제약 때문에 약간의 두려움이 있었는데 이를 극복도 하고 나중에 서핑할 때 어느정도 도움을 얻기 위해서 수영을 하기로 했다(그럼에도 바다에서 노는 건 좋아했지만). 사실 어떤 운동을 다시 해볼까 하다가, 예전에 라켓볼 대회에 나가서 경기 중에 다친 오른쪽 어깨가 아직도 아프기 때문에 물 속에서 운동하는 관절에 부담이 적은 수영이 그나마 제일 나은 선택이었다. 지금까지는 굉장히 재미있다. 20살 1월에 다녔던 수영장에서의 기억은 지금보다 10kg이나 적게 나가는 마른 체격에 운동이나 삶의 지혜와 지식적인 측면에서 배움이 더 느렸다면 지금은 더 습득력도 빨라졌고 근력도 늘어난 것 같다. 다만 폐기능이 너무 안좋아서 몸을 좀 끌어올릴 필요가 있어보인다.</p><p>지속적으로 꾸준히 수영에 다니는게 가장 큰 목표. 이게 가능한 이유는 또 회사의 배려와 정책이 아닌가 싶다. 사무실 출근을 했으면 생각도 못할 선택지니까 말이다. 회사에도 감사하자.</p><h1>2022-02-09</h1><ul><li>자유형은 롤링만 잘하면 그 안에 호흡, 글라이딩, 킥이 다 들어 있는 것 같다</li><li>평영 발차기를 처음 배웠다 - 10분 동안</li><li>이제 25m 자유형은 쉽게 갈 수 있지만 그래도 숨이 많이 헐떡인다</li><li>팔을 꺽으면 안되고 배운 적도 없지만 힘을 빼려니까 팔이 꺽인다</li><li>배영은 하면 물을 먹고 다리가 너무 힘들다</li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2022-01-03-meaning_of_life.html</id>
    <link href="https://github.com/borkdude/quickblog/2022-01-03-meaning_of_life.html"/>
    <title>새해의 첫 기록</title>
    <updated>2022-01-03T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>내가 독서를 제일 많이 했던 시기는 군대에 있을 때였다. 과거 학교 다닐 땐 학업과 노는 것을 핑계로, 현재 직장을 다닐 땐 업무와 기타 일을 핑계로 독서를 하지 않고 있다.</p><p>일을 하다 보면 많은 양의 지식들이 쏟아진다. 이를 소모하려면 에너지가 많이 든다. 사람들과 생활하다 보면 관계를 유지하기 위한 에너지 역시 꽤 크다.</p><p>사람마다 우선 순위라는게 다르다.</p><p>회사에서는 일 밖에 모를 것처럼 보이는 사람들일지라도 일보다 더 중요한 가치들도 많을 것이라고 생각한다. 이는 비단 일과 비교했을 때 뿐만 아니다. 식상한 질문이지만 물에 빠진 내 아내와 자식이 있으면 누구를 구할 거냐는 질문이나 아빠랑 엄마 중에 누가 더 좋은지 라는 질문을 하는 것만 봐도 안다. 우리는 사실 대답하기 어렵지만 불편한 진실을 몰래 숨기고 살아간다.</p><p>아침에 일어날 때 30분 더 잘 것인지 아니면 바로 일어나서 아침이라도 먹을 것인지부터 고민한다. 새해에는 운동 해야지, 독서 해야지 등등 개개인의 목표를 정한다. 다이어트 중이니까 식단 해야지라는 마음과 동시에 배달앱을 키면서 건강한 배달음식을 시켜 먹으려고 한다.</p><p>내일 수영을 하려고 수영복을 사고 나서 문득 드는 생각이다. 이것은 얼마나 꾸준히 할 수 있을까? 나 자신을 위해서 이번에는 좀 꾸준히 해야겠다. 혼자라도 꾸준히.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-12-18-understanding-clojure-macro.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-12-18-understanding-clojure-macro.html"/>
    <title>Clojure macro 초급 딱지 떼기</title>
    <updated>2021-12-18T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>목표</h1><p>클로저 매크로를 할 일은 별로 없을 것 같고 보고 읽을 줄 아는 정도의 지식을 쌓자</p><h1>알거라 생각하거나 알면 좋은 내용</h1><ul><li>clojure는 lisp 계열의 언어이다.</li><li>클로저의 기본 문법을 알고 있다.</li></ul><h1>사전 지식</h1><p>사전 지식이라기 보다는 이 글 내내 반복해서 나오는 것들에 대해서 미리 요약해둔 것이라고 보면 된다. 이 글을 다 읽고도 아래 나열된 것들을 이해 못했다면 글쓴이의 잘못이다.</p><ul><li><code>symbol</code>: <code>심볼</code>, <code>기호</code>라고 부른다. 값을 가지지 않는 심볼이다. 평가된 심볼인지 평가되지 않는 심볼인지 구분할 수 있다.</li><li><code>'</code>: <code>single quoting</code>이라고 부른다. <strong>평가(evaluation)을 꺼놓는 것(turn off). 평가되지 않은 데이터 구조를 반환한다. 심볼을 그대로 반환하기 위함이다.</strong></li><li>`` <code> ``: </code>syntax quoting<code>이라고 부른다. single quote와 비슷하지만 중요한 2가지 다른 점이 있다. 하나는 namespace를 포함한 심볼을 반환한다.&#40;the fully qualified symbols&#41; 두번째는 </code>~<code>를 사용해서 </code>unquote<code>하게 만들 수 있게 한다. 즉, 평가되지 않게 한 것을 평가를 시켜버린다. 다르게 말하면 </code>syntax quoting` 능력을 없애버린다.</li><li><code>&#126;</code>: <code>syntax quoting</code>에서 설명했지만 <code>quoting</code>된 것을 <code>unquote</code>해버린다. 반복하자면 평가되지 않게 한 것을 평가를 시켜버린다.</li><li><code>&#126;@</code>: <code>unquote slicing</code>이라고 부른다. 망치 모양이랑 비슷하다. <code>seqable data structure</code>를 unwrap한다고 보면 된다. 간단하게 생각해서 괄호를 벗겨내고 그 발가벗은 순서대로 둔다고 생각하자.</li><li><code>&#126;'</code>: 일종의 꼼수인데, <code>syntax quote</code>으로 평가를 미뤘는데 let binding 같은 것 할 때 네임스페이스가 붙어 있는데, 네임스페이스를 없애줘야할 때 <code>'</code>를 통해 평가 되지 않게 다시 하고 <code>&#126;</code>를 통해 다시 평가하여 심볼을 얻을 때 필요하다. 즉, 매크로 안에서 심볼을 만들어써야될 때 필요한데 아니면 <code>symbol#</code>을 직접 사용해도 된다. 사실 <code>symbol#</code>은 사실 auto-gensym'd 심볼이다.</li><li><code>symbol#</code>: auto-gensym'd symbol인데 <code>&#126;'</code>는 지양하고 얘를 쓰도록 하자.</li><li><code>gensym</code>: variable capture에 쓰인다. 고유한 symbol을 만들어준다.</li></ul><h1>예제를 사전 지식을 가다듬자</h1><p>예제는 <a href='ttps://www.braveclojure.com/writing-macros/'>Brave Clojure</a>를 일부 참고했다.</p><p>평가된 <code>+</code> 심볼.<pre><code class="lang-clojure">+
; =&gt; #function&#91;clojure.core/+&#93;
</code></pre></p><p>평가되지 않은 <code>+</code> 심볼.<pre><code class="lang-clojure">'+
; +
&#40;quote +&#41;
; +
</code></pre></p><p>보통 클로저에서 사용하는 (함수 인자 인자) 표현식이다. 괄호 안의 표현이 평가되었다.<pre><code class="lang-clojure">&#40;+ 1 2&#41;
; =&gt; 3
</code></pre></p><p><code>single quote</code>를 사용해보자. 괄호 안의 표현이 평가되지 않았다.<pre><code class="lang-clojure">'&#40;+ 1 2&#41;
; =&gt; &#40;+ 1 2&#41;
</code></pre></p><p><code>syntax quote</code>를 사용해보자. <code>+</code> 함수의 네임스페이스를 포함한 심볼을 반환했다.<pre><code class="lang-clojure">`&#40;+ 1 2&#41;
; =&gt; &#40;clojure.core/+ 1 2&#41;
</code></pre></p><p>조금 어색하지만 <code>syntax quote</code>를 하고도 <code>&#126;</code>를 이용해서 평가를 해보자.<pre><code class="lang-clojure">`&#126;&#40;+ 1 2&#41;
; =&gt; 3
</code></pre></p><p>생각해보자. 우리가 평소에 사용하는 방식이다. 쉽다.<pre><code class="lang-clojure">&#40;+ 1 &#40;inc 1&#41;&#41;
; =&gt; 3
</code></pre></p><p>이거는 어떻게 될까? 위에서 생각한대로 평가를 미룬다고 생각해보자. 괄호 안의 평가들이 다 미루어졌다.<pre><code class="lang-clojure">`&#40;+ 1 &#40;inc 1&#41;&#41;
; =&gt; &#40;clojure.core/+ 1 &#40;clojure.core/inc 1&#41;&#41;
</code></pre></p><p><code>&#40;inc 1&#41;</code>는 평가가 되었으면 한다. 그럼 이렇게 해보면 된다.<pre><code class="lang-clojure">`&#40;+ 1 &#126;&#40;inc 1&#41;&#41;
; =&gt; &#40;clojure.core/+ 1 2&#41;
</code></pre></p><p>이렇게 하면 어떻게 될까?<pre><code class="lang-clojure">`&#40;+ 1 &#40;&#126;inc 1&#41;&#41;
; =&gt; &#40;clojure.core/+ 1 &#40;#function&#91;clojure.core/inc&#93; 1&#41;&#41;
; inc만 평가되어 나왔다.
</code></pre></p><p><code>unquote slicing</code>을 알아보자. 먼저 우리가 알던 것을 보자. 평가를 꺼뒀지만 두번째 괄호 안에서는 평가를 바로 하도록 <code>&#126;</code>를 썼다. list가 잘 반환되었다.<pre><code class="lang-clojure">`&#40;+ &#126;&#40;list 1 2 3&#41;&#41;
; =&gt; &#40;clojure.core/+ &#40;1 2 3&#41;&#41;
</code></pre></p><p>망치로 바꿔보자. 이렇게 하면 unwrap된 상태이다.<pre><code class="lang-clojure">`&#40;+ &#126;@&#40;list 1 2 3&#41;&#41;
; =&gt; &#40;clojure.core/+ 1 2 3&#41;
</code></pre></p><p>gensym은 아래처럼 만든다.<pre><code class="lang-clojure">&#40;gensym foo&#41;
; =&gt; foo7366
</code></pre></p><p>auto-gensym'd는 아래처럼 만든다. unquote 상태에서 #을 붙인다..<pre><code class="lang-clojure">`foo#
; =&gt; foo&#95;&#95;7368&#95;&#95;auto&#95;&#95;
</code></pre></p><p>이것처럼 활용할 수 있다.<pre><code class="lang-clojure">`&#40;let &#91;my-symbol# 3&#93;
    &#40;...&#41;&#41;
</code></pre></p><h1>Reference</h1><ul><li><a href='ttps://green-labs.github.io/the-macro'>그린랩스 기술블로그 매크로</a></li><li><a href='ttps://eunmin.gitbooks.io/clojure-for-beginners/content/9_macros.html'>eunmin-gitbooks-macro</a></li><li><a href='ttps://www.braveclojure.com/writing-macros/'>Writing macros in Brave Clojure </a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-11-26-clojure-polymorphism.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-11-26-clojure-polymorphism.html"/>
    <title>Clojure Polymorphism</title>
    <updated>2021-11-26T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>clojure의 다형성에 대해서 알아보자</p><p>다형성이라고 불러도 될 지 모르겠는데 약간 디자인 패턴 중 하나인 팩토리 패턴과 유사하다고도 볼 수 있겠다.</p><pre><code class="lang-clojure">&#40;defmulti 이름 docString? attr-map? 디스패치함수 &amp; 옵션&#41;

&#40;defmethod 멀티펑션 디스패치값 &amp; fn-tail&#41; ; fn-tail이라고 하면 파라미터와 함수 내용을 말하는 것으로 확인
</code></pre><p>예제들을 여러개 봤는데 처음엔 이해하기 힘들었는데 천천히 이해해보도록 하자.</p><pre><code class="lang-clojure">&#40;ns were-creatures&#41;
➊ &#40;defmulti full-moon-behavior &#40;fn &#91;were-creature&#93; &#40;:were-type were-creature&#41;&#41;&#41; ; multifn 함수 이름과 함수 내용을 정의했다
➋ &#40;defmethod full-moon-behavior :wolf
  &#91;were-creature&#93;
  &#40;str &#40;:name were-creature&#41; &quot; will howl and murder&quot;&#41;&#41; ; multifn이름을 그대로 쓰고 두번째 인자는 파라미터로 받은 후 디스패치 함수의 실행 결과값이 :wolf와 대응되는지 확인하는 인자, 그리고 그게 대응된다면 마지막에 fn-tail이 호출된다
➌ &#40;defmethod full-moon-behavior :simmons
  &#91;were-creature&#93;
  &#40;str &#40;:name were-creature&#41; &quot; will encourage people and sweat to the oldies&quot;&#41;&#41;

&#40;full-moon-behavior {:were-type :wolf
➍                      :name &quot;Rachel from next door&quot;}&#41;
; =&gt; &quot;Rachel from next door will howl and murder&quot;

&#40;full-moon-behavior {:name &quot;Andy the baker&quot;
➎                      :were-type :simmons}&#41;
; =&gt; &quot;Andy the baker will encourage people and sweat to the oldies&quot;
</code></pre><h1>Reference</h1><ul><li><a href='https://www.braveclojure.com/multimethods-records-protocols/'>brave clojure multimethods</a></li><li><a href='https://clojuredocs.org/clojure.core/defmulti'>defmulti</a></li><li><a href='https://clojuredocs.org/clojure.core/defmethod'>defmethod</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-11-26-clojure-bootcamp-retro.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-11-26-clojure-bootcamp-retro.html"/>
    <title>Restrospectives of the clojure advenutre</title>
    <updated>2021-11-26T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>회고</h1><p>Clojure 부트캠프의 <code>회고</code>를 해보자.</p><h2>문화</h2><p>회사의 문화에 감사하자. 회사에서 순수하게 공부만을 위해서 2주의 시간을 부여 받은 적이 있었을까? 나는 이전의 회사들에서도 처음 배우는 언어, 프레임워크들에 대해서 <code>학습에만 집중할 시간</code>을 받은 적은 없었다. 보통은 주요 로직을 건드리지 않는 간단한 기능을 만들면서 언어나 프레임워크 등에 익숙해지는 시간을 가지며 혼자서 낑낑되고는 했기 때문에. 뭐가 더 빠르게 배우고 늦게 배우고 좋고 나쁘고를 떠나서 일단 <code>문제 푸는 것</code>과 <code>새로운 언어를 학습하는 것</code>을 할 수 있는 시간과 지원을 받아서 기분이 좋았다. 앞으로 이런 날은 없겠지...</p><h2>리뷰어</h2><p>리뷰어들이 리뷰를 적당히 빡빡하게 해준 것 같다. 너무 사소하다고 느낀 것들도 있었지만 코드 작성에 굉장히 도움을 많이 준 팁들도 있었고, 새로운 core 함수들 같은 것도 많이 알게 된 것 같다. 확실한 건 모든 리뷰들은 일관성이 있었는데 프로덕션 레벨에서 이 코드가 작성된다고 생각할 때, 어떤 방향으로 코드가 작성되어야하는지를 제시해줬다.</p><h2>빠른 개선</h2><p>그리고 특히 1주차에 언어에 익숙치 잘 몰라서 생기는 사소한 문제들로 인해서 막히는 부분들이 있었는데 좀 쉽게 쉽게 질문할 수 있었으면 좋겠다고 생각해서 의견을 제시(매니저님이 들어주었는데)했는데 이것이 2주차부터 바로 반영되어서 시간이나 <code>심리적 안도감</code>을 많이 가졌던 것 같다. 그 전에는 혼자 있는 느낌을 받았는데 이후에는 함께 하는 느낌이었다.</p><h2>클로저 그 자체</h2><p>솔직히 딱히 단점은 잘 모르겠다. 처음에는 <code>Sequence</code>, <code>Collections</code>가 혼동되고 <code>list</code>나 <code>vector</code> 중에 뭘 써야하나 등등에 대해서 헷갈렸는데 지금도 여전히 헷갈리긴 하지만 사용에는 큰 문제는 없는 것 같다. 클로저스럽게 개발한다는게 뭔지는 모르겠지만 클로저의 기본 of 기본에 대해서는 아주 조금 이해한 것 같다. <strong><code>사실 아직 잘 모르겠지만 재밌는 건 맞다.</code></strong> 그걸로 됐다.</p><h2>그럼에도 아쉬운 점</h2><p>아쉬운 점이 없다면 더 아쉬우니 아쉬운 점도 기록해보자.</p><ul><li>내가 클로저를 잘 이해하고 사용 있는 건지 모를 때가 종종 있다: 말 그대로다. 지금도 잘 하고 있는 건지 모르겠다. 뚜렷한 목표 같은게 없다랄까?</li><li>AOC 부트캠프의 목적을 적절히 상기시키면 좋겠다<ul><li>클로저스럽게 코드짜기</li><li>문제 풀이도 중요하지만 프로덕션 코드처럼 재사용 가능하게 짜보는 것</li></ul></li><li>파일을 한줄씩 읽지 말고 꼭 한번에 다 읽으라고 미리 알려주면 좋겠다. <&ndash; 이거에 잘못빠지면 문제 풀이 방식이 달라져서...</li><li>다른 사람이 기존에 작성했던 코드들도 잘 아카이빙 되어있어서 적어도 푼 문제에 대해서는 다른 사람의 코드도 보면 좋겠다. 잘 짠 코드나.</li></ul><h1>Reference</h1><ul><li><a href='ttps://github.com/jungwookim/aoc-exercise/tree/master/src'>my github repo</a></li><li><a href='ttps://adventofcode.com/2018/day/3'>advent of code</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-11-17-training_clojure_with_aoc.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-11-17-training_clojure_with_aoc.html"/>
    <title>Clojure와 친해지기</title>
    <updated>2021-11-17T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p><code>clojure</code>와 친해져보도록 하자. <code>clojure</code>가 <code>JVM</code> 위에서 동작한다는 성질이나 함수형 언어로서의 특성을 글로 배우기 보다는 기본기를 <a href='ttps://adventofcode.com/2018/day/3'><code>advent of code</code></a>에서 닦아보도록 하자. 전체 코드는 <a href='ttps://github.com/jungwookim/aoc-exercise/tree/master/src'>Github</a>에 다 올려놓았다.</p><h1>Day 1</h1><p>AOC 2018년 Day1으로 시작. 아주 간단한 로직을 다루고 숫자 관련한 조작을 연습해보는 섹션이 아닌가 싶다.</p><h2>파일 읽고 파싱하기</h2><p>text file을 읽어서 문제를 풀다보니 파일 입출력을 해야했다. line by line으로 읽고 싶었지만 문제의 의도와 거리가 있어서 <code>clojure.core/slurp</code>를 사용했다.</p><pre><code class="lang-clojure">&#40;slurp &quot;resources/input.txt&quot;&#41; ; 간단하다
</code></pre><p>아주 간단하다. 읽은 결과는 string이므로 splitting을 잘 해준다.</p><pre><code class="lang-clojure">&#40;clojure.string/split #&quot;\n&quot;&#41; ; delimiter는 줄바꿈으로 했다
</code></pre><p>아래와 같은 방식으로 string을 Integer로 파싱할 수 있다. java 라이브러리를 사용하는 것을 확인하면 된다.<pre><code class="lang-clojure">&#40;Integer/parseInt &quot;+3&quot;&#41;
</code></pre></p><h2>part 1</h2><p>문제는 간단하다. Interger collecitons를 받아서 합을 구하면 된다. <code>apply</code>나 <code>reduce</code>를 이용하면 쉽게 구현할 수 있다.</p><h2>part 2</h2><p>주어진 Integer collections가 무한히 반복된다고 할 때, 같은 누적합이 발견될 때의 누적합을 반환하면 된다.</p><h3>풀이</h3><p>먼저 <code>repeat</code>을 알고 있었고 collections를 <code>repeat</code>하면 collections of collections이기 때문에 <code>flatten</code>을 사용했었다.</p><pre><code class="lang-clojure">&#40;defn solve-part2 &#91;li&#93;
  &#40;let &#91;inf-seq &#40;flatten &#40;repeat li&#41;&#41;&#93;
    &#40;loop &#91;temp-set #{}
           cur-seq inf-seq
           acc 0&#93;
      &#40;if &#40;temp-set &#40;+ acc &#40;first cur-seq&#41;&#41;&#41;
        &#40;+ acc &#40;first cur-seq&#41;&#41;
        &#40;recur &#40;conj temp-set &#40;+ acc &#40;first cur-seq&#41;&#41;&#41;
               &#40;rest cur-seq&#41;
               &#40;+ acc &#40;first cur-seq&#41;&#41;&#41;&#41;&#41;&#41;&#41;
</code></pre><h3>리뷰</h3><ul><li>flatten + repeat -> cycle로 바꾸면 좋다. (lazy한 특성 때문에 둘 다 사용 가능한 점을 인지하자)</li><li>let을 사용하는게 오히려 안좋지 않을까? 라고 생각했었는데 쓰는게 더 나음</li><li>loop-recur가 clojure에서 좀 쓸만한 녀석인 줄 알고 남발했었는데 anti-pattern이라고 한다. 앞으로는 사용을 자제해보자</li><li>문제의 특성상 같은 누적합이 2번 나오면 그게 정답이다. 누적합은 <code>clojure.core/reductions</code>를 사용하면 쉽게 구할 수 있다.</li><li>docString으로 input, output의 형태나 함수 설명을 해주도록 노력하자.</li></ul><h3>reductions를 이용한 더 나은 코드</h3><pre><code class="lang-clojure">&#40;defn solve-part2-advanced &#91;li&#93;
  &#40;let &#91;inf-partial-sum li&#93;
    &#40;loop &#91;temp-set #{}
           cur-seq inf-partial-sum&#93;
      &#40;let &#91;sum &#40;first cur-seq&#41;&#93;
        &#40;if &#40;temp-set sum&#41;
          sum
          &#40;recur &#40;conj temp-set sum&#41;
                 &#40;rest cur-seq&#41;&#41;&#41;&#41;&#41;&#41;&#41;

&#40;-&gt;&gt; &quot;resources/input.txt&quot;
      parse-input
      cycle
      &#40;reductions +&#41; ; reductions의 결과인 partial-sum을 파라미터로 넘겼다.
      solve-part2-advanced&#41;
</code></pre><h1>Day 2</h1><p>어제 삽질한 파일 읽는 것 때문에 시간은 잘 벌었다. <code>string</code>을 잘 다루어보는 섹션인 것 같다. 근데 <code>laySeq</code>의 결과를 <code>contains?</code>와 함께 사용할 수 없고 map 데이터구조를 핸들링하는 것에 익숙치 않아서 시간이 오래 걸린 것 같다. 그리고 원래 string 관련 문제는 좀 귀찮은 면은 있는 것 같긴 하다. AOC 2018년 Day 2를 보자.</p><h2>part 1</h2><p>결과부터 말하자면 <code>frequencies + char-array</code>를 사용하면 아주 쉽게 풀리는 문제였는데 저 2가지를 직접 구현하면서 겪은 각종 삽질들 때문에 시간을 많이 잡아 먹었다. 그리고 아래와 같은 코드를 빨리 구현하지 못해서 엄청 헤맸다. 어떻게 삽질 했는지 같이 살펴보자.</p><pre><code class="lang-typescript">// 조건을 만족할 때만 do sth을 하고 그 조건이 여러개일 때
if &#40;isValid&#40;&#41;&#41; {
    // do sth...
}

if &#40;isValid2&#40;&#41;&#41; {
    // do sth...
}

</code></pre><p>이라는 간단한 걸 구현하고 싶었는데 클로저에서는 이렇게 안된다.<pre><code class="lang-clojure">&#40;when &#40;isValid&#41; &#40;...&#41;
when &#40;isValid2&#41; &#40;...&#41;&#41; ; 난감했다
</code></pre></p><p>사실 expression자체도 지금 생각하면 안좋다는게 보이지만 아직은 적응이 덜 됐는지 위에 처럼 구현하고 싶은 마음이 처음엔 컸다. 여튼, 그래서 처음에 답은 구했는데 아주 아주 돌아갔다.</p><pre><code class="lang-clojure">; 처음 제출한 답의 메인 로직
&#40;defn calc-freq
  &quot;input: &#91;&#91;1 2 3 4&#93; &#91;2 2 0 0&#93; ...&#93;&quot;
  &#91;li&#93;
  &#40;apply &#42; &#40;reduce &#40;fn &#91;&#91;res1 res2&#93; val&#93;
                     &#40;cond
                       &#40;and &#40;some #&#40;== % 3&#41; val&#41; &#40;some #&#40;== % 2&#41; val&#41;&#41; &#91;&#40;inc res1&#41; &#40;inc res2&#41;&#93;
                       &#40;some #&#40;== % 3&#41; val&#41; &#91;res1 &#40;inc res2&#41;&#93;
                       &#40;some #&#40;== % 2&#41; val&#41; &#91;&#40;inc res1&#41; res2&#93;
                       :else &#91;res1 res2&#93;&#41;&#41; &#91;0 0&#93; li&#41;&#41;&#41;
</code></pre><h2>리뷰</h2><ul><li>reduce는 almighty해서 가급적 안쓰는게 좋다. 쓰더라도 뚱뚱하게 쓰면 안된다.</li><li>고차함수인 map, filter 같은 것만 사용해서 다 구현할 수 있다 (대부분): 이 사실을 명심하자</li><li>contains?는 set에서만 쓰고 map, vector, list에서는 다른 방식으로 contains? 유무를 확인하자</li><li><code>frequencies</code>, <code>char-array</code>를 알자</li></ul><h2>리뷰 반영 후 다시 구현해본 코드</h2><p>이게 베스트인지는 모르겠지만 일단 조금 더 가독성이 높아졌다.<pre><code class="lang-clojure">&#40;defn logic-part1 &#91;n str-li&#93;
  &#40;-&gt;&gt; str-li
       &#40;map frequencies&#41;
       &#40;map vals&#41;
       &#40;filter #&#40;some &#40;fn &#91;x&#93; &#40;== x n&#41;&#41; %&#41;&#41;
       count&#41;&#41;

&#40;defn solve-part1 &#91;path&#93;
  &#40;let &#91;res1 &#40;-&gt;&gt; path
                  read-input
                  &#40;logic-part1 2&#41;&#41;
        res2 &#40;-&gt;&gt; path
                  read-input
                  &#40;logic-part1 3&#41;&#41;&#93;
    &#40;&#42; res1 res2&#41;&#41;&#41;
</code></pre></p><h2>part 2</h2><h1>Day 3</h1><p><code>matrix</code>를 다루는 내용이 목적인 것 같다. 2d 리스트나 벡터를 사용하는 것을 다루어보자는 느낌.</p><h2>Intro</h2><p>part 1과 part 2가 거의 한번에 풀릴 수 있을 것처럼 보였다. 전체 색칠한 <code>matrix</code>를 만들어내고 이 결과를 가지고 두번 색칠해졌는지 아니면 해당 id가 한번도 덮어씌어진 적 없는지 확인하면 됐었다.</p><h2>parsing</h2><p>input이 좀 괴상하게 들어와서 이것도 연습하는 건가? 싶었다. <code>#1 1,3: 4x4</code> 이런 식으로 인풋이 들어오는데 이걸 잘 파싱해보도록 하자. 위에서 사용한 <code>clojure.string/split</code>과 <code>subs</code>를 요리조리 사용했다.</p><pre><code class="lang-clojure">&#40;defn read-input &#91;path&#93;
  &#40;-&gt; path
      slurp
      &#40;clojure.string/split #&quot;\n&quot;&#41;&#41;&#41;

&#40;defn parse-id
  &quot;input: \&quot;#44\&quot;, output: 44&quot;
  &#91;string&#93;
  &#40;Integer/parseInt &#40;subs string 1 &#40;count string&#41;&#41;&#41;&#41;

&#40;defn parse-pos
  &quot;input: \&quot;1,3:\&quot;, output: &#40;1 3&#41;&quot;
  &#91;string&#93;
  &#40;-&gt;&gt; &#40;clojure.string/split &#40;subs string 0 &#40;dec &#40;count string&#41;&#41;&#41; #&quot;,&quot;&#41;
       &#40;mapv &#40;fn &#91;x&#93; &#40;Integer/parseInt x&#41;&#41;&#41;&#41;&#41;

&#40;defn parse-size
  &quot;input: \&quot;4x4\&quot;, output: &#40;4 4&#41;&quot;
  &#91;string&#93;
  &#40;-&gt;&gt; &#40;clojure.string/split string #&quot;x&quot;&#41;
       &#40;mapv &#40;fn &#91;x&#93; &#40;Integer/parseInt x&#41;&#41;&#41;&#41;&#41;

&#40;defn prepare-data &#91;path&#93;
  &#40;-&gt;&gt; path
       read-input
       &#40;mapv &#40;fn &#91;x&#93; &#40;clojure.string/split x #&quot; &quot;&#41;&#41;&#41;
       &#40;mapv &#40;fn &#91;&#91;id &#95; pos size&#93;&#93;
               &#40;list &#40;parse-id id&#41; &#40;parse-pos pos&#41; &#40;parse-size size&#41;&#41;&#41;&#41;&#41;&#41;
</code></pre><p>id, position, size를 위와 같은 방식으로 파씽했었다.</p><h2>part 1</h2><p>주요 아이디어는 <code>#99 1,3: 2x2</code>라는 인풋이 있을 때 <code>&#40;1 3 99&#41;</code>, <code>&#40;1 4 99&#41;</code>, <code>&#40;2 3 99&#41;</code>, <code>&#40;2 4 99&#41;</code> 의 collections을 가지고 있으면 x, y index에 원하는 id를 갱신하면 된다고 생각했다. 초기값을 다 -1 로 matrix를 만들어 놓았으므로 id가 양수임을 가정했을 때 갱신하려고 할 때 -1이 아니라면 (갱신된 적 있다면) 0으로 갱신해서 (문제에서 X랑 같음) 중복 칠한게 있는지 확인했다. 그리고 마지막으로 0의 갯수를 세어주면서 마무리하였다.</p><p>코드는 아래와 같다.<pre><code class="lang-clojure">&#40;defn vec2d
  &quot;2d vectors&quot;
  &#91;sx sy f&#93;
  &#40;mapv &#40;fn &#91;x&#93; &#40;mapv &#40;fn &#91;y&#93; &#40;f x y&#41;&#41; &#40;range sx&#41;&#41;&#41; &#40;range sy&#41;&#41;&#41;

&#40;defn matrix
  &quot;init matrix&quot;
  &#91;&#93;
  &#40;vec2d 2000 2000 &#40;constantly -1&#41;&#41;&#41;

&#40;defn update-cell
  &quot;update cell to id at position &#40;x, y&#41; on matrix&quot;
  &#91;matrix id x y&#93;
  &#40;if &#40;neg? &#40;get-in matrix &#91;x y&#93;&#41;&#41;
    &#40;update-in matrix &#91;x y&#93; &#40;constantly id&#41;&#41;
    &#40;update-in matrix &#91;x y&#93; &#40;constantly 0&#41;&#41;&#41;&#41;

&#40;defn gen-modified-vals
  &quot;output: sequence of &#40;target-x target-y id&#41;&quot;
  &#91;id ix iy sx sy&#93;
  &#40;map &#40;fn &#91;x&#93; &#40;map &#40;fn &#91;y&#93; &#40;list &#40;+ iy x&#41; &#40;+ ix y&#41; id&#41;&#41; &#40;range sx&#41;&#41;&#41; &#40;range sy&#41;&#41;&#41;

&#40;defn flatten-vals
  &quot;flatten and partition 3&quot;
  &#91;values&#93;
  &#40;-&gt;&gt; values
       flatten
       &#40;partition 3&#41;&#41;&#41;

&#40;defn logic-part1
  &quot;update each one&quot;
  &#91;data&#93;
  &#40;reduce &#40;fn &#91;acc &#91;x y id&#93;&#93;
            &#40;update-cell acc id x y&#41;&#41;
          &#40;matrix&#41;                                          ; 초기값
          &#40;flatten-vals &#40;map &#40;fn &#91;&#91;id &#91;ix iy&#93; &#91;sx sy&#93;&#93;&#93;
                               &#40;gen-modified-vals id ix iy sx sy&#41;&#41;
                             data&#41;&#41;&#41;&#41;

&#40;defn solve-part1 &#91;path&#93;
  &#40;-&gt;&gt; path
       prepare-data
       logic-part1
       flatten
       &#40;filter #&#40;zero? %&#41;&#41;
       count&#41;&#41;
</code></pre></p><p>get-in, update-in 같은 nested structure에 사용하는 core 함수를 알게 되었다. 이렇게 푸는게 맞나? 그리고 flatten 왠만해서 안쓰려고 했는데 쓰면 편해서 써버렸다.</p><h2>part 2</h2><p>part 1을 풀어서 쉽게 풀었다. 필터만 잘 하면 된다.</p><pre><code class="lang-clojure">&#40;defn get-total-count-by-id &#91;path&#93;
  &#40;-&gt;&gt; path
       prepare-data
       &#40;map &#40;fn &#91;&#91;id &#95; &#91;sx sy&#93;&#93;&#93; &#40;list id &#40;&#42; sx sy&#41;&#41;&#41;&#41;&#41;&#41;

&#40;defn logic-part2
  &#91;path data&#93;
  &#40;let &#91;current-id-count &#40;-&gt;&gt; data
                              logic-part1
                              flatten
                              &#40;filter #&#40;pos? %&#41;&#41;
                              frequencies&#41;

        total-count-by-id &#40;get-total-count-by-id path&#41;&#93;
    &#40;-&gt; &#40;filter &#40;fn &#91;&#91;x y&#93;&#93; &#40;== y &#40;get current-id-count x -1&#41;&#41;&#41; total-count-by-id&#41;
        first
        first&#41;&#41;&#41;

&#40;defn solve-part2 &#91;path&#93;
  &#40;-&gt;&gt; path
       prepare-data
       &#40;logic-part2 path&#41;&#41;&#41;
</code></pre><h2>회고</h2><p>언제나 그렇듯 2차 배열을 다루는 문제들은 약간 까다로운 면은 있다. 자주 풀어야 좀 익숙해지는 느낌. 그리고 immutable하게 짠다는게 문제 풀이에서 꽤 까다롭다고 느껴졌다.</p><h2>리뷰</h2><ul><li>전반적으로 naming을 더 신경쓰면 좋을 것 같다는 의견</li><li>naming을 할 때 함수 이름만 보고 뭐하는 건지 알 수 있도록, 함수 이름이 짓기 어렵다고 느꼈다면? 함수로 꼭 만들지 않아도 되는게 아닐까? 라는 생각도 해보자</li><li>matrix 만들 때 2중 map도 좋지만 2중 for를 쓰면 좀 더 나을 수도</li><li>복잡한 parsing을 할 때는 정규식을 쓰는 것도 방법 (항상 좋은 것은 아니지만 경우에 따라)</li><li><code>vector</code>는 random access가 필요할 때 쓰긴 하지만 <code>list</code>보다는 <code>vector</code>를 주로 사용해도 좋음</li><li>그리고 <code>vector</code>나 <code>list</code>를 사용하더라도 순서에 따른 context가 있는 경우에는 <code>hash-map</code>을 쓰는게 훨씬 좋음</li><li>matrix 초기활 때 사용한 -1, 0 같은 값들은 keyword로 관리하는 것이 좋음</li><li><code>==</code>와 <code>=</code> 차이: <code>==</code>는 타입은 신경 쓰지 않고 값만 비교, <code>=</code>는 타입과 값 모두 비교</li><li>ex. <code>&#40;= 0.2 1/5&#41; ; false &#40;== 0.2 1/5&#41; ; true</code></li><li>관용적으로... <code>-&gt;</code>의 경우 hash-map이나 string에 쓰고 <code>-&gt;&gt;</code> seq를 다룰 때 많이 씀 (생각해보기)</li><li><code>first + first</code> 는 <code>ffirst</code>로</li></ul><h2>리뷰 반영 후 전체적인 코드</h2><p>길이가 길어서 링크로 대체한다. 2중 for 문이 2d 다루는데 꽤나 편리한 것 같아서 유용했고 javascript의 object처럼 hash-map을 주요하게 사용하니 편했다. 이게 ps에서는 혼자 빨리 푸는게 중요해서 신경 안썼는데 부트캠프이니만큼 가독성도 신경 써야겠다.<h2></h2><h1>쉬어가기</h1>3일정도 리뷰해보니 코드를 대충 어떻게 작성해야하는지 약간 감은 왔다. 물론 뒤에도 더 많은 리뷰가 있겠지만 뒷 부분에는 코드는 <a href='ttps://github.com/jungwookim/aoc-exercise/tree/master/src'>github 링크</a>로 대체하고 내용 위주로 살펴보자.<h2></h2><h1>Day 4</h1>AOC 2018 Day 4. datetime을 parsing해서 사용하고 싶지만 그렇게 풀지 않아도 되는 문제. 문제의 요구사항을 잘 읽어보면 <code>00:00&#126;00:59</code> 사이의 데이터만 있는 것을 알 수 있음. input이 까다로워서 데이터 전처리하는데 드는 시간이 더 많았던 것 같다. 앞선 리뷰들을 잘 반영해서 <code>hash-map</code> 적절히 잘 사용했음.</p><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day4.clj'>내가 푼 코드</a></li><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day4_reviewed.clj'>리뷰 받은 코드</a></li></ul><h2>주요 내용</h2><ul><li><code>map + flatten</code> 같은 건 <code>mapcat</code>을 사용해도 좋다.</li><li><code>assoc-in + get-in</code> 대신 <code>update-in</code>이라는 걸 쓰자</li><li><code>map</code>을 부분적으로 바꿀 때는 <code>update</code> 혹은 <code>update-in</code>을 쓰자</li><li><code>hash-map desctructing</code>은 같은 이름을 사용할거라면 <code>{:keys &#91;…&#93;}</code>를 사용해도 좋다.</li><li><code>max-key</code>라는 좋은게 있더라</li></ul><h1>Day 5</h1><p>AOC 2018 Day 5. 대소문자 잘 구분하는 문제. 어렵진 않았음.</p><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day5.clj'>내가 푼 코드</a></li><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day5_reviewed.clj'>리뷰 받은 코드</a></li></ul><p>특별한 내용은 없었던 것 같다.</p><h1>Day 6</h1><p>AOC 2018 Day 6. 문제 접근법 자체가 약간 어려웠던 문제. 실제 코드로 구현하는데도 시간이 많이 걸렸음.</p><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day6.clj'>내가 푼 코드</a></li><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day6_reviewed.clj'>리뷰 받은 코드</a></li></ul><h2>주요 내용</h2><ul><li>return type의 일관성을 가지면 좋다.</li><li><code>group-by</code>라는 것이 있다.</li><li>aggregation할 때 <code>frequencies</code>를 잘 사용해도 좋다.</li></ul><h1>Day 7</h1><p>AOC 2018 Day 7. workers들의 일을 잘 할당하는 문제.</p><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day7.clj'>내가 푼 코드</a></li><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day7_reviewed.clj'>리뷰 받은 코드</a></li></ul><h2>주요 내용</h2><ul><li><code>iterate</code> + <code>drop-while</code>을 잘 쓰면 좋다</li><li>^ 이거 쓸 때 주의해야하는 점들이 몇가지 있는데 일단 <code>LazySeq</code>를 반환하기 때문에 이게 가능한 것을 꼭 인지하고 써야한다. 그렇기 때문에 <code>take</code>, <code>first</code> 등 을 하지 않으면 drop-while의 결과가 무한하기 때문에 <code>first</code>, <code>take</code> 등을 같이 꼭 사용하도록 하자.</li><li>사용법이라기 보다는 이 개념을 인지하고 있자.</li><li>상태라는 걸 잘 관리한다는 측면에서 생각하자</li><li>이 문제는 클로저랑 상관 없이 ps적으로 봐도 좋은 문제인 것 같다</li><li><code>threading-macro</code>는 함수의 조합으로 절차지향적으로 생각할 수 있게 하는 좋은 도구인 것 같다</li><li>그럼에도 불구하고 디버깅에 많은 시간을 썼는데 반성하도록 하자</li></ul><h1>Day 8</h1><p>AOC 2020 Day 1, 4. 간단한 두 문제였다. 생각해보면 첫 날에 풀었다면 조금 헤맸을 수도 있지만 부트캠프 8일차에 풀기엔 쉬웠던 문제.</p><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2018_day8.clj'>내가 푼 코드</a></li></ul><h2>주요 내용</h2><ul><li>Day 7에서 얻은 지식들 덕분에 쉽게 푼 것 같다.</li><li><code>map-indexed</code>는 처음 써봤다.</li><li><code>drop-while</code> 멈추는 조건이 할 때마다 약간 헷갈린다.</li></ul><h2>코드</h2><ul><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2020_day1.clj'>2020-day1</a></li><li><a href='https://github.com/jungwookim/aoc-exercise/blob/master/src/p2020_day4.clj'>2020-day4</a></li></ul><h2>회고</h2><ul><li><code>for</code>를 사용하면 combination처럼 2중 루프를 돌게 되는데 이 때 break-point로 <code>:when</code> 같은 걸 사용할 수 있더라.</li><li>두번째 문제는 파싱만 잘하면 되는 문제였다. 다만 정규식에 익숙하지 않아 항상 불편한 점은 있다.</li><li><code>clojure.spec</code> 이라는 걸 활용해보자</li></ul><h1>Reference</h1><ul><li><a href='ttps://github.com/jungwookim/aoc-exercise/tree/master/src'>my github repo</a></li><li><a href='ttps://clojuredocs.org/clojure.core'>clojure library documents</a></li><li><a href='ttps://adventofcode.com/2018/day/3'>advent of code</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-09-13-lock_and_lock.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-09-13-lock_and_lock.html"/>
    <title>lock &amp; lock</title>
    <updated>2021-09-13T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>개발을 하다보면 Lock에 대해서 생각해볼 일이 많다. 일단 Lock이 뭔지 기본 개념부터 알아보자. 추후에 어디에서 발생하는지 알아보고 각 경우에 대해서 어떻게 처리하고 대응하는지 알아보자.</p><h1>Lock</h1><p>컴퓨터과학에서 <strong>Lock(or mutex - 상호 배제)</strong>이라고 하면 synchronization primitive(동기화의 기본요소)이다. 여러 쓰레드의 실행이 있을 때 자원(resource)에 대한 접근을 제한하는 메커니즘이다. 락은 동시성 제어 정책을 시행하도록 만들어진 디자인이고, 가능한 다양한 방법을 통해 서로 다른 애플리케이션에 대해 여러 고유한 구현이 존재한다.</p><h2>Granularity</h2><p>lock의 단위를 소개 하기 전에 lock에 대한 3가지 컨셉을 먼저 이해하는게 좋다.</p><ul><li>lock overhead: 잠금을 위해 할당된 메모리 공간, 잠금을 초기화하고 파괴하는 시간, 잠금 획득, 해제 시간과 같은 잠금 사용을 위한 추가 리소스를 말한다. 즉, 잠금을 하면서 생기는 overhead를 말한다. 프로그램이 잠금을 많이 사용할 수록 오버헤드가 발생한다.</li><li>lock contention: 한 프로세스나 스레드가 다른 프로세스나 스레드가 이미 획득한 lock을 획득하려고 시도할 때마다 발생. 사용 가능한 잠금이 세분화될수록 잠금 요청할 가능성이 줄어든다. 예를 들어 테이블 전체를 잠그는 것보다는 row를 잠그는게 낫다.</li><li>deadlock: 최소 2개의 태스크가 다른 태스크가 사용하고 있는 잠금을 기다리고 있는 상태. 서로 물고 물리는 교착상태를 의미한다. 데드락은 아래에서 더 자세히 알아보자.</li></ul><h1>Database locks</h1><p>먼저 데이터베이스에서의 트랜잭션에 대해서 먼저 알아보자. </p><p>데이터베이스에서의 락은 <code>Transactions</code>의 동기성을 보장한다. 즉, tx 프로세싱이 동시에 만들어질 때, <a href='ttps://en.wikipedia.org/wiki/Two-phase_locking'>two-phase locking</a>을 이용해 동시적 실행이 순차적 실행처럼 실행되게 도와준다. </p><pre><code>two-phase locking

동시성 제어와 관련된 내용이다. 데이터베이스와 트랜잭션 프로세싱에서, two-phase locking&#40;2PL&#41;은 연속성을 보장하는 동시성 제어 방식이다. 이건 다른 이름으로는 데이터베이스 트랜잭션 스케줄이라고도 불립니다. Locks에 관련된 프로토콜은 locks을 이용하여, 한 트랜잭션의 라이프 사이클동안 다른 트랜잭션이 같은 데이터를 접근 하는 것을 막습니다. 2PL 프로토콜에 의해 락은 2가지 단계로 적용 및 제거됩니다.

Expanding phase: locks are acquired and no locks are released.
Shrinking phase: locks are released and no locks are acquired.

기본 프로토콜에서는 공유 및 배타적 잠금의 두 가지 유형의 잠금을 사용하는데 기본 프로토콜을 개선하면 더 많은 잠금 유형을 사용할 수 있다. 프로세스를 잠ㄱ느ㅡㄴ 잠김 방식을 사용하면, 2PL은 프로세스를 차단하는 잠금을 사용하여 둘 이상의 트랜잭션을 상호 차단하여 교착 상태에 빠질 수 있다.
</code></pre><p>그러나 데드락 같은 부작용이 발생할 수 있다. 데드락은 트랜잭션 사이에서 락 순서를 지정해 방지하거나 waits-for graphs를 통해 확인할 수 있다. 데이터베이스 동기화를 위해 lock의 대안으로 글로벌 타임스탬프를 사용하는 방식으로 데드락 같은 것도 있다.</p><p>데이터베이스에서 여러 동시 사용자의 작업을 관리하는 데 사용되는 메커니즘이 있다. 그 목적은 손실된 업데이트 및 더티 읽기를 방지하는 것이다. 두 가지 유형의 잠금은 비관적 잠금과 낙관적 잠금이다.</p><ul><li>Pessimistic locking(비관적 잠금): 업데이트할 의도로 레코드를 읽는 사용자는 다른 사용자가 레코드를 조작하지 못하도록 레코드에 배타적 잠금(exclusive lock)을 설정한다. 이는 사용자가 잠금을 해제할 때까지 아무도 해당 레코드를 조작할 수 없음을 의미한다. 단점은 사용자가 매우 오랫동안 잠길 수 있어 전체 시스템 응답이 느려지고 좌절감을 유발할 수 있다는 것이다.<pre><code>비관적 잠금 사용 위치: 주로 데이터 경합&#40;사용자가 한 번에 데이터베이스 시스템에 요청하는 정도&#41;이 심한 환경에서 사용됩니다. 동시성 충돌이 발생하는 경우 잠금을 통해 데이터를 보호하는 비용이 트랜잭션을 롤백하는 비용보다 적습니다. 비관적 동시성은 레코드의 프로그래밍 방식 처리에서와 같이 잠금 시간이 짧을 때 가장 잘 구현됩니다. 비관적 동시성은 데이터베이스에 대한 지속적인 연결이 필요하며 사용자가 데이터와 상호 작용할 때 확장 가능한 옵션이 아닙니다. 레코드가 비교적 오랜 기간 동안 잠겨 있을 수 있기 때문이다. 웹 애플리케이션 개발에 사용하기에는 적합하지 않습니다.
</code></pre></li><li>Optimistic locking(낙관적 잠금): 이를 통해 여러 동시 사용자가 데이터베이스에 액세스할 수 있으며 시스템은 각 사용자가 작성한 초기 읽기 사본을 보관한다. 사용자가 레코드를 업데이트하려고 할 때 애플리케이션은 마지막으로 읽은 이후에 다른 사용자가 레코드를 변경했는지 여부를 확인한다. 애플리케이션은 메모리에 보관된 초기 읽기를 데이터베이스 레코드와 비교하여 레코드에 대한 변경 사항을 확인함으로써 이를 수행한다. 초기 읽기와 데이터베이스 레코드 간의 불일치는 동시성 규칙을 위반하므로 시스템이 업데이트 요청을 무시한다. 오류 메시지가 생성되고 사용자에게 업데이트 프로세스를 다시 시작하라는 메시지가 표시됩니다. 필요한 잠금의 양을 줄여 데이터베이스 성능을 향상시켜 데이터베이스 서버의 부하를 줄인다. 잠긴 사용자가 없기 때문에 제한된 업데이트가 필요한 테이블에서 효율적으로 작동한다. 그러나 일부 업데이트는 실패할 수 있다. 단점은 여러 동시 사용자의 많은 업데이트 요청으로 인한 지속적인 업데이트 실패이며 사용자에게 실망을 줄 수 있다.</li></ul><p>낙관적 잠금을 사용하는 위치: 데이터에 대한 경합이 낮거나 데이터에 대한 읽기 전용 액세스가 필요한 환경에 적합한다. 낙관적 동시성은 .NET에서 광범위하게 사용되어 장기간 데이터 행을 잠글 수 없는 모바일 및 연결이 끊긴 애플리케이션의 요구 사항을 해결한다. 또한 레코드 잠금을 유지하려면 데이터베이스 서버에 대한 지속적인 연결이 필요하며 이는 연결이 끊긴 응용 프로그램에서는 불가능하다.</p><p>데이터베이스 락을 좀 더 깊게 이해해보기 위해 <a href='ttps://en.wikipedia.org/wiki/Record_locking'>Recording lock</a>에 대해서 알아보자.</p><h2>Recording lock</h2><p><code>Recording lock</code>은 데이터베이스에서 동시에 데이터에 접근하는 것을 막는 기술이다. 일관된 결과를 얻기 위함이다. 간단한 방법으로, 레코드를 수정하고 있을 때 다른 사용자가 변경하지 못하게 하도록 하는 것이다. 데이터베이스 매니지먼트 이론에서, locking은 <code>isolation</code>을 구현하는데 사용된다. <code>ACID</code>에서 말하는 <code>I</code>이다.</p><h3>Use of Locks</h3><h4>Exclusive locks</h4><p>이름에서 알 수 있듯이, 일반적으로 레코드에 write하기 위한 목적으로 단일 엔티티가 독점적으로 보유한다. locking schema가 목록으로 표시되는 경우, 보유자 목록(holder list)에는 항목이 하나만 보여진다. <code>Exclusive locks</code>는 락이 필요한 다른 엔티티가 처리되지 않도록 효과적으로 차단하므로 아래 상황을 주의해야한다.</p><p><strong>lock이 가능한 짧은 시간 동안 유지되도록 해야한다.</strong> 그러지 않으면 Deadlock에 빠질 수 있다.</p><p>lock을 가지고 있지 않은 사람은 라운드 로빈 방식으로 처리되는 목록이나 FIFO 큐에 보관될 수 있다. 이렇게 하면 가능한 모든 waiter가 락을 가지고 푸는데 같은 기회를 가진다.</p><h4>Shared locks</h4><p>Exclusive locks와 다르게 Shared locks는 보유자 목록(holder list)에 여러 항목이 포함될 수 있다. 이를 사용하면 모든 보유자가 잠금을 ㅎ재ㅔ할 때까지 레코드를 변경할 수 없음을 알고 모든 보유자가 내용을 read할 수 있다.</p><p>동일한 엔터티에 대한 잠금 요청이 대기열에 있는 경우 공유 잠금이 허용되면 대기열에 있는 모든 공유 잠금도 허용될 수 있다. 큐에서 배타적 잠금이 다음에 발견되면 모든 공유 잠금이 해제될 때까지 기다려야 한다. 배타적 잠금과 마찬가지로 이러한 공유 잠금은 가능한 한 최소 시간 동안 유지되어야 한다.</p><h1>Python GlobalInterpreterLock</h1><h2>Overview</h2><p>Python은 동시성 프로그램이 힘들다는 특성이 있는데 이는 GIL 때문이다. GIL은 언제나 단 하나의 쓰레드만 동작하도록 한다. 한번에 하나의 쓰레드만 실행가능하기 때문에, 쓰레드가 여러 프로세스를 사용하는게 불가능하다. 하지만 걱정할 필요는 없다. <a href='ttps://python.land/python-concurrency/python-multiprocessing'>다른 대체 수단</a>들이 있긴 하다.</p><h2>Thread-safety</h2><p>파이썬 쓰레드는 같은 메모리를 공유한다. 동시에 여러 쓰레드가 실행되면, 우리는 공유 데이터에 접근하는 쓰레드가 뭔지 알 수 없다. 그러므로, 데이터 접근의 결과는 스케줄링 알고리즘에 의존한다.쓰레드들이 데이터를 접근/변경하기 위해 <code>racing</code>을 할 때, 이 알고리즘이 어떤 쓰레드가 실행될 지 결정한다. CPython에서, GIL은 여러 쓰레드가 한번에 파이썬 바이트코드를 실행하는 것을 막는 mutex(lock)이다. race conditions를 방지하고, 쓰레드의 안정성을 보장하는 역할을 한다. 간단히 말해서, 이 mutex CPython의 메모리 매니지먼트가 안전하지 않기 때문에 필수적이다.</p><h2>Race condition 설명</h2><pre><code>a = 2
threadA: a = a + 2
threadB: a = a &#42; 3
</code></pre><p><code>thread A -&gt; threadB</code>로 순서로 실행될 때랑 <code>threadB -&gt; threadA</code>로 실행될 때의 결과는 다르다. 심지어 두 thread가 <code>a=2</code>의 값으로 실행될 수도 있다. <strong>Race condition</strong>이란 시스템의 행동이 시간이나 다른 사건들에 의해 연속성에 의존하는 시스템의 상태를 의미하게 된다. 이것들은 몇몇의 개발자들도 익숙하지 않은 개념이긴 하다. 또한 이거는 랜덤하게, 예측할 수 없는 행동을 하는 경향도 있다. 디버깅하기도 힘들고. <strong>이러한 이유들 때문에 파이썬이 GIL을 사용한다.</strong></p><p>나중에 이어서 경험했던 Lock들에 대해서 살펴보도록 하자. (golang and MySQL)<h2>Reference</h2></p><ul><li><a href='ttps://en.wikipedia.org/wiki/Lock_(computer_science)'>Lock</a></li><li><a href='ttps://en.wikipedia.org/wiki/Record_locking'>Recording Lock</a></li><li><a href='ttps://en.wikipedia.org/wiki/Two-phase_locking'>2PL</a></li><li><a href='ttps://en.wikipedia.org/wiki/Deadlock'>Deadlock</a></li><li><a href='ttps://wiki.python.org/moin/GlobalInterpreterLock'>python GIL</a></li><li><a href='ttps://python.land/python-concurrency/the-python-gil'>python GIL2</a></li><li><a href='ttps://python.land/python-concurrency/python-multiprocessing'>python Multiprocessing</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-09-11-spark_and_spark.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-09-11-spark_and_spark.html"/>
    <title>Apache Spark misc</title>
    <updated>2021-09-11T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>저번에 간단히 공부하고 소개한 적 있는 스파크에 대해서 조금 더 정리하고 가려운 구석을 긁어보도록 하자.</p><h1>Overview</h1><p>아파치 스파크는 대용량 데이터처리를 위한 오픈소스 플랫폼이다. 데이터 병렬처리나, 장애 허용(Data paralleism and fault tolerance)가 가능한 클러스터를 위한 API를 제공한다. UC Berkely의 한 랩에서 만들었다. Resilient distributed dataset(RDD)라는 데이터 아키텍처를 근본적으로 사용하면서 시작되었다. 이는 read-only 데이터들이 클러스터 위에 분산되어 있는 구조로 설계되었다. 이게 fault-tolerant하다는 의미이다. DataFrame의 경우 RDD의 최상위 레벨로 추상화해서 출시되었는데, Spark 2.x.x 버전 이후로는 RDD보다는 DataFrame API를 사용하도록 권장되고 있다. 즉, Dataset과 Dataframe은 RDD 기술에 기반하고 있다.</p><h1>Motivation</h1><p>스파크와 RDD는 2012년에  MapReduce 처리 방식의 컴퓨팅 패러다임의 한계를 뛰어 남기 위해 나왔다. MapReduce의 경우 리니어한 데이터플로우를 분산환경에서 가지는데, 디스크(hdfs사용)에서 데이터를 읽고 데이터를 map하고, map의 결과를 reduce하고, 다시 그 결과를 디스크에 쓰는 과정이었다. 이것들이 iterative한 일들에서 너무 비효율적이고 시간이 오래 걸렸다. 이를 RAM으로 하는게 어떨까? 라고 생각했다. RAM 위에서 동작하지만 read-only로 데이터를 관리하면서 immutable스럽게 관리하기로 했다. 어떻게 데이터가 만들어졌는지만 기록해도 fault-tolerance를 가질 수 있기 때문이다. 그래서 스파크의 RDD는 분산환경에서 동작하는 데이터셋을 고안했으며 이는 분산 공유 메모리 위에서 데이터를 제어하도록 디자인되었다.</p><p>즉, Spark는 데이터 세트를 여러 번 방문하는 알고리즘과 대화형/탐색 데이터 분석 - 예를 들어 반복적인 데이터베이스 스타일 데이터 쿼리 - 의 구현을 쉽게 했다. 이러한 애플리케이션의 latency는 맵리듀스 방식으로 구현된 하둡에 비해서 몇 배나(100배가량) 줄 수 있었다. 반복 알고리즘 클래스 중에는 머신러닝 알고리즘 같은게 있을 수 있다. 그리고 주요 특징 중 하나가 transformation만 하면 lazy-executing을 해서 실제로 실행은 안되고 lazy한 상태이다. 이 상태에서 실행 플랜이 결정되므로 적당히 최적화된 리소스 사용을 할 수도 있다.</p><h1>Components</h1><p>Spark에는 1. 클러스터 매니저와 2. 분산 스토리지 시스템이 필요하다.</p><ol><li>클러스터 매니징을 위해서, standalone 방식이나, Hadoop YARN, Apache Mesos, Kubernetes 같은 것들이 있다.</li><li>그리고 분산 스토리지의 경우, Spark는 HDFS를 비롯하여 다양한 것들이 있다.</li></ol><h2>Reference</h2><ul><li><a href='https://en.wikipedia.org/wiki/Apache_Spark'>Apache Spark Wiki</a></li><li><a href='https://spark.apache.org/docs/latest/index.html'>Apache Spark Official Docs</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-07-28-network_embedding.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-07-28-network_embedding.html"/>
    <title>node2vec</title>
    <updated>2021-07-28T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>아래 글은 <a href='blog'>여기</a>를 제 언어로 번역한 글입니다. 글 아래에 약간의 <a href='#배경지식'>필요 지식</a>에 대한 추가 설명도 있습니다.</p><h1>동기</h1><p>데이터 과학자라면 임베딩이라는 말을 자주 듣는데, 대부분 NLP분야에서 들어봤을 것이다. 이 번거로운 embedding 녀석..</p><h1>Embedding process 임베딩 과정</h1><p>그래서 어떻게 동작할까? 임베딩 학습 방식은 <a href='#skip-gram'><code>skip-gram</code></a> 모델을 사용하는 <code>word2vec</code> 임베딩 학습 방식이랑 똑같다. 만약에 여러분이 <code>word2vec skip-gram model</code>이 익숙하면 좋겠지만 아니라면, <a href='http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/'>여기 설명</a>을 보는 걸 추찬한다.</p><p>내가 <code>node2vec</code>을 설명하는 것에 대해 생각할 수 있는 가장 자연스러운 방법은 <code>node2vec</code>이 <code>corpus&#40;말뭉치&#41;</code>를 생성하는 방법을 설명하는 것이다. 그리고 만약 우리가 <code>word2vec</code>을 이해한다면, 우리는 이미 <code>corpus</code>를 어떻게 <code>embed</code>하는지 알고 있는 것이다.</p><p>그럼 <code>graph</code>에서 어떻게 <code>corpus</code>를 뽑아낼 수 있을까? 그게 <code>node2vec</code>의 혁신적인 부분이고, 그것은 멋진 <code>sampling strategy&#40;샘플링 전략&#41;</code>에서 나온다.</p><p>입력된 그래프로부터 <code>corpus</code>를 생성하기 위해, 최대 out-degree가 1인 directed acyclic graphs를 생각해보자. 각각의 단어가 노드이고 문장의 다음 단어를 가르킨다고 생각하면 된다.</p><p><img src="https://miro.medium.com/max/982/1*oEuJHzd3iPpord7sFR1AhQ.png" alt="Setence in a graph representation" /></p><p>이 방법으로는 word2vec이 이미 임베드된 그래프가 된 것을 볼 수 있지만 많은 경우 중 하나일 뿐이다. 하지만 대부분 그래프들은 (un)directed, (un)weighted, (a)cyclic 하고, 대부분 text보다 구조가 복잡하다.</p><p>이를 해결하기 위해서, node2vec은 directed acyclic subgraphs를 샘플링 하기 위해 하이퍼파라미터에 의해 수정 가능한 (tweakable by hyperparameters) 샘플링 전략을 사용한다. 이는 각각의 노드들의 랜덤 워크(random walks)에 의해 생성된다. 간단한 듯?</p><p>이를 생성하기 전에 먼저 시간화를 한번 해봅시다.</p><p><img src="https://miro.medium.com/max/2000/1*3pmstIOig4Qc3lrQS4xrNg.png" alt="node2vec embedding process" /></p><h2>샘플링 전략</h2><p>우리는 큰 그림을 그렸고, 이제 조금 더 깊게 파고 들어봅시다. <code>Node2Vec</code>의 샘플링 전략은 4가지 arguments를 가지는데,</p><ul><li><strong>Number of walks(워크의 수)</strong>: 그래프의 각 노드에서 생성될 랜덤 워크 수</li><li><strong>Walk length(워크의 길이)</strong>: 각 랜덤 워크에 몇개의 노드가 있는 지</li><li><strong>P</strong>: Return(돌아가는) hyperparameter</li><li><strong>Q</strong>: Inout(들어가고 나오는) hyperparameter</li></ul><p>그리고 표준 skip-gram parameters(context window size, number of iterations, etc)등을 가진다.</p><p>처음 2개 hyperparameter들은 설명 그대로이다. 랜덤 워크 생성을 위한 알고리즘은 각 노드를 방문하면서 <code>number of walk</code>와 <code>walk length</code>를 생성한다.</p><p>Q와 P는 시각적으로 설명하면 좋은데, 여러분이 랜덤 워크 위에 있다고 하고, 방금 node <code>t</code>로부터 <code>v</code>로 아래 그림처럼 전이(have just transitioned) 되었다.</p><p><img src="https://miro.medium.com/max/1400/1*44_Ys2JeD8B0NVdbJ4TQlg.png" alt="image" /></p><p><code>v</code>에서 다른 인접한 곳으로 이동하는 확률은 <code>edge weight &#42; α</code>(normalized) (<code>α</code>는 하이퍼파라미터에 의존함, 즉, P와 Q에 의해서 결정됨).</p><p>(역주: 위 그래프를 보면 node v에서 t로 돌아갈 확률은 1/p, x1으로 갈 확률은 1, x2, x3는 각각 1/q이다.)</p><p>P는 v를 방문했다가 t로 돌아가는 확률을 통제하고, Q는 방문하지 않은 이웃들을 탐색하는 확률을 통제한다. 직관적인 방식으로 보면, 이거 약간 <code>tSNE</code>에서의 perplexity parameter 같기도 한데, 이것은 우리가 그래프의 로컬/글로벌 구조를 강조할 수 있게 해준다.</p><p>weight도 중요한 사항임을 까먹지 말아야한다. 그래서 최종적으로 travel probability는</p><ol><li>The previous node in the walk (직전에 방문한 노드)</li><li>P and Q</li><li>Edge weight</li></ol><p>의 함수이다.</p><p>이것들이 node2vec의 본질이다. 한번에 다 이해하지 못했으면 한번 더 읽어보기를 강력히 권장한다</p><p>샘플링 전략을 이용해, node2vec은 word2vec에서 사용한 것과 같은 임베딩에 사용되는 sentences(the directed subgraphs)를 만들 것이다.</p><h1>배경지식</h1><h2>Skip-gram</h2><ul><li><a href='skip-gram'>link</a></li></ul><h2>Reference</h2><ul><li><a href='https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef'>blog</a></li><li><a href='http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/'>skip-gram</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-07-21-infra_ai.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-07-21-infra_ai.html"/>
    <title>인공지능이 코드를 짠다고?</title>
    <updated>2021-07-21T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p><code>Github</code>에서 발표한 <code>copilot</code>라는 녀석이 최근에 꽤나 화제였다. <code>AI</code>가 작성하고 싶은 코드를 <strong>잘</strong> 적으면 코드로 잘 바꿔준다는 내용이다. 좋다. 좋아~ 사실 막 그렇게 감흥은 없다. 어차피 엔트리 레벨의 버전이고 실제 업무에서 사용할 일은 당장은 없을 것 같다. 그리고 워낙 코드라는게 좀 그런게 있어서... 근데 문득 생각해보니 인프라 작업에서는 이 일이 <strong>가능했으면</strong> 좋겠다. 무슨 말이냐고 하면, 흔히 인프라 설계할 때 다이어그램 같은 거 그려 놓고 보안, 고가용성 등을 신경 쓰는 디자인을 하게 된다. AZ 구성은 어떻고, 서브넷은 여기 이렇게 두고, private, public, 중간에 게이트웨이를 두니 마니 등등. 스케일링도 마찬가지고. 데이터 파이프라인 구성도 마찬가지다. 데이터 수집 서버는 어떻게 구성하고 ETL 처리부터 모델 서빙하는 것까지. 다들 그림 그리라면 잘 그리는데 실제 구축하려고 하면 시간이 꽤 걸리기 마련이다.</p><p>이걸 편하게 해준다고 아마존, 구글, 마이크로소프트에서는 <code>console ui</code>로 편하게 구성할 수 있게 해주는데 이게 또 동전의 양면처럼 불편한 점도 많다. 이게 가볍게 할 땐 좋은데 결국 팀 입장에서는 관리가 안된다. 그래서 <code>Infrastructure as Code</code>라는게 나오게 된건데, 역설적인 것 같다. 편하게 쓰려고 <code>ui</code>를 만들었는데 <code>ui</code>가 불편해서 <code>IaC</code>가 나왔다.</p><p>그럼 뻘소리를 마무리 해보자. 인프라를 인공지능이 구성 해주면 어떨까? 흠... 꽤 재밌을 것 같다.</p><h2>Reference</h2><ul><li><a href='https://copilot.github.com/'>copilot</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-07-14-thinking_of_ps.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-07-14-thinking_of_ps.html"/>
    <title>꾸준히 PS를 공부하자</title>
    <updated>2021-07-14T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>Problem Solving(이하 PS)를 평생 모르고 살다가 2년 전에 처음 접하게 되었다. 현욱님, 연우님이랑 같이 일하면서 경험하게 되었는데 이 시작과 경험은 내게 지금까지 소중한 출발점이다.</p><p>PS를 잘하고 싶지만 아직 잘하지 못한다. 투입한 시간도 적고, PS를 잘하기에는 시작한 나이도 좀 늦은 것 같다. 그래도 왜 재밌게 하고 있느냐가 좀 중요한 포인트 같다.</p><p>나는 산업경영공학을 전공했는데, 대학교 1학년때까지만 해도 수학/과학/컴퓨터만 했는데 2학년 때부터는 전공과목을 주로 들으니 전공 관련한 수학/컴퓨터만 공부했다. 이론 공부라기 보다는 약간 응용쪽. 근데 내가 대학교 졸업할 때까지 꾸쭌하게 한 것이 모든 학기에 수학과 과목을 하나씩 넣는 것이었다. 수학과의 수치해석, 수리통계, 해석학 등등을 들었었는데 그 이유가 내 수학머리를 굳게 하지 않기 위함이었다. 이는 성공적이었다.<del>모든 과목을 다 듣고 군대가서 다 잃어버리게 되는 슬픈 결말이...</del></p><p>PS도 비슷한 것 같다. 실제 실무에서 서버 개발, 인프라 개발에 거의 쓰인 적은 없지만 PS를 할 때 문제를 보고 풀이를 접근해나가는 과정에서 내 머리는 구현을 생각하고 종이에 적어보기도 한다. 이 과정에서 일상 생활에서 하지 않는 사고 과정이 조금씩 나오는 것 같다.</p><p>PS 공부의 문제점이 있는데 무언가에 막힐 때이다. 사실 고등학교 공부만 해도 막힐 일이 없었고, 대학 공부는 모르면 찾아보고 친구들, 교수님들이랑 토론하면 되는 문제였다. 그래서 문제 해결하는 능력이 길러졌는데, 생각해보니 검색하는 능력만 길러낸 것 같기도 하고. 여튼, 돌아와서 PS 공부하면 막히면 좀 짜증이 날 때가 많다. 답을 검색해서 찾아낼 수 있는데 이러면 또 내가 푼게 아니게 되는 것 같고. 그러한 느낌이 들 때 쉬쉬 거리게 된다.</p><p>포기하지 않고 꾸준히 하는게 더 중요하다고 느끼는 요즘이다. 하루에 한문제씩 꼭 풀자.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-06-22-scala_collection.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-06-22-scala_collection.html"/>
    <title>Scala Collections</title>
    <updated>2021-06-22T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>Intro</h1><p>Scala를 사용하면서 Collection에 대해서 많이 헷갈려서 정리를 좀 하고자 한다. 정적 언어로 개발을 안했다보니 익숙치 않는게 문제.</p><h1>Mutable and Immutable collections</h1><p>Scala collections은 mutable/immutable한 컬렉션으로 나뉜다. mutable하다는 건 값을 바로 변경하거나 확장할 수 있다. 이 말은 값을 수정할 수 있다는 말이고, 이는 side effect가 발생할 수 있다는 말과 같다. 반대로, immutable한 것은 여전히 변경 가능하지만, return값으로 새로운 컬렉션을 준다. 즉, 이전 collection은 변하지 않는다.</p><p>모든 collection은 <code>scala.collection</code> 패키지 혹은 이것의 서브 패키지 중 하나인 <code>scala.collection.mutable</code> or <code>scala.collection.immutable</code> 패키지에서 찾을 수 있다. <code>mutable</code> 패키지는 다 mutable하고 <code>immutable</code> 패키지는 다 immutable하다.</p><p><code>scala.collection</code>에 있는 컬렉션은 둘 중 하나가 될 수 있는데, 예를 들어 <code>collection.IndexedSeq&#91;T&#93;</code>는 <code>collection.immutable.IndexedSeq&#91;T&#93;</code>와 <code>collection.mutable.IndexedSeq&#91;T&#93;</code>의 슈퍼 클래스이다.</p><p>기본적으로 scala는 immutable 컬렉션을 선택한다.</p><p>아래는 <code>scala.collection</code> 패키지에 있는 컬렉션들이다. 이것들은 전부 high-level abstract classes or traits이며 mutable할수도, immutable 할 수도 있다.</p><p><img src="https://docs.scala-lang.org/resources/images/tour/collections-diagram-213.svg" alt="scala.collection" /> <code>scala.collection.immutable 패키지</code> <img src="https://docs.scala-lang.org/resources/images/tour/collections-immutable-diagram-213.svg" alt="scala.collection.immutable" /> <code>scala.collection.mutable 패키지</code> <img src="https://docs.scala-lang.org/resources/images/tour/collections-mutable-diagram-213.svg" alt="scala.collection.mutable" /> 그래프 읽는 법 ![legend]https://docs.scala-lang.org/resources/images/tour/collections-legend-diagram.svg</p><p>collection api들은 대부분 위 그림에서 보이는 이름으로 바로 선언해서 쓸 수 있다.<pre><code class="lang-scala">Iterable&#40;&quot;x&quot;, &quot;y&quot;, &quot;z&quot;&#41;
Map&#40;&quot;x&quot; -&gt; 24, &quot;y&quot; -&gt; 25, &quot;z&quot; -&gt; 26&#41;
Set&#40;Color.red, Color.green, Color.blue&#41;
SortedSet&#40;&quot;hello&quot;, &quot;world&quot;&#41;
Buffer&#40;x, y, z&#41;
IndexedSeq&#40;1.0, 2.0&#41;
LinearSeq&#40;a, b, c&#41;
</code></pre></p><p>그리고 api들의 결과 타입은 콜렉션 그 타입을 그대로 가진다. 이를 <code>uniform return type principle</code>이라고 한다. 대부분의 컬렉션 클래스는 root, mutable, immutable 중 하나인데, 단 하나 <code>Buffer</code>의 경우에만 mutable collection을 가진다.</p><h2>Reference</h2><ul><li><a href='https://docs.scala-lang.org/overviews/collections-2.13/overview.html'>Scala Collection</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-06-09-spark_switch_context.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-06-09-spark_switch_context.html"/>
    <title>Switch spark context</title>
    <updated>2021-06-09T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>Intro</h1><p><code>Azure Databricks</code> 원격에서 바로 작업하던 환경에서 아래와 같은 몇가지 이유로 로컬 환경에서도 작업할 수 있도록 해야했다.</p><ul><li>버전 관리가 되었으면 한다</li><li>코드 재사용이 더 활발했으면 한다</li><li>인터페이스 사용이 더 빨랐으면 한다</li><li>로컬 컴퓨터에서 원격으로 <code>Job sumit</code>을 할 수 있으면 좋겠다</li><li><code>local spark</code> 환경에서 <code>Unit test</code> 할 수 있으면 좋겠다</li></ul><p>작업자가 혼자일 때는 저런 것들이 필요 없을 수 있었다면 2명이 된 지금, 앞으로를 생각했을 때 필요한 일이었다.</p><h1>버전 관리과 코드 재사용</h1><p><code>.ipynb</code>과 <code>.py</code>을 관리 해야했는데 크게 어려운 건 없었다. policy만 일단 잘 정하고 빨리 시작하는게 중요했다. 모듈화 할만한 건 <code>.py</code>으로 다 관리하고 <code>notebook</code>에서는 <code>data manipulation</code>만 하기로 하자. 그럼 코드 재사용도 가능해지게 되었고 추후에 <code>serving api</code> 만들 때도 도움이 될 것 같다.</p><h1>로컬 IDE 사용과 원격 연결</h1><p>원격에서 작업하면, 특히 나처럼 spark 입문자에게 <code>api reference</code>를 매번 찾아보면서 하기엔 속도가 안나는 경우가 종종 있다. 실수도 잦고. 그래서 <code>pycharm</code>에서 조금만 설정하면 연결할 수 있게 된다. 그럼 <code>azure databricks notebook</code>에서 작업하는 것과 거의 똑같다. 다만 azure databricks에서 제공하는 몇가지 서비스들(display해서 plotting하거나 등등)을 못쓰는데 그건 <code>pandas</code>로 몇몇 모듈만 만들어놓으면 되니까 문제는 아니다.</p><h1>Spark Context 변경 및 단위 테스트</h1><p>로컬에서 원격에 <code>job sumit</code>을 하면서 작업을 주로 하겠지만, 때론 이게 번거로울 때도 많다. 예를 들어, 쉬고 있던 클러스터를 <code>run</code> 해야하거나 <code>unit test</code>를 해야하는데 비싼 원격 클러스터를 쓰기엔 돈이 아까웠다. 그래서 로컬 클러스터랑 원격 클러스터랑 쉽게 context switching을 하고 싶었다. <code>master</code> 연결만 잘 하면 되겠거니 했는데 아무리 해도 <code>session</code>이 원격에 붙길래, 그게 보니까 databricks-connect 패키지에 pyspark 설정을 건드려줘야할 것 같은데 뭔가 영 이상하다. pyspark sub-dep로 관리하는게 아니라 지들 pyspark를 wrapping한 것처럼 사용해서 context switching이 잘 안됐다.</p><p>결론만 말하면, 가상환경을 새로 만들어서 <code>pyspark</code>를 pypi로 새로 깔고 그 환경을 interpreter에 연결해서 실행하니 로컬에서 잘 실행되었다. 후... 로컬 테스트할 때는 환경도 바꾸고 interpreter도 바꾸고 해야해서 약간 번거롭지만 일단 목표한 바는 이루었으니 쓰도록 하자.</p><pre><code class="lang-python">from pyspark.sql import SparkSession

# 기본 원격 클러스터 연결 방식 &#40;databricks-connect configure 에 세팅된대로 연결&#41;
spark = SparkSession.builder.getOrCreate&#40;&#41;

# 로컬 클러스터 연결 방식
spark = SparkSession.builder.master&#40;'local'&#41;.getOrCreate&#40;&#41;
</code></pre><h1>SPARK_HOME 충돌 문제</h1><p>원격에서 로컬로 잘 연결을 바꿨었다. 문제는 다시 원격에 붙어서 작업하고 싶어서 python interpreter를 바꾸고 작업하는데 여전히 로컬에 붙고 있었다. 이상해서 계속 삽질하다가 <code>databricks-connect test</code>의 결과를 보고 단서를 찾았다. 내용은 <code>SPARK&#95;HOME</code> 환경 변수가 local이랑 가상환경에서 사용하는 거랑 충돌이 나고 있었던 것. 그래서 환경 변수를 잘 수정해서 해결했다. 근데 이거 환경 바꿀 때마다 일일이 신경써주기 귀찮은데 좀 더 편하게 하는 방향을 잡긴 해야겠다.<h2>Reference</h2></p><ul><li><a href='https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect'>databricks-connect</a></li><li><a href='https://docs.microsoft.com/en-us/azure/databricks/notebooks/github-version-control'>github-version-control</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-05-25-scala_and_spark.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-05-25-scala_and_spark.html"/>
    <title>Scala and Spark</title>
    <updated>2021-05-25T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>Intro</h1><p>스칼라는 객체 지향 언어와 함수형 언어의 특징을 합쳐 놓은 거라고 함. 자바처럼 JVM(자바가상머신) 위에서 동작함. </p><h1>쌩 기본</h1><h2><code>Hello world</code>를 띄워보자</h2><pre><code class="lang-scala">// way 1 싱글톤 오브젝트의 main 함수 구현
object HelloWorldObject {
    def main&#40;ars: Array&#91;String&#93;&#41;: Unit = {
        println&#40;&quot;Hello World main&quot;&#41;
    }
}

// way2: app 트레잇 상속
object HelloWorld extends App {
    println&#40;&quot;Hello World&quot;&#41;
}
</code></pre><h2>객체, 자료형, 문자열, 변수, 함수, 클래스, 트레잇, 싱글톤 객체, 콜렉션</h2><p>스칼라는 기본 자료형, 함수, 클래스 등 모든 것을 객체로 취급함. Any를 최상위로 해서 AnyVal(Double, Float, Long, Int, Short, Byte, Unit, Boolean, Char), AnyRef(List, Option, Class)가 있음.</p><p>객체 비교는 <code>==</code> 아니면 <code>!=</code>를 사용함.</p><p>자료형, 문자열은 문서 찾아보자.</p><p>변수는 재할당 가능한 <code>var</code>와 재할당 불가능한 <code>val</code>이 있음. 가급적 <code>val</code>을 써서 동시성 처리에 유용하도록 하자.</p><p>함수는 <code>def</code>로 선언. 리턴과 리턴 타입은 생략 가능. 파라미터 타입은 생략 불가. 리턴 값이 없으면 <code>Unit</code> 이용하거나 리턴 값이 없으면 마지막 값을 리턴. 함수의 매개변수는 불변 변수(<code>val</code>)이기 때문에 재할당 불가. 리턴 타입 생략하면 자동으로 추론 함.</p><pre><code class="lang-scala">// 함수 선언 
def add&#40;x: Int, y: Int&#41;: Int = {
  return x + y
}

// x는 val 이기 때문에 변경 불가 
def add&#40;x: Int&#41;: Int = {
  x = 10 
}

// 리턴 타입 생략 가능 
def add&#40;x: Int, y: Double&#41; = {
  x + y
}

// 리턴 타입이 Unit 타입도 생략 가능 
def add&#40;x: Int, y: Int&#41; = {
  println&#40;x + y&#41;
}

// 리턴 데이터가 없는 경우 Unit을 선언  
def add&#40;x: Int, y: Int&#41;: Unit = {
  println&#40;x + y&#41;
}
</code></pre><p>가변 길이 파라미터는 파라미터 입력시 <code>&#42;</code>를 이용하면 <code>Seq</code>형으로 변환되어 입력.<pre><code class="lang-scala">// 여러개의 Int 형을 입력받아서 합계 계산 
def sum&#40;num:Int&#42;&#41; = num.reduce&#40;&#95; + &#95;&#41;

scala&gt; sum&#40;1, 2, 3&#41;
res22: Int = 6

scala&gt; sum&#40;1&#41;
res23: Int = 1
</code></pre></p><p>함수</p><ul><li>람다함수</li></ul><pre><code class="lang-scala">// exec는 3개의 파라미터&#40;함수 f, x, y&#41;를 받음
def exec&#40;f: &#40;Int, Int&#41; =&gt; Int, x: Int, y: Int&#41; = f&#40;x, y&#41;

// 람다 함수를 전달하여 처리. x+y 작업을 하는 함수 전달 
scala&gt; exec&#40;&#40;x: Int, y: Int&#41; =&gt; x + y, 2, 3&#41;
res12: Int = 5

// 선언시에 타입을 입력해서 추가적인 설정 없이 처리 가능 
scala&gt; exec&#40;&#40;x, y&#41; =&gt; x + y, 7, 3&#41;
res13: Int = 10

// 함수에 따라 다른 처리 
scala&gt; exec&#40;&#40;x, y&#41; =&gt; x - y, 7, 3&#41;
res14: Int = 4

// 언더바를 이용하여 묵시적인 처리도 가능 
scala&gt; exec&#40;&#95; + &#95;, 3, 1&#41;
res15: Int = 4
</code></pre><ul><li>커링</li><li>클로저</li><li>타입(Generic)</li></ul><pre><code class="lang-scala">// for class
import scala.collection.mutable

trait TestStack&#91;T&#93; {
  def pop&#40;&#41;:T
  def push&#40;value:T&#41;
}

class StackSample&#91;T&#93; extends TestStack&#91;T&#93; {

  val stack = new scala.collection.mutable.Stack&#91;T&#93;

  override def pop&#40;&#41;: T = {
    stack.pop&#40;&#41;
  }

  override def push&#40;value:T&#41; = {
    stack.push&#40;value&#41;
  }
}

val s = new StackSample&#91;String&#93;

s.push&#40;&quot;1&quot;&#41;
s.push&#40;&quot;2&quot;&#41;
s.push&#40;&quot;3&quot;&#41;

scala&gt; println&#40;s.pop&#40;&#41;&#41;
3
scala&gt; println&#40;s.pop&#40;&#41;&#41;
2
scala&gt; println&#40;s.pop&#40;&#41;&#41;
1

// for method
def sample&#91;K&#93;&#40;key:K&#41; {
  println&#40;key&#41;
}

def sample2 = sample&#91;String&#93; &#95;

scala&gt; sample2&#40;&quot;Hello&quot;&#41;
Hello
</code></pre><p>트레잇(Trait)</p><ul><li>자바나 nodejs의 interface와 유사함.</li></ul><pre><code class="lang-scala">// Machine 트레잇 
trait Machine {
  val serialNumber: Int = 1
  def work&#40;message: String&#41;
}

// KrMachine 트레잇 
trait KrMachine {
  var conturyCode: String = &quot;kr&quot;
  def print&#40;&#41; = println&#40;&quot;한글 출력&quot;&#41;    // krprint 기본 구현 
}

// Computer 클래스는 Machine, KrMachine를 둘다 구현합니다. 
class Computer&#40;location: String&#41; extends Machine with KrMachine {
  this.conturyCode = &quot;us&quot;   // code 값 변경 
  def work&#40;message: String&#41; = println&#40;message&#41;
}

// Car 클래스는 Machine, KrMachine를 둘다 구현합니다.
class Car&#40;location: String&#41; extends Machine with KrMachine {
  def work&#40;message: String&#41; = println&#40;message&#41;
  override def print&#40;&#41; = println&#40;&quot;운전중입니다.&quot;&#41; // print 재정의
}

var machine = new Computer&#40;&quot;노트북&quot;&#41;
var car = new Car&#40;&quot;포르쉐&quot;&#41;

scala&gt; machine.work&#40;&quot;computing...&quot;&#41;
computing...

scala&gt; machine.print&#40;&#41;
한글 출력

scala&gt; println&#40;machine.conturyCode&#41;
us

scala&gt; car.work&#40;&quot;driving...&quot;&#41;
driving...

scala&gt; car.print&#40;&#41; 
운전중입니다.

scala&gt; println&#40;car.conturyCode&#41;
kr
</code></pre><p>클래스 - 그냥 클래스스럽다.</p><h2>Spark</h2><p>인메모리 기반의 대용량 데이터 고속 처리 엔진, 범용 분산 클러스터 컴퓨팅 프레임워크. 스파크는 작업을 관리하는 드라이버와, 작업이 실행되는 노드를 관리하는 클러스터 매니저 - 크게 두가지로 구성되어있음.</p><p>스파크 app 구현 방법은 RDD를 이용하는 방법과, Dataset, DataFrame을 이용하는 방법이 있는데, 주로 DataFrame을 이용하는 방법을 알아보자.</p><h3>Spark Sessin init</h3><p>Dataset, DataFrame은 스파크 세션을 이용하여 처리 함.</p><pre><code class="lang-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder&#40;&#41;
  .appName&#40;&quot;Spark SQL basic example&quot;&#41;
  .config&#40;&quot;spark.some.config.option&quot;, &quot;some-value&quot;&#41;
  .getOrCreate&#40;&#41;
</code></pre><h3>DataFrame init</h3><p>DataFrame은 스파크세션의 <code>read</code> method로 생성할 수 있음. <code>read</code>는 json, parquet, orc, text 등 다양한 형식의 데이터를 읽을 수 있음. 자세한 건 <a href='read_dfreader_api'>API 문서</a>를 확인.</p><h3>DataFrame 연산</h3><h4>스키마 확인</h4><p><code>printSchema</code>를 이용함.<pre><code class="lang-scala">val df = spark.read.json&#40;&quot;/user/people.json&quot;&#41;

// 스키마 출력 
scala&gt; df.printSchema&#40;&#41;
root
 |-- age: long &#40;nullable = true&#41;
 |-- name: string &#40;nullable = true&#41;

scala&gt; df.show&#40;&#41;
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
</code></pre></p><h4>조회</h4><p><code>select</code><pre><code class="lang-scala">// name 칼럼만 조회 
scala&gt; df.select&#40;&quot;name&quot;&#41;.show&#40;&#41;
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

// name, age 순으로 age에 값을 1더하여 조회 
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot; + 1&#41;.show&#40;&#41;
+-------+---------+
|   name|&#40;age + 1&#41;|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+
</code></pre></p><h4>show() 함수 설정</h4><p>데이터의 길이와 칼럼의 사이즈를 제한하여 출력할 수 있음<pre><code class="lang-scala">// show 함수 선언 
def show&#40;numRows: Int, truncate: Boolean&#41;: Unit = println&#40;showString&#40;numRows, truncate&#41;&#41;

// 사용방법 
scala&gt; show&#40;10, false&#41;
scala&gt; show&#40;100, true&#41;
</code></pre></p><h4>필터링</h4><p><code>filter</code><pre><code class="lang-scala">// 필터링 처리 
scala&gt; df.filter&#40;$&quot;age&quot; &gt; 21&#41;.show&#40;&#41;
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

// select에 filter 조건 추가 
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot;&#41;.filter&#40;$&quot;age&quot; &gt; 20&#41;.show&#40;&#41;
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot;&#41;.filter&#40;&quot;age &gt; 20&quot;&#41;.show&#40;&#41;
+----+---+
|name|age|
+----+---+
|Andy| 30|
+----+---+
</code></pre></p><h4>그룹핑</h4><p><code>groupBy</code><pre><code class="lang-scala">// 그룹핑 처리 
scala&gt; df.groupBy&#40;&quot;age&quot;&#41;.count&#40;&#41;.show&#40;&#41;
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+
</code></pre></p><h4>칼럼 추가</h4><p>새로운 칼럼을 추가할 때는 <code>withColumn</code>을 이용<pre><code class="lang-scala">// age가 NULL일 때는 KKK, 값이 있을 때는 TTT를 출력 
scala&gt; df.withColumn&#40;&quot;xx&quot;, when&#40;$&quot;age&quot;.isNull, &quot;KKK&quot;&#41;.otherwise&#40;&quot;TTT&quot;&#41;&#41;.show&#40;&#41;
+----+-------+---+
| age|   name| xx|
+----+-------+---+
|null|Michael|KKK|
|  30|   Andy|TTT|
|  19| Justin|TTT|
+----+-------+---+
</code></pre></p><h4>DataFrame의 SQL을 이용한 데이터 조회</h4><p>데이터프레임은 SQL 쿼리를 이용해 데이터를 조회할 수 있음. 데이터프레임을 이용하여 뷰를 생성하고 SQL쿼리를 실행하면 됨.</p><pre><code class="lang-scala">// 뷰생성
val df = spark.read.json&#40;&quot;/user/people.json&quot;&#41;

// DataFrame으로 뷰를 생성 
df.createOrReplaceTempView&#40;&quot;people&quot;&#41;

// 스파크세션을 이용하여 SQL 쿼리 작성 
scala&gt; spark.sql&#40;&quot;SELECT &#42; FROM people&quot;&#41;.show&#40;&#41;
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

// SQL 사용
// 조회 조건 추가 
scala&gt; spark.sql&#40;&quot;SELECT &#42; FROM people WHERE age &gt; 20&quot;&#41;.show&#40;&#41;
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

// 그룹핑 추가 
scala&gt; spark.sql&#40;&quot;SELECT age, count&#40;1&#41; FROM people GROUP BY age&quot;&#41;.show&#40;&#41;
+----+--------+
| age|count&#40;1&#41;|
+----+--------+
|  19|       1|
|null|       1|
|  30|       1|
+----+--------+
</code></pre><h3>DataFrame 저장/불러오기</h3><p>Dataset이랑 DataFrame이랑 같음.</p><h4>데이터 저장</h4><p><code>save</code> 이용<pre><code class="lang-scala">val peopleDF = spark.read.json&#40;&quot;/user/people.json&quot;&#41;
case class People&#40;name: String, age: Long&#41;
val peopleDS = peopleDF.as&#91;People&#93;


peopleDS.write.save&#40;&quot;/user/ds&quot;&#41;
peopleDF.write.save&#40;&quot;/user/df&quot;&#41;
</code></pre></p><h4>저장 포맷 지정</h4><p><code>format</code>을 이용해 json, csv 등 포맷을 설정할 수 있음<pre><code class="lang-scala">// 데이터 저장 
peopleDS.select&#40;&quot;name&quot;&#41;.write.format&#40;&quot;json&quot;&#41;.save&#40;&quot;/user/ds&#95;1&quot;&#41;

// 저장 위치 확인 
$ hadoop fs -ls /user/ds&#95;1/
Found 2 items
-rw-r--r--   2 hadoop hadoop          0 2019-01-24 07:19 /user/ds&#95;1/&#95;SUCCESS
-rw-r--r--   2 hadoop hadoop         53 2019-01-24 07:19 /user/ds&#95;1/part-r-00000-88b715ad-1b5b-480c-8e17-7b0c0ea93e9f.json

// 저장 형식 확인 
$ hadoop fs -text /user/ds&#95;1/part-r-00000-88b715ad-1b5b-480c-8e17-7b0c0ea93e9f.json
{&quot;name&quot;:&quot;Michael&quot;}
{&quot;name&quot;:&quot;Andy&quot;}
{&quot;name&quot;:&quot;Justin&quot;}
</code></pre></p><h4>압축 포맷 지정</h4><p><code>option</code>을 이용하여 압축 포맷을 지정할 수 있음. gzip, snappy 등의 형식을 이용할 수 있음. <pre><code class="lang-scala">// snappy 형식 압축 
peopleDS.select&#40;&quot;name&quot;&#41;.write.format&#40;&quot;json&quot;&#41;.option&#40;&quot;compression&quot;, &quot;snappy&quot;&#41;.save&#40;&quot;/user/ds&#95;1&quot;&#41;

// 저장 모드, SaveMode.ErrorIfExists, SaveMode.Append, SaveMode.Overwrite, SaveMode.Ignore 등의 모드가 있음
// SaveMode 사용을 위해 import
import org.apache.spark.sql.&#95;

val peopleDF = spark.read.json&#40;&quot;/user/people.json&quot;&#41;
peopleDF.select&#40;&quot;name&quot;, &quot;age&quot;&#41;.write.mode&#40;SaveMode.Overwrite&#41;.save&#40;&quot;/user/people/&quot;&#41;
</code></pre></p><h4>테이블 저장</h4><p><code>saveAsTable</code>을 이용합니다.<pre><code class="lang-scala">peopleDF.select&#40;&quot;name&quot;, &quot;age&quot;&#41;.write.saveAsTable&#40;&quot;people&quot;&#41;
</code></pre></p><h4>데이터 불러오기</h4><pre><code class="lang-scala">val peopleDF = spark.read.json&#40;&quot;/user/ds&#95;1/&quot;&#41;
scala&gt; peopleDF.show&#40;&#41;
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+
</code></pre><h2>Spark SQL and Hive</h2><ul><li>Spark SQL</li><li>UDF(User Defined Function)</li></ul><h2>Reference</h2><ul><li><a href='https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameReader.html'>read_dfreader_api</a></li><li><a href='https://wikidocs.net/book/2350'>book</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-05-24-FastAPI.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-05-24-FastAPI.html"/>
    <title>FastAPI로 파이썬 프로젝트 시작하기</title>
    <updated>2021-05-24T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h1>History</h1><p>2년 동안 서버 개발을 <code>node.js</code>로만 하다 보니 다른 언어와 프레임워크로 서버 개발을 해보고 싶었다. 새로운 환경에서 새로운 도전을 하는 건 항상 즐겁다. 개인적으로는 <code>golang</code>이나 <code>rust</code>로 개발해보고 싶은 생각이 있었다.</p><p>하지만 지금 해야하는 프로젝트들이 데이터 관련 일들이 좀 많아가지고, 분석부터 엔지니어링까지 <code>python</code>으로 결국 개발하게 될 것 같다. 그래서 주시하고 있던 <a href='fastapi'><code>FastAPI</code></a> 문서를 좀 보고, 기존에 작업된 레포들을 봤는데 <a href='pydantic'><code>pydantic</code></a>과 <a href='starlette'><code>starlette</code></a> 뿐만 아니라 <code>swagger</code>도 다 한번에 wrapping되어 있어서 <code>api server</code> 만들기에 최적화된 느낌이다. 실제로 벤치마크 지표들을 보면 <code>Django</code>나 <code>Flask</code>보다 엄청나게 빠르다.</p><h1>Intro</h1><p>첫 술에 배 부르랴. 좋은 아키텍처와 코드를 가지기 전에 하나씩 익혀나가도록 하자. 파이썬 프로젝트를 제대로 처음 해본 입장에서 쓴 글임. <code>node.js</code>와 비교 형식이 많을 수도 있다.</p><h1>Project 관리</h1><p>파이썬 패키지들은 pip를 통해서 많이 받는다. 그리고 로컬 개발은 거의 가상 환경에 의해서 개발되기 때문에 가상 환경에 따른 패키지 관리를 따로 해줘야한다. 환경 별로 <code>requirements.txt</code>를 통해서 패키지들을 관리하고 설치한다. 처음 가상 환경에서 개발할 때 현재 사용하는 <code>packages</code>를 파일로 떨구려면 <code>pip freeze &gt; requirements.txt</code>해서 파일로 떨군다. 반대로 클론한 프로젝트에 세팅하려면 <code>pip install -r requirements.txt</code>로 한꺼번에 설치한다.</p><pre><code>nodejs와 비교하면, package.json에 dependencies를 requirements.txt가 관리한다고 생각하면 이해하기 쉽다.
</code></pre><p>...이후에 계속</p><h2>Reference</h2><ul><li><a href='https://fastapi.tiangolo.com/'>fastapi</a></li><li><a href='https://www.starlette.io/'>starlette</a></li><li><a href='https://pydantic-docs.helpmanual.io/'>pydantic</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-04-15-teamplay.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-04-15-teamplay.html"/>
    <title>이것저것 드는 팀에 대한 생각</title>
    <updated>2021-04-15T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>잡다한 글을 좀 싸질러야겠다. 최근에 회사 매니저에게 충격적인 말을 들었다. 일주일 전 일인데 아직까지도 화가 나면서도 당혹스럽다. 겸손하게 말하면 내게 문제가 있나? 라고 생각할 수 있지만 아무리 생각해도 아닌 건 아닌 것 같았다.</p><p>임원진, 팀장급 매니저들의 이해할 수 언행을 2021년이 4개월 지난 지금 시점에 벌써 3번이나 보았다. 첫번째는 실수일 수 있고 두번째은 다른 사람이 했으니까 그러려니 했지만 3번 반복되고 나니 오랫동안 회사에 계셨던 분들의 성향이고 그들의 습성이라고 느껴지니 나와 맞지 않다고 느꼈다.</p><p>부족한 내가 그들을 쫓아갈 수 없으니 떠나는게 맞을 것 같다. 입사 후 첫 한달동안 받은 느낌은 틀리지 않았는데, 그 때 바로 잡았으면 어땠을까 라는 생각도 들지만, 또 어딜가나 배움도 항상 존재하므로 좋게 마무리 짓고 나가고 싶다.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-03-10-alb_ingress.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-03-10-alb_ingress.html"/>
    <title>ALB ingress on EKS</title>
    <updated>2021-03-10T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>운영하고 있는 서비스에서 <code>Network Load Balncer</code>를 사용하고 있었다. 이 전의 글들을 다 읽었다면 알겠지만 우리는 <code>Kops</code>에서 <code>EKS</code>로 옮기는 중이었다. Migration을 잘 했다고 생각했을 시점에 마지막 하나의 문제가 남아있었다는 것을 발견했다. 정확히는 데이터팀에서 확인 요청이 들어왔다. <code>maxmind</code>에서 제공하는 <code>geoip</code>를 사용하고 있는데 이 때 <code>NLB</code>를 사용하면 client ip를 잃어버리게 된다. <code>Classic LB</code>와 <code>Application LB</code>에서는 생기지 않는 상황. 그래서 <a href='application_load_balancer'>Application Load Balancer with Ingress</a>로 자연스레 넘어가게 되었다.</p><p>위에 링크된 곳에 잘 설명되어있는데, 대략적으로 몇가지만 기록해둔다.</p><ol><li>일단 <code>Service</code>는 <code>NodePort</code> 타입으로 열어둬야한다.</li><li><code>Ingress Rule</code> 관련해서는 요구사항에 맞게 작성하면 되는데, 나의 경우에는 <code>annotations</code>가 몇가지 까다로웠다.<pre><code class="lang-json">annotations = {
      &quot;kubernetes.io/ingress.class&quot;                = &quot;alb&quot;                                                                                                 // alb
      &quot;alb.ingress.kubernetes.io/target-type&quot;      = &quot;ip&quot;                                                                                                  // fargate
      &quot;alb.ingress.kubernetes.io/scheme&quot;           = &quot;internet-facing&quot;                                                                                     // public subnets
      &quot;alb.ingress.kubernetes.io/certificate-arn&quot;  = each.value.ssl&#95;cert&#95;arn                                                                               // cert
      &quot;alb.ingress.kubernetes.io/healthcheck-path&quot; = &quot;/ping&quot;                                                                                    // health check
      &quot;alb.ingress.kubernetes.io/backend-protocol&quot; = &quot;HTTP&quot;                                                                                                // traffics route to pod 
      &quot;alb.ingress.kubernetes.io/listen-ports&quot;     = &quot;&#91;{\&quot;HTTP\&quot;: 80}, {\&quot;HTTPS\&quot;: 443 }, {\&quot;HTTPS\&quot;: ${var.my&#95;port}}&#93;&quot;                                    // listeners
      &quot;alb.ingress.kubernetes.io/ssl-policy&quot;       = &quot;ELBSecurityPolicy-TLS-1-1-2017-01&quot;                                                                   // SSL policy. there is default policy though.
    }
</code></pre></li></ol><p><a href='aws-load-balancer-controller'>이 문서</a>를 보고 대략 위와 같이 작성했다. <code>ingress.class</code>는 필요에 맞게 작성해주고, 대부분 <code>nginx</code>와 관련된 문서가 많았던 것 같다. <code>target&#95;type</code>의 경우에 <code>fargate</code>만 사용하는 경우라면 <code>ip</code>로 설정해야하고, <code>scheme</code>은 <code>default</code>값이 <code>internal</code>인데 우리는 <code>public subnets</code>에 로드밸런서를 위치 시키고 싶어서 <code>internet-facing</code>으로 설정. 다른 <code>healthcheck</code>나 <code>listen-ports</code>들은 서버 설정에 맞게 변경해주면 된다. 문서를 처음에 대충 읽었을 때는 <code>backend-protocol</code>이 <code>ingress</code>에서 <code>https</code>요청 처리하는 그런 건 줄 알았는데 알고 보니 <code>proxy</code>할 때 <code>pod</code>으로 보내는 요청에 관한 것이었다. 그래서 그냥 <code>HTTP</code>로 뒀다. 위처럼 하니까 잘 됐다.</p><h2>미심쩍은 것</h2><p>사실 <code>NLB</code>에서도 <a href='https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation'>방법</a>이 없는 것은 아니다. 근데 왜 해당 설정을 다 했어도 안되는지는 파악하지 못했다. 다음 과제로 남겨두거나... 다른 <code>proxy</code> 설정이 있나 찾아봐야할 것 같다.</p><h2>Reference</h2><ul><li><a href='https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html'>application_load_balancer</a></li><li><a href='https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/ingress/annotations/'>aws-load-balancer-controller</a></li><li><a href='https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html#client-ip-preservation'>client-ip-preservation</a></li><li><a href='https://www.stacksimplify.com/aws-eks/aws-alb-ingress/learn-aws-alb-ingress-on-aws-eks/'>Outline</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-02-10-deploy_k8s_using_terraform.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-02-10-deploy_k8s_using_terraform.html"/>
    <title>Terraform으로 k8s 배포하기</title>
    <updated>2021-02-10T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<h2>Terraform k8s 파일 구성</h2><p><code>kubernetes yaml</code>파일을 환경별로, 서비스별로 관리하다보니 비슷하게 생긴 <code>yaml</code>이 아주아주 많았다. <code>helm</code>을 쓸까도 했지만 테라폼으로도 원하는 걸 할 수 있어서 <code>yaml</code> 파일들을 <code>terraform</code>으로 관리하는게 어떨까 하는 생각에서 시작했다. <a href='https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs/resources/kubectl_manifest'>해당 provider</a>를 사용했다. 이유는 <code>yaml</code>을 그대로 쓸 수 있었기 때문이다. 그리고 <code>for&#95;each</code>를 적절히 사용해서 필요한 리소스별로 하나씩만 정의하고 환경별로 둘 필요는 없었다. 가장 좋은 예시인 servcie를 어떻게 만들었는지 보자.</p><pre><code class="lang-terraform">resource &quot;kubectl&#95;manifest&quot; &quot;service&quot; {
  for&#95;each  = var.service&#95;infos
  yaml&#95;body = &lt;&lt;YAML
    apiVersion: v1
    kind: Service
    metadata:
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-ssl-cert: ${each.value.ssl&#95;cert&#95;arn}
        service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http
        service.beta.kubernetes.io/aws-load-balancer-type: nlb-ip
      labels:
        app: hybrid-server-${each.value.app}
      name: hybrid-server-${each.value.app}
      namespace: ${each.value.namespace}
    spec:
      ports:
      - port: ${each.value.port}
        protocol: TCP
        targetPort: ${each.value.app}-port
      selector:
        app: hybrid-server-${each.value.app}
      sessionAffinity: None
      type: LoadBalancer
  YAML

  depends&#95;on = &#91;kubectl&#95;manifest.namespace&#93;
}

variable &quot;service&#95;infos&quot; {
  description = &quot;k8s service informations&quot;
  type        = map&#40;any&#41;
  default = {
    service-a = {
      app          = &quot;A&quot;,
      port         = 9000,
      ssl&#95;cert&#95;arn = &quot;arn...&quot;,
      namespace    = &quot;dev&quot;
    },
    service-b = {
      app          = &quot;B&quot;,
      port         = 9001,
      ssl&#95;cert&#95;arn = &quot;arn...&quot;,
      namespace    = &quot;dev&quot;
    },
  }
}
</code></pre><h2>Terraform 이용해서 연속적 배포하기</h2><p>이런식으로 사용하면 리소스들이 여러개 손쉽게 생성된다. 테라폼 코드를 작성하였으니 젠킨스와 연결해서 연속적 배포를 연결만 하면 됐다. 배포 서비스를 젠킨스를 사용하는데 <code>local</code>에서 하는 것과 다르게(로컬에서는 <code>aws configure</code>를 맞춘 후에 함) <code>terraform init</code>할 때 <code>S3</code>에서 백엔드 데이터를 가져오는데 문제가 있어서 <code>--backend-config</code>를 추가해줬다. (테라폼에서 terraform block에서는 variables을 사용할 수 없기 때문에 이런 식으로 처리해줘야한다)</p><pre><code class="lang-zsh">// 이런식으로 사용하면 profile 키, 값이 추가된다.
terraform init --backend-config='profile=${profile}'
</code></pre><p>환경별로 만들어둔 <code>tfvars</code>와, <code>docker image</code>를 변수로 넘기고, (정상적으로는 추천 방법은 아니지만) 타겟 리소스만 배포되도록 했다.</p><pre><code class="lang-zsh">terraform apply -var-file=${env}.tfvars -var image=${dockerImage} -target=kubectl&#95;manifest.deploy-admin&#91;&quot;A&quot;&#93; -auto-approve
</code></pre><p>테라폼은 기본적으로 종속 관계를 고려하여 병렬적으로 실행하게 되어있으며 잘 배포됨을 확인했다.</p><h2>번외</h2><p>groovy 문법 + 오타들 때문에 삽질을 좀 많이 했는데 까먹지 않기 위해 기록해둔다. 추가로 groovy에서 특정 디렉토리 밑에서 꼭 실행하려면</p><pre><code class="lang-groovy">dir&#40;&quot;&lt;destination&gt;&quot;&#41; {
  stage&#40;&quot;deploy&quot;&#41; {
    ...
  }
}
</code></pre><p>이런 식으로 해야하더라.</p><h2>Reference</h2><ul><li><a href='https://www.terraform.io/docs/language/settings/backends/configuration.html'>backend_configuration</a></li><li><a href='https://www.terraform.io/docs/cli/commands/apply.html'>terraform_apply</a></li><li><a href='https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs/resources/kubectl_manifest'>kubectl_provider</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-02-03-terraform_separate_configuration.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-02-03-terraform_separate_configuration.html"/>
    <title>Terraform 환경별로 사용하기</title>
    <updated>2021-02-03T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>어느정도 <code>main.tf</code> 작성을 끝내고, <code>dev</code>에서 테스트가 잘 되었다보니 이제 환경별로 이를 적용하고 관리해야했다. 우리는 <code>migration</code>을 해야했으므로 기존 인프라에 대한 정보도 <code>variable</code>로 좀 관리 되어야 할 필요가 있었다. 기타 네이밍들도 마찬가지고 뭐. 그래서 테라폼 공식 홈페이지에 <a href='https://www.terraform.io/docs/language/values/variables.html'>Input Variables</a>를 꼼꼼히 살피고, <code>tf.vars</code>를 이용해서 <code>apply</code>할 때 <code>option</code>으로 먹이는게 제일 나은 관리처럼 보였다. 해당 전략이 <code>practice</code>가 있는지 찾아보았고, 역시 <a href='https://learn.hashicorp.com/tutorials/terraform/organize-configuration#separate-configuration'>공식 문서</a>에 나와있었다.</p><p>나의 경우에는 <code>root module</code>안에서 다 관리하도록 했다. (참고로 <code>directory</code> 전략은 케바케로 엄청 많다. 상황에 따라서 적당히 골라서 쓰면 된다)</p><pre><code class="lang-tree">$ terraform
.
├── main.tf
├── variables.tf
├── dev.tfvars
├── st.tfvars
└── prod.tfvars
</code></pre><p>문서와 차이점이 있는데 <code>tfstate</code>의 경우 우리는 원격에서 관리하기로 해서 <code>tfstate</code>를 로컬에 저장해두지 않는다. 그렇기 때문에 <code>terraform workspace</code>를 사용한다거나 환경별로 디렉토리를 관리할 필요가 없게 되었다.</p><p><code>variables</code>에서 관리되는 <code>variable</code>의 <code>default</code>값은 안전하게 <code>dev</code>껄로 해두었고 그거랑 상관 없이 환경별로 <code>tfvars</code>를 정의하고 <code>terraform apply -var-file=&quot;dev.tfvars&quot;</code>처럼 <code>option</code>을 추가해서 환경별로 인프라를 관리하도록 했다. 이렇게 환경별로 특별히 다른 <code>infra</code>를 관리하는게 아닌 이상 잘 관리될 것이다.</p><p><code>variables.tf</code> 예시</p><pre><code class="lang-terraform">variable &quot;env&quot; {
  type    = string
  default = &quot;dev&quot;
}

variable &quot;region&quot; {
  type    = string
  default = &quot;us-east-1&quot;
}

variable &quot;internal&#95;route53&#95;id&quot; {
  type    = string
  default = &quot;ZAADSFADSFADSF&quot;
}

variable &quot;internal&#95;vpc&#95;id&quot; {
  type    = string
  default = &quot;vpc-adslfkjasdkfl122a&quot;
}

variable &quot;internal&#95;security&#95;group&#95;id&quot; {
  type    = string
  default = &quot;sg-aldksfjalksdfaskl&quot;
}
</code></pre><p><code>dev.tfvars</code> 예시</p><pre><code class="lang-tfvars">env                        = &quot;dev&quot;
region                     = &quot;us-east-1&quot;
internal&#95;route53&#95;id        = &quot;ZAADSFADSFADSF&quot;
internal&#95;vpc&#95;id            = &quot;vpc-adslfkjasdkfl122a&quot;
internal&#95;security&#95;group&#95;id = &quot;sg-aldksfjalksdfaskl&quot;
</code></pre><h2>Reference</h2><ul><li><a href='https://learn.hashicorp.com/tutorials/terraform/organize-configuration#separate-configuration'>separate_configuration</a></li><li><a href='https://www.terraform.io/docs/language/values/variables.html'>input_variables</a></li></ul>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-01-31-fargate_on_eks_using_terraform_from_kops.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-01-31-fargate_on_eks_using_terraform_from_kops.html"/>
    <title>Terraform으로 kops에서 eks fargate로 갈아타기</title>
    <updated>2021-01-31T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>제목이 참 길다. 그만큼 글도 길어질 것 같다. 2021년에 개발하고 있는 서비스에서 몇가지 큰 줄기 및 목표가 있는데 그 중의 하나가 <code>kops</code>에서 <code>fargate on EKS</code>로 <code>migration</code>하는 것이다. 마침 새해 초반에 밀린 피처 개발이 많이 없고 간단한거라서 빨리 해버리고 남는 시간에 하기로 마음 먹었다. 그 전에 시간 날 때마다 조금씩 찾아 본 <code>eks</code> 관련 지식들과 <code>Terraform</code>을 꼭 사용해서 성공적으로 서버를 옮기고자 한다.</p><p>현재 우리 서비스의 인프라 상태를 알아보자.</p><ul><li>약 2년 전에 설치된 <code>k8s</code> <code>1.11</code> 버전, <code>kops</code> 사용 중. <code>nodes</code>들은 <code>ec2 on AWS</code> 있음</li><li>글을 쓰는 시점에 <code>k8s</code>는 <code>1.19</code>까지 릴리즈. 버전을 7번이나 올려야하는 귀찮음</li><li><code>k8s</code> <code>1.11</code> → <code>1.12</code> 올릴 때 노드 하나가 이상해져서 서버 장애를 일으킨 경험이 본인이 있음</li><li>심지어 이 인프라를 셋업한 개발자는 옆에 팀 개발자</li><li>현재 인프라 구조를 설명하는 문서가 없고 다 아는 사람은 잘 없음</li></ul><p>인프라 관리를 우리 팀에서는 아무도 안하다보니 인프라 버전이 너무 노후되어 있었고 어떻게 구성되어있는지 관리도 되고 있지 않았다. 그래서 뭔가를 만지려고 해도 리스크도 컸고 솔직히 물어볼 사람도 거의 없었다. (옆에 팀에 맨날 물어보기도 미안하니...) 그래서 현재 구조를 상세히 파악하고 번거로운 서버관리를 하느니 <code>farget on EKS</code>라는 걸 주워들은 기억도 있고 하니 클러스터, 노드 관리는 <code>AWS</code>에 넘기고 도커 이미지만 관리하고 싶었다.</p><p>그럼 서버 다운타임 없이 이전하는게 가장 큰 목표이므로, 이에 대해서 대충 계획을 세웠다.</p><ul><li><code>Terraform</code>을 <code>꼭</code> 사용해서 <code>migration</code>을 한다 → 그러지 않으면 나중의 누군가는 우리가 겪은 것과 같이 인프라 구성에 대해서 이해하고 유지/보수하기 힘들다</li><li><code>EKS</code>와 <code>Kops</code> 클러스터에 모든 서비스를 안정적으로 띄운 후에 <code>blue/green</code> 배포 방식과 유사하게 <code>weighted routing policy</code>를 사용해서 점진적으로 이전한다</li><li><code>dev</code>, <code>staging</code> 환경에서 안정적으로 테스트를 마친 후에 <code>production</code>에 적용한다</li></ul><p>간단한게 최고이기 때문에 큰 그림은 저렇게 잡았다.</p><blockquote><p> <strong><i>(쉬어가기) kops vs. eks?</i></strong> 검색만 해도 많이 나오지만 우리가 <code>kops</code>를 사용하지 않기로한 이유는 다음 몇가지 이유 때문이다. 1. 어차피 <code>ec2</code>위에 올리기 때문에 딱히 <code>kops</code>를 왜 써야하는지 모르겠다 (처음 <code>kops</code> 쓸 때는 <code>eks</code>가 없었음) 2. 클러스터 관리를 하기 싫었다 3. 더 작은 단위로 오토스케일링을 할 수 있어서 비용 절감을 기대했다 4. 조금 더 트렌디하다 (심지어 <code>fargate</code>를 쓰기 때문...) </p></blockquote><p>그래도 시작에 앞서서 선수 지식이나 허들이 좀 있었는데(<code>Terraform</code>, <code>AWS 관련 지식</code>, <code>k8s</code>) 부딪혀가면서 더 많이 배워갔다.</p><p>테라폼 공식 홈페이지 기본 튜토리얼을 따라가면서 입문하기 시작했고 구글링을 통해서 많은 예시들을 찾아보면서 삽질을 했는데 처음 <code>module</code>을 찾았을 때 엄청나게 신났던 기억이 갑자기 난다. 여하튼, 대충 그럼 어떻게! 실제로! 진행했는지 코드와 함께 살펴보자.</p><p>일단 우리는 시간이 촉박했으므로, <code>eks policy</code> 관련된 <code>iam user</code>를 그냥 콘솔을 이용해서 만들고 연결해서 사용했다. 이 부분은 현재 메뉴얼하게 사용하고 있다.</p><h2>Code</h2><h3>Terraform block</h3><pre><code class="lang-terraform">terraform {
  backend &quot;s3&quot; {
    bucket         = &quot;my-tfstate&quot;
    key            = &quot;main/terraform.tfstate&quot;
    region         = &quot;us-east-1&quot;
    encrypt        = true
    dynamodb&#95;table = &quot;TerraformStateLock&quot;
    acl            = &quot;bucket-owner-full-control&quot;
  }
  required&#95;version = &quot;&gt;= 0.14.4&quot;
  required&#95;providers {
    aws = {
      source  = &quot;hashicorp/aws&quot;
      version = &quot;&gt;= 3.3.0&quot;
    }
    kubernetes = {
      source  = &quot;hashicorp/kubernetes&quot;
      version = &quot;&gt;= 2.0&quot;
    }
  }
}
</code></pre><p><code>terraform block</code>에서 사용할 <code>providers</code>를 정의하고 <code>backend</code>도 연결했다. 처음 코드 볼 때 <code>backend</code>는 <code>tfstate</code>를 어디에 저장하고 어디서 가져올지에 대한 것인데, 우리는 <code>s3</code>에서 관리할 것이므로 위와 같이 정의했다. 로컬에서 관리하지 않고 원격에서 관리되는 <code>tfstate</code>를 사용하기 위함이다.</p><h3>tfstate 관리</h3><p>바로 직전에 사용한 <code>backend</code>를 위해서는 <code>s3</code>를 정의해야한다. 근데 인프라를 원격으로 관리한다는 것은 여럿이서 동시에 작업이 될 수 있고 이에 대한 리스크가 있기 때문에 이를 <code>DynamoDB Table</code>을 이용해, <code>lock</code>을 관리해서 동시에 작업하는 것을 막을 수 있다.</p><pre><code class="lang-terraform">resource &quot;aws&#95;dynamodb&#95;table&quot; &quot;terraform&#95;state&#95;lock&quot; {
  name           = &quot;TerraformStateLock&quot;
  read&#95;capacity  = 5
  write&#95;capacity = 5
  hash&#95;key       = &quot;LockID&quot;

  attribute {
    name = &quot;LockID&quot;
    type = &quot;S&quot;
  }
  lifecycle {
    prevent&#95;destroy = true
  }
}

resource &quot;aws&#95;s3&#95;bucket&quot; &quot;terraform-state&quot; {
  bucket = &quot;my-tfstate&quot;
  acl    = &quot;private&quot;
  versioning {
    enabled = true
  }
  tags = {
    Name = &quot;terraform state&quot;
  }
  logging {
    target&#95;bucket = aws&#95;s3&#95;bucket.logs.id
    target&#95;prefix = &quot;log/&quot;
  }
  lifecycle {
    prevent&#95;destroy = true
  }
}
</code></pre><p><code>resource</code>에 사용할 수 있는 <code>meta-argument</code> 중에 <a href='https://www.terraform.io/docs/language/meta-arguments/lifecycle.html'>lifecycle</a>이라는게 있는데, 말그대로 리소스들의 라이프사이클에 대한 제어를 약간 할 수 있다. 여기서는, <code>s3</code>나 <code>db</code> 관련된 정보는 날리지 못하도록 했다. 이럴 경우에 <code>terraform destroy</code>를 할 경우에 에러메시지를 출력할 것이다. (사고 방지)</p><h3>vpc</h3><p><code>vpc</code>는 <a href='https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest'>terraform vpc module</a>을 사용했다. 아주아주 편리 했다. 문서를 쥐잡듯이 꼼꼼히 읽어보면 쓸만한 옵션들도 많고 사실 적당한 예시만 봐도 필요한 vpc 세팅들에 대해서 잘 안내 되어있다. 왜 진작 몰랐을까? 라는 생각이 들 정도였으니. 인프라를 코드 관리한다는게 뭔지 느낀 시점이다.</p><pre><code class="lang-terraform">data &quot;aws&#95;availability&#95;zones&quot; &quot;available&quot; {
  exclude&#95;names = &#91;&quot;us-east-1c&quot;&#93;
}

module &quot;eks&#95;vpc&quot; {
  source  = &quot;terraform-aws-modules/vpc/aws&quot;
  version = &quot;2.66.0&quot;

  name                 = &quot;my-eks-vpc&quot;
  cidr                 = &quot;172.16.0.0/16&quot;
  azs                  = data.aws&#95;availability&#95;zones.available.names
  private&#95;subnets      = &#91;&quot;172.16.1.0/24&quot;, &quot;172.16.2.0/24&quot;, &quot;172.16.3.0/24&quot;&#93;
  public&#95;subnets       = &#91;&quot;172.16.4.0/24&quot;, &quot;172.16.5.0/24&quot;, &quot;172.16.6.0/24&quot;&#93;
  enable&#95;nat&#95;gateway   = true
  single&#95;nat&#95;gateway   = true
  enable&#95;dns&#95;hostnames = true

  public&#95;subnet&#95;tags = {
    &quot;kubernetes.io/cluster/${local.cluster&#95;name}&quot; = &quot;shared&quot;
    &quot;kubernetes.io/role/elb&quot;                      = &quot;1&quot;
  }

  private&#95;subnet&#95;tags = {
    &quot;kubernetes.io/cluster/${local.cluster&#95;name}&quot; = &quot;shared&quot;
    &quot;kubernetes.io/role/internal-elb&quot;             = &quot;1&quot;
  }
}
</code></pre><p>필수로 주어야하는 입력 파라미터들을 필요에 맞게 세팅한다. <code>private</code>, <code>public</code> 서브넷들을 잘 구성한다. <code>us-east-1</code>을 쓴다면 현재 <code>us-east-1c</code>의 <code>AZ</code>를 eks 클러스터의 서브넷으로 사용할 수 없어서 제외시켰다.</p><blockquote><p> <strong><i>(쉬어가기) data vs. resources</i></strong>: 공식 홈페이지를 찾아보면 잘 정의되어있다. 간단하게 이해하면 <code>Resource</code>는 인프라를 <strong>설명하고, 만들고, 업데이트하고 삭제</strong>하고 등등에 사용하는 것이고, <code>Data</code>는 정의된 리소스들을 <strong>조회</strong>하는데 사용하는 것이라고 생각하니 이해가 잘 됐다. <em>Resources</em> are the most important element in the Terraform language. Each resource block describes one or more infrastructure objects, such as virtual networks, compute instances, or higher-level components such as DNS records. <em>Data sources</em> allow data to be fetched or computed for use elsewhere in Terraform configuration. Use of data sources allows a Terraform configuration to make use of information defined outside of Terraform, or defined by another separate Terraform configuration. </p></blockquote><p>사실 나의</p><h3>eks</h3><p><code>eks</code>는 <a href='https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest'>terraform eks module</a>을 사용했다. 아주아주 편리 했다. 바로 위의 <code>vpc</code>에서 만든 네트워크를 사용하고 있다. 그리고 <code>faragate profile</code>을 바로 정의해서 사용했다. <code>profile</code>에 해당하는 <code>pod</code>은 <code>fargate</code>에서 띄우겠다는 뜻이라서 필요한 3가지 <code>namespace</code>의 것들은 다 띄우도록 했다.</p><pre><code class="lang-terraform">
data &quot;aws&#95;eks&#95;cluster&quot; &quot;cluster&quot; {
  name = module.eks.cluster&#95;id
}

data &quot;aws&#95;eks&#95;cluster&#95;auth&quot; &quot;cluster&quot; {
  name = module.eks.cluster&#95;id
}

provider &quot;kubernetes&quot; {
  host                   = data.aws&#95;eks&#95;cluster.cluster.endpoint
  cluster&#95;ca&#95;certificate = base64decode&#40;data.aws&#95;eks&#95;cluster.cluster.certificate&#95;authority.0.data&#41;
  token                  = data.aws&#95;eks&#95;cluster&#95;auth.cluster.token
}

module &quot;eks&quot; {
  source          = &quot;terraform-aws-modules/eks/aws&quot;
  cluster&#95;name    = local.cluster&#95;name
  cluster&#95;version = &quot;1.18&quot;
  subnets         = module.eks&#95;vpc.private&#95;subnets
  enable&#95;irsa     = true

  tags = {
    Environment = &quot;dev&quot;
    GithubRepo  = &quot;terraform-aws-eks&quot;
    GithubOrg   = &quot;terraform-aws-modules&quot;
  }

  vpc&#95;id = module.eks&#95;vpc.vpc&#95;id

  fargate&#95;profiles = {
    server = {
      namespace = &quot;dev&quot;
      tags = {
        env = &quot;dev&quot;
      }
    },
    kube-system = {
      namespace = &quot;kube-system&quot;
    },
    kubernetes-dashboard = {
      namespace = &quot;kubernetes-dashboard&quot;
    }
  }

  map&#95;roles = var.map&#95;roles
  map&#95;users = var.map&#95;users
}

resource &quot;null&#95;resource&quot; &quot;core&#95;dns&#95;only&#95;fargate&quot; {
  provisioner &quot;local-exec&quot; {
    command = &lt;&lt;EOF
      kubectl patch deployment coredns -n kube-system --type json \
      -p='&#91;{&quot;op&quot;: &quot;remove&quot;, &quot;path&quot;: &quot;/spec/template/metadata/annotations/eks.amazonaws.com&#126;1compute-type&quot;}&#93;'
    EOF
  }

  depends&#95;on = &#91; module.eks.cluster&#95;id &#93;
}
</code></pre><p><code>eks</code>를 생성하는 것 왜에 <code>null&#95;resource</code>를 사용한 리소스가 있는데, 이게 뭐냐면, 리소스를 생성하지 않으면서 <code>provisioner</code> 등을 사용할 때 쓴다. 여기서 실행한 커맨드는 <code>coredns</code>에서 <code>fargate</code>만 사용할 때 저 <code>annotation</code>을 지워줘야한다. 추가적으로 <code>terraform</code>은 리소스들이 만들어질 때 순서(chain)이 있는데 이것들을 명시적(implicit)으로 설정하고 싶으면 <a href='https://www.terraform.io/docs/language/meta-arguments/depends_on.html'>depends_on</a>을 사용한다. 즉, <code>eks module</code>이 정상적으로 생성한 이후에 <code>null resource</code>를 실행하길 명시적으로 나타낸 것이다.</p><h3>network</h3><p>기존 인프라에서 <code>private</code>하게, <code>internal</code>하게 통신해야하는 <code>backend</code> 서비스들이 있고, 이들이 포함된 <code>security group</code>에 새로 만든 <code>eks용 private subnet</code>과는 통신이 가능해야했다. 그래서 <code>vpc peering</code>을 만들고 <code>route table</code>에 연결하고 <code>route53 zone</code>에 <code>eks vpc</code>를 추가해줘야했고, <code>internal-sg</code>에 <code>eks sg</code>들이 접근가능하게 <code>ingress rule</code>을 추가했다.</p><pre><code class="lang-terraform">// 기존 인프라에서 조회
data &quot;aws&#95;vpc&quot; &quot;internal&quot; {
  id = var.internal&#95;vpc&#95;id
}

// 기존 인프라에서 조회
data &quot;aws&#95;route&#95;table&quot; &quot;internal&quot; {
  vpc&#95;id = var.internal&#95;vpc&#95;id
}

// pcx 생성
resource &quot;aws&#95;vpc&#95;peering&#95;connection&quot; &quot;eks&#95;to&#95;internal&quot; {
  peer&#95;vpc&#95;id = module.eks&#95;vpc.vpc&#95;id    # Accepter - eks
  vpc&#95;id      = data.aws&#95;vpc.internal.id # Requester - internal
  auto&#95;accept = true


  tags = {
    Name = &quot;VPC peering between eks and internal&quot;
  }
}

data &quot;aws&#95;route&#95;table&quot; &quot;eks&#95;private&quot; {
  vpc&#95;id         = module.eks&#95;vpc.vpc&#95;id
  route&#95;table&#95;id = module.eks&#95;vpc.private&#95;route&#95;table&#95;ids&#91;0&#93;
}

// route 생성 &#40;route table에 연결 - 쌍방 연결 해야함&#41;
resource &quot;aws&#95;route&quot; &quot;internal&#95;route&quot; {
  route&#95;table&#95;id            = data.aws&#95;route&#95;table.internal.id
  destination&#95;cidr&#95;block    = module.eks&#95;vpc.vpc&#95;cidr&#95;block
  vpc&#95;peering&#95;connection&#95;id = aws&#95;vpc&#95;peering&#95;connection.eks&#95;to&#95;internal.id
}

// route 생성 &#40;route table에 연결- 쌍방 연결 해야함&#41;
resource &quot;aws&#95;route&quot; &quot;eks&#95;route&quot; {
  route&#95;table&#95;id            = data.aws&#95;route&#95;table.eks&#95;private.id
  destination&#95;cidr&#95;block    = data.aws&#95;vpc.internal.cidr&#95;block
  vpc&#95;peering&#95;connection&#95;id = aws&#95;vpc&#95;peering&#95;connection.eks&#95;to&#95;internal.id
}

resource &quot;aws&#95;route53&#95;zone&#95;association&quot; &quot;eks&quot; {
  zone&#95;id = var.internal&#95;route53&#95;id
  vpc&#95;id  = module.eks&#95;vpc.vpc&#95;id
}

resource &quot;aws&#95;security&#95;group&#95;rule&quot; &quot;eks&#95;cluster&#95;for&#95;internal&quot; {
  type                     = &quot;ingress&quot;
  from&#95;port                = 6379
  to&#95;port                  = 6379
  protocol                 = &quot;tcp&quot;
  security&#95;group&#95;id        = var.internal&#95;security&#95;group&#95;id
  source&#95;security&#95;group&#95;id = module.eks.cluster&#95;security&#95;group&#95;id
  description              = &quot;redis from eks&quot;
}

resource &quot;aws&#95;security&#95;group&#95;rule&quot; &quot;eks&#95;workers&#95;for&#95;internal&quot; {
  type                     = &quot;ingress&quot;
  from&#95;port                = 6379
  to&#95;port                  = 6379
  protocol                 = &quot;tcp&quot;
  security&#95;group&#95;id        = var.internal&#95;security&#95;group&#95;id
  source&#95;security&#95;group&#95;id = module.eks.worker&#95;security&#95;group&#95;id
  description              = &quot;redis from eks&quot;
}

resource &quot;aws&#95;security&#95;group&#95;rule&quot; &quot;cluster&#95;primary&quot; {
  type                     = &quot;ingress&quot;
  from&#95;port                = 6379
  to&#95;port                  = 6379
  protocol                 = &quot;tcp&quot;
  security&#95;group&#95;id        = var.internal&#95;security&#95;group&#95;id
  source&#95;security&#95;group&#95;id = module.eks.cluster&#95;primary&#95;security&#95;group&#95;id
  description              = &quot;redis from eks&quot;
}

resource &quot;aws&#95;security&#95;group&#95;rule&quot; &quot;default&#95;eks&#95;vpc&quot; {
  type                     = &quot;ingress&quot;
  from&#95;port                = 6379
  to&#95;port                  = 6379
  protocol                 = &quot;tcp&quot;
  security&#95;group&#95;id        = var.internal&#95;security&#95;group&#95;id
  source&#95;security&#95;group&#95;id = module.eks&#95;vpc.default&#95;security&#95;group&#95;id
  description              = &quot;redis from eks&quot;
}
</code></pre><p>이렇게까지 하면 네트워크 구성은 다 끝이 났다. 정리하고 보니 깔끔하고 코드로 보니 명확했지만 이를 말로 적고, <code>cli</code>나 <code>console</code>로 작업한 것을 캡처해서 설명하고 했으면 관리하기도 힘들고 수정해야할 때의 <del>고통까지 생각하니 아주 끔찍하다</del>.</p><h3>kubernetes</h3><p>기존 <code>k8s yaml</code>을 그대로 사용하고자 했다. <code>Network Load Balancer</code>를 사용하고 있었고, 이를 <code>eks</code>에서 사용하기 위해서는 <code>aws lb controller</code>가 필요해 몇가지 세팅을 한다. 여기서도 모듈 하나를 사용하는데 <a href='https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest'>iam module</a>을 썼다. 이것들이 참 번거로웠고 아직도 어떻게 동작하는지 솔직히 잘 모르는 부분이기도 한데, 이것과 이것을 보고 작업을 한 후에 코드로 한번 옮겨봤다.</p><pre><code class="lang-terraform">resource &quot;aws&#95;iam&#95;policy&quot; &quot;aws&#95;lb&#95;controller&quot; {
  name        = &quot;eks&#95;lb&#95;controller&quot;
  path        = &quot;/&quot;
  policy      = file&#40;&quot;./aws&#95;lb&#95;iam&#95;policy.json&quot;&#41;
  description = &quot;aws eks loadbalancer controller iam policy made by terraform&quot;
}


module &quot;aws&#95;eks&#95;lb&#95;controller&#95;role&quot; {
  source  = &quot;terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc&quot;
  version = &quot;&#126;&gt; 3.6&quot;

  create&#95;role = true

  role&#95;name = &quot;AmazonEKSLoadBalancerControllerRole&quot;

  tags = {
    Role = &quot;AmazonEKSLoadBalancerControllerRole&quot;
  }

  provider&#95;url = trimprefix&#40;module.eks.cluster&#95;oidc&#95;issuer&#95;url, &quot;https://&quot;&#41;

  role&#95;policy&#95;arns = &#91;
    aws&#95;iam&#95;policy.aws&#95;lb&#95;controller.arn,
  &#93;

  number&#95;of&#95;role&#95;policy&#95;arns = 1

  oidc&#95;fully&#95;qualified&#95;subjects = &#91;&quot;system:serviceaccount:kube-system:${local.aws&#95;lb&#95;controller&#95;name}&quot;&#93;
}

resource &quot;kubernetes&#95;service&#95;account&quot; &quot;aws&#95;load&#95;balancer&#95;controller&#95;sa&quot; {
  metadata {
    labels = {
      &quot;app.kubernetes.io/component&quot; = &quot;controller&quot;,
      &quot;app.kubernetes.io/name&quot;      = local.aws&#95;lb&#95;controller&#95;name
    }
    name      = local.aws&#95;lb&#95;controller&#95;name
    namespace = &quot;kube-system&quot;
    annotations = {
      &quot;eks.amazonaws.com/role-arn&quot; = module.aws&#95;eks&#95;lb&#95;controller&#95;role.this&#95;iam&#95;role&#95;arn
    }
  }
}

resource &quot;aws&#95;iam&#95;policy&quot; &quot;additional&#95;aws&#95;lb&#95;controller&quot; {
  name        = &quot;AWSLoadBalancerControllerAdditionalIAMPolicy&quot;
  path        = &quot;/&quot;
  policy      = file&#40;&quot;./iam&#95;policy&#95;v1&#95;to&#95;v2&#95;additional.json&quot;&#41;
  description = &quot;aws load balancer controller additional iam policy&quot;
}

resource &quot;aws&#95;iam&#95;role&#95;policy&#95;attachment&quot; &quot;lb&#95;additional&#95;policy&#95;attach&quot; {
  role       = module.aws&#95;eks&#95;lb&#95;controller&#95;role.this&#95;iam&#95;role&#95;name
  policy&#95;arn = aws&#95;iam&#95;policy.additional&#95;aws&#95;lb&#95;controller.arn

  provisioner &quot;local-exec&quot; {
    command = &lt;&lt;EOF
      kubectl apply -k &quot;github.com/aws/eks-charts/stable/${local.aws&#95;lb&#95;controller&#95;name}//crds?ref=master&quot;
    EOF
  }

  provisioner &quot;local-exec&quot; {
    command = &quot;helm repo add eks https://aws.github.io/eks-charts&quot;
  }

  provisioner &quot;local-exec&quot; {
    command = &lt;&lt;EOF
      helm upgrade -i ${local.aws&#95;lb&#95;controller&#95;name} eks/${local.aws&#95;lb&#95;controller&#95;name} \
        --set clusterName=${local.cluster&#95;name} \
        --set region=${var.region} \
        --set vpcId=${module.eks&#95;vpc.vpc&#95;id} \
        --set serviceAccount.create=false \
        --set serviceAccount.name=${local.aws&#95;lb&#95;controller&#95;name} \
        -n kube-system
    EOF
  }
}
</code></pre><h2>실제 Migration</h2><p>위처럼 설정하면 이제 거의 기존에 있던 각종 <code>kubernetes yaml</code> 파일들을 <code>cluster</code>에 다 올려주기만 하면 된다. <code>Weighted routing</code> 비율은 테스트 단계에서는 5:5 정도로 시작해서 점진적으로 새로운 클러스터에 비율을 늘려준다. 아참, <code>ec2 auto scaling</code> 같은 경우에는 따로 할 필요가 없고 <code>hpa</code>를 사용하기만 하면 되는데 이는 간단하다.</p><h2>정리하며</h2><p><code>migration</code>이 새로 만드는 것보다 훨씬 귀찮고 어려운 것 같다. 그래서 더 많이 배운 것 같기도 하고. 그리고 모든 버전은 되도록 최신 버전으로 사용했다. 버전 올리는 일도 굉장히 귀찮고 관리 포인트이기 때문에 <code>Terraform</code>도 <code>0.14</code>를 썼고 <code>EKS</code>도 지원하는 최신 쿠버네티스 버전인 <code>1.18</code>을 사용했다. 다른 것들도 비슷하다. 그리고 어느정도 <code>manual</code>하게 사용하는 부분도 많이 있는데 이를 나중에는 다 <code>code</code>로 옮기면 좋을 것 같다. 더 공유할만한 내용이 있으면 추가로 꼭 적는 것을 스스로에게 약속한다.</p><h2>Reference</h2><ul><li><a href='https://www.terraform.io/'>테라폼 공식 사이트</a></li><li><a href='https://blog.outsider.ne.kr/1290'>tfstate 관리</a></li><li>[개괄적 이해에 도움된 글][개괄]</li><li><a href='https://registry.terraform.io/modules/terraform-aws-modules/iam/aws/latest'>iam 모듈</a></li><li><a href='https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest'>vpc 모듈</a></li><li><a href='https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest'>eks 모듈</a>  </li></ul>[개괄]: https://www.44bits.io/ko/post/terraform<i>introduction</i>infrastrucute<i>as</i>code]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-01-28-working_at_home.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-01-28-working_at_home.html"/>
    <title>완전 재택근무 7개월차 근황</title>
    <updated>2021-01-28T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>2020년 7월에 새로운 회사로 옮겼다. 업무적으로는 비슷하다. 조금 더 무언가를 하거나 (레거시 박살내기, 라이브 대응), 조금 덜 하거나 (그냥 심심한 기능 개발) 혹은 최근처럼 완전 갈아 엎거나 (<code>kops</code>에서 <code>fargate on EKS</code>로 갈아타기, 그와 동시에 <code>Terraform</code> 사용하기) 정도의 일을 하고 있다. 근데 가장 큰 차이점은 일을 하는 장소다.</p><p>그 전에는 100% 회사에 가서 일을 했다. 업무용으로 랩탑이 아닌 데탑을 받아서 일을 했으므로 그게 당연했다. 사람도 많이 없어서 일하기 조용하고 그거에 대한 거부감도 없었다. 우리는 항상 그렇게 일을 해왔으니까. 하지만 지금 회사에서는 재밌게도 (물론 외부 영향 - 코로나 바이러스 때문이지만) 입사 이후 처음 2주 빼고는 다 재택근무로 지내고 있다. 같은 팀인데도 실제 얼굴은 몇번 보지 못했다. 뭐 어차피 영국에 있는 팀원들은 언제 볼지도 모르겠다만은. 여차저차해서, 재택근무를 하면서 느낀 재택근무의 장점과 단점, 효율적으로 재택근무를 하는 법에 대해서 적어보자 한다.</p><h2>재택근무의 장점</h2><ul><li>출퇴근을 1초만에 할 수 있다</li><li>가족들과 집에서 보내는 시간이 길어진다</li><li>아침 먹을 시간이 생기고 저녁 먹을 시간이 생긴다</li><li>잡담이나 최소한의 사회생활을 위한 시간을 사용하지 않아서 시간이 많다</li><li>집중할 수 있는 시간이 늘어난다</li><li>쉴 때 편하게 쉴 수 있다</li></ul><h2>재택근무의 단점</h2><ul><li>혼자 있는 경우에 하루종일 말을 안할 때가 많다</li><li>휴대폰, 컴퓨터 등 전자기기 등 쉴 때도 이들과 함께 하다보니 여유가 없어진다</li><li>인간 대 인간으로서의 팀원들간의 유대관계가 약하다</li><li>바로바로 답장을 가끔 받기 어려울 때가 있다</li><li>회의할 때 오디오가 물려서 진행되기 힘들다 보니 조금 길어지는 경향이 있다</li></ul><h2>더 나은 근무 환경</h2><ul><li>의자와 책상이 조금 더 편리하게 세팅하면 좋을 것 같다</li><li>마이크, 스피커 세팅이 조금 더 잘 되어 있어서 귀 피로감을 줄였으면 좋겠다</li><li>칠판 같은 걸 가끔 못쓰다보니 생산성 향상 도구에 대해서 더 신경을 써야할 것 같다</li></ul><p>익히 알려져있는 출퇴근의 경계가 없어진다고 하는 점은 나는 딱히 느낀 적은 없다. 출근을 하더라도 비슷하기 때문... 혹은 집에 일보다 더 중요한 가족이 있으면 어쩔 수 없이 일을 멀리 해야하기 때문이다. 재택근무가 길어지다보니 재택 근무 관련된 팁이나 블로그 글 등이 쏟아져나오는데 최근에 본 글 중 재미난 것은 <code>Social Gathering</code> 같은 걸 가끔 하는게 좋다고 하더라. 우리 팀에서도 2주마다 하고 있는데 괜찮은 것 같다. 주제로는 가볍게 얘기하는 것도 좋지만 가끔은 자기 집들이도 한다고 하는데, 다음에 약간 게임 방송이나 요리 방송을 하는 것도 괜찮을 것 같다. 2월 회식 때는 한번 해볼까도 생각 중.</p>]]></content>
  </entry>
  <entry>
    <id>https://github.com/borkdude/quickblog/2021-01-25-long_time_no_see.html</id>
    <link href="https://github.com/borkdude/quickblog/2021-01-25-long_time_no_see.html"/>
    <title>진짜 오랜만에 블로그에 글 쓴다</title>
    <updated>2021-01-25T23:59:59+00:00</updated>
    <content type="html"><![CDATA[<p>요즘 코로나가 한국에 보급된(?)지 1년 정도 지났는데 그 사이에 많은 일이 생겼다. 몸도 안좋아서 병원도 다녔고, 다니고 있고, 좀 쉬기도 했고 회사도 옮겼다. 회사 옮기면서 몇가지 생긴 변화가 있는데 대충 기억해두기 위해 적어둔다.</p><ul><li>재택 근무의 장점과 단점</li><li>스스로 발전해 나간다는 것</li><li>가능한 작은 조직에서 일하자</li><li>팀을 만들어 나간다는 것</li></ul><p>테크니컬한 이야기는 테크파트에서 하기로 하고 잡담하는 곳에서는 위에 주제들로 글을 써내려가보자. 언제나 그렇듯이 꾸준히. 필요하면 영어로도 쓸까 한다.</p>]]></content>
  </entry>
</feed>
