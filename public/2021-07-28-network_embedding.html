<!DOCTYPE html>
<html lang="en">
  <head>
    <title>node2vec</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link type="application/atom+xml" rel="alternate" href="atom.xml" title="node2vec">
    <link rel="stylesheet" href="style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-clojure.min.js"></script>
    <script type="text/javascript" src="https://livejs.com/live.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css">



    <!-- Social sharing (Facebook, Twitter, LinkedIn, etc.) -->
    <meta name="title" content="node2vec">
    <meta name="twitter:title" content="node2vec">
    <meta property="og:title" content="node2vec">
    <meta property="og:type" content="website">


    <meta name="twitter:url" content="https://github.com/borkdude/quickblog/2021-07-28-network_embedding.html">
    <meta property="og:url" content="https://github.com/borkdude/quickblog/2021-07-28-network_embedding.html">


    <meta name="twitter:card" content="summary">



  </head>
  <body>

    <div class="site-header">
      <div class="wrapper">
        <div class="site-nav">
          <a class="page-link" href="archive.html">Archive</a>
          <a class="page-link" href="tags/index.html">Tags</a>
          <a class="page-link" href="">Discuss</a>
	  <a class="page-link" href="atom.xml">
            Feed
          </a>
	  
	  
        </div>
        <div>
          <h1 class="site-title">
            <a class="page-link" href="index.html">Jungwoo Kim</a>
          </h1>
	  <p>A blog about blogging quickly</p>
        </div>
      </div>
    </div>

    <div class="wrapper">

      <h1>
  
  node2vec
  
</h1>
<p>아래 글은 <a href='blog'>여기</a>를 제 언어로 번역한 글입니다. 글 아래에 약간의 <a href='#배경지식'>필요 지식</a>에 대한 추가 설명도 있습니다.</p><h1>동기</h1><p>데이터 과학자라면 임베딩이라는 말을 자주 듣는데, 대부분 NLP분야에서 들어봤을 것이다. 이 번거로운 embedding 녀석..</p><h1>Embedding process 임베딩 과정</h1><p>그래서 어떻게 동작할까? 임베딩 학습 방식은 <a href='#skip-gram'><code>skip-gram</code></a> 모델을 사용하는 <code>word2vec</code> 임베딩 학습 방식이랑 똑같다. 만약에 여러분이 <code>word2vec skip-gram model</code>이 익숙하면 좋겠지만 아니라면, <a href='http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/'>여기 설명</a>을 보는 걸 추찬한다.</p><p>내가 <code>node2vec</code>을 설명하는 것에 대해 생각할 수 있는 가장 자연스러운 방법은 <code>node2vec</code>이 <code>corpus&#40;말뭉치&#41;</code>를 생성하는 방법을 설명하는 것이다. 그리고 만약 우리가 <code>word2vec</code>을 이해한다면, 우리는 이미 <code>corpus</code>를 어떻게 <code>embed</code>하는지 알고 있는 것이다.</p><p>그럼 <code>graph</code>에서 어떻게 <code>corpus</code>를 뽑아낼 수 있을까? 그게 <code>node2vec</code>의 혁신적인 부분이고, 그것은 멋진 <code>sampling strategy&#40;샘플링 전략&#41;</code>에서 나온다.</p><p>입력된 그래프로부터 <code>corpus</code>를 생성하기 위해, 최대 out-degree가 1인 directed acyclic graphs를 생각해보자. 각각의 단어가 노드이고 문장의 다음 단어를 가르킨다고 생각하면 된다.</p><p><img src="https://miro.medium.com/max/982/1*oEuJHzd3iPpord7sFR1AhQ.png" alt="Setence in a graph representation" /></p><p>이 방법으로는 word2vec이 이미 임베드된 그래프가 된 것을 볼 수 있지만 많은 경우 중 하나일 뿐이다. 하지만 대부분 그래프들은 (un)directed, (un)weighted, (a)cyclic 하고, 대부분 text보다 구조가 복잡하다.</p><p>이를 해결하기 위해서, node2vec은 directed acyclic subgraphs를 샘플링 하기 위해 하이퍼파라미터에 의해 수정 가능한 (tweakable by hyperparameters) 샘플링 전략을 사용한다. 이는 각각의 노드들의 랜덤 워크(random walks)에 의해 생성된다. 간단한 듯?</p><p>이를 생성하기 전에 먼저 시간화를 한번 해봅시다.</p><p><img src="https://miro.medium.com/max/2000/1*3pmstIOig4Qc3lrQS4xrNg.png" alt="node2vec embedding process" /></p><h2>샘플링 전략</h2><p>우리는 큰 그림을 그렸고, 이제 조금 더 깊게 파고 들어봅시다. <code>Node2Vec</code>의 샘플링 전략은 4가지 arguments를 가지는데,</p><ul><li><strong>Number of walks(워크의 수)</strong>: 그래프의 각 노드에서 생성될 랜덤 워크 수</li><li><strong>Walk length(워크의 길이)</strong>: 각 랜덤 워크에 몇개의 노드가 있는 지</li><li><strong>P</strong>: Return(돌아가는) hyperparameter</li><li><strong>Q</strong>: Inout(들어가고 나오는) hyperparameter</li></ul><p>그리고 표준 skip-gram parameters(context window size, number of iterations, etc)등을 가진다.</p><p>처음 2개 hyperparameter들은 설명 그대로이다. 랜덤 워크 생성을 위한 알고리즘은 각 노드를 방문하면서 <code>number of walk</code>와 <code>walk length</code>를 생성한다.</p><p>Q와 P는 시각적으로 설명하면 좋은데, 여러분이 랜덤 워크 위에 있다고 하고, 방금 node <code>t</code>로부터 <code>v</code>로 아래 그림처럼 전이(have just transitioned) 되었다.</p><p><img src="https://miro.medium.com/max/1400/1*44_Ys2JeD8B0NVdbJ4TQlg.png" alt="image" /></p><p><code>v</code>에서 다른 인접한 곳으로 이동하는 확률은 <code>edge weight &#42; α</code>(normalized) (<code>α</code>는 하이퍼파라미터에 의존함, 즉, P와 Q에 의해서 결정됨).</p><p>(역주: 위 그래프를 보면 node v에서 t로 돌아갈 확률은 1/p, x1으로 갈 확률은 1, x2, x3는 각각 1/q이다.)</p><p>P는 v를 방문했다가 t로 돌아가는 확률을 통제하고, Q는 방문하지 않은 이웃들을 탐색하는 확률을 통제한다. 직관적인 방식으로 보면, 이거 약간 <code>tSNE</code>에서의 perplexity parameter 같기도 한데, 이것은 우리가 그래프의 로컬/글로벌 구조를 강조할 수 있게 해준다.</p><p>weight도 중요한 사항임을 까먹지 말아야한다. 그래서 최종적으로 travel probability는</p><ol><li>The previous node in the walk (직전에 방문한 노드)</li><li>P and Q</li><li>Edge weight</li></ol><p>의 함수이다.</p><p>이것들이 node2vec의 본질이다. 한번에 다 이해하지 못했으면 한번 더 읽어보기를 강력히 권장한다</p><p>샘플링 전략을 이용해, node2vec은 word2vec에서 사용한 것과 같은 임베딩에 사용되는 sentences(the directed subgraphs)를 만들 것이다.</p><h1>배경지식</h1><h2>Skip-gram</h2><ul><li><a href='skip-gram'>link</a></li></ul><h2>Reference</h2><ul><li><a href='https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef'>blog</a></li><li><a href='http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/'>skip-gram</a></li></ul>
<p>Discuss this post <a href="">here</a>.</p>
<p><i>Published: 2021-07-28</i></p>

<p>
  <i>
  Tagged:
  
  <span class="tag">
    <a href="tags/DeepLearning.html">DeepLearning</a>
  </span>
  
  <span class="tag">
    <a href="tags/MachineLearning.html">MachineLearning</a>
  </span>
  
  </i>
</p>



      
      <div style="margin-bottom: 20px; float: right;">
        <a class="page-link" href="archive.html">Archive</a>
      </div>
      
    </div>
  </body>
</html>
