<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Scala and Spark</title>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link type="application/atom+xml" rel="alternate" href="atom.xml" title="Scala and Spark">
    <link rel="stylesheet" href="style.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/components/prism-clojure.min.js"></script>
    <script type="text/javascript" src="https://livejs.com/live.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.28.0/themes/prism.min.css">



    <!-- Social sharing (Facebook, Twitter, LinkedIn, etc.) -->
    <meta name="title" content="Scala and Spark">
    <meta name="twitter:title" content="Scala and Spark">
    <meta property="og:title" content="Scala and Spark">
    <meta property="og:type" content="website">


    <meta name="twitter:url" content="https://github.com/borkdude/quickblog/2021-05-25-scala_and_spark.html">
    <meta property="og:url" content="https://github.com/borkdude/quickblog/2021-05-25-scala_and_spark.html">


    <meta name="twitter:card" content="summary">



  </head>
  <body>

    <div class="site-header">
      <div class="wrapper">
        <div class="site-nav">
          <a class="page-link" href="archive.html">Archive</a>
          <a class="page-link" href="tags/index.html">Tags</a>
          <a class="page-link" href="tech-radar.html">Tech Radar</a>
	  <!-- <a class="page-link" href="atom.xml">
            Feed
          </a> -->
	  
	  <a class="page-link" href="about.html">About</a>
        </div>
        <div>
          <h1 class="site-title">
            <a class="page-link" href="index.html">Jungwoo Kim</a>
          </h1>
	  <p>Define the problem first</p>
        </div>
      </div>
    </div>

    <div class="wrapper">

      <h1>
  
  Scala and Spark
  
</h1>
<h1 id="intro">Intro</h1><p>스칼라는 객체 지향 언어와 함수형 언어의 특징을 합쳐 놓은 거라고 함. 자바처럼 JVM(자바가상머신) 위에서 동작함. </p><h1 id="%EC%8C%A9_%EA%B8%B0%EB%B3%B8">쌩 기본</h1><h2 id="%3Ccode%3Ehello_world%3C%2Fcode%3E%EB%A5%BC_%EB%9D%84%EC%9B%8C%EB%B3%B4%EC%9E%90"><code>Hello world</code>를 띄워보자</h2><pre class="language-scala"><code class="lang-scala language-scala">// way 1 싱글톤 오브젝트의 main 함수 구현
object HelloWorldObject {
    def main&#40;ars: Array&#91;String&#93;&#41;: Unit = {
        println&#40;&quot;Hello World main&quot;&#41;
    }
}

// way2: app 트레잇 상속
object HelloWorld extends App {
    println&#40;&quot;Hello World&quot;&#41;
}
</code></pre><h2 id="%EA%B0%9D%EC%B2%B4%2C_%EC%9E%90%EB%A3%8C%ED%98%95%2C_%EB%AC%B8%EC%9E%90%EC%97%B4%2C_%EB%B3%80%EC%88%98%2C_%ED%95%A8%EC%88%98%2C_%ED%81%B4%EB%9E%98%EC%8A%A4%2C_%ED%8A%B8%EB%A0%88%EC%9E%87%2C_%EC%8B%B1%EA%B8%80%ED%86%A4_%EA%B0%9D%EC%B2%B4%2C_%EC%BD%9C%EB%A0%89%EC%85%98">객체, 자료형, 문자열, 변수, 함수, 클래스, 트레잇, 싱글톤 객체, 콜렉션</h2><p>스칼라는 기본 자료형, 함수, 클래스 등 모든 것을 객체로 취급함. Any를 최상위로 해서 AnyVal(Double, Float, Long, Int, Short, Byte, Unit, Boolean, Char), AnyRef(List, Option, Class)가 있음.</p><p>객체 비교는 <code>==</code> 아니면 <code>!=</code>를 사용함.</p><p>자료형, 문자열은 문서 찾아보자.</p><p>변수는 재할당 가능한 <code>var</code>와 재할당 불가능한 <code>val</code>이 있음. 가급적 <code>val</code>을 써서 동시성 처리에 유용하도록 하자.</p><p>함수는 <code>def</code>로 선언. 리턴과 리턴 타입은 생략 가능. 파라미터 타입은 생략 불가. 리턴 값이 없으면 <code>Unit</code> 이용하거나 리턴 값이 없으면 마지막 값을 리턴. 함수의 매개변수는 불변 변수(<code>val</code>)이기 때문에 재할당 불가. 리턴 타입 생략하면 자동으로 추론 함.</p><pre class="language-scala"><code class="lang-scala language-scala">// 함수 선언 
def add&#40;x: Int, y: Int&#41;: Int = {
  return x + y
}

// x는 val 이기 때문에 변경 불가 
def add&#40;x: Int&#41;: Int = {
  x = 10 
}

// 리턴 타입 생략 가능 
def add&#40;x: Int, y: Double&#41; = {
  x + y
}

// 리턴 타입이 Unit 타입도 생략 가능 
def add&#40;x: Int, y: Int&#41; = {
  println&#40;x + y&#41;
}

// 리턴 데이터가 없는 경우 Unit을 선언  
def add&#40;x: Int, y: Int&#41;: Unit = {
  println&#40;x + y&#41;
}
</code></pre><p>가변 길이 파라미터는 파라미터 입력시 <code>&#42;</code>를 이용하면 <code>Seq</code>형으로 변환되어 입력.<pre class="language-scala"><code class="lang-scala language-scala">// 여러개의 Int 형을 입력받아서 합계 계산 
def sum&#40;num:Int&#42;&#41; = num.reduce&#40;&#95; + &#95;&#41;

scala&gt; sum&#40;1, 2, 3&#41;
res22: Int = 6

scala&gt; sum&#40;1&#41;
res23: Int = 1
</code></pre></p><p>함수</p><ul><li>람다함수</li></ul><pre class="language-scala"><code class="lang-scala language-scala">// exec는 3개의 파라미터&#40;함수 f, x, y&#41;를 받음
def exec&#40;f: &#40;Int, Int&#41; =&gt; Int, x: Int, y: Int&#41; = f&#40;x, y&#41;

// 람다 함수를 전달하여 처리. x+y 작업을 하는 함수 전달 
scala&gt; exec&#40;&#40;x: Int, y: Int&#41; =&gt; x + y, 2, 3&#41;
res12: Int = 5

// 선언시에 타입을 입력해서 추가적인 설정 없이 처리 가능 
scala&gt; exec&#40;&#40;x, y&#41; =&gt; x + y, 7, 3&#41;
res13: Int = 10

// 함수에 따라 다른 처리 
scala&gt; exec&#40;&#40;x, y&#41; =&gt; x - y, 7, 3&#41;
res14: Int = 4

// 언더바를 이용하여 묵시적인 처리도 가능 
scala&gt; exec&#40;&#95; + &#95;, 3, 1&#41;
res15: Int = 4
</code></pre><ul><li>커링</li><li>클로저</li><li>타입(Generic)</li></ul><pre class="language-scala"><code class="lang-scala language-scala">// for class
import scala.collection.mutable

trait TestStack&#91;T&#93; {
  def pop&#40;&#41;:T
  def push&#40;value:T&#41;
}

class StackSample&#91;T&#93; extends TestStack&#91;T&#93; {

  val stack = new scala.collection.mutable.Stack&#91;T&#93;

  override def pop&#40;&#41;: T = {
    stack.pop&#40;&#41;
  }

  override def push&#40;value:T&#41; = {
    stack.push&#40;value&#41;
  }
}

val s = new StackSample&#91;String&#93;

s.push&#40;&quot;1&quot;&#41;
s.push&#40;&quot;2&quot;&#41;
s.push&#40;&quot;3&quot;&#41;

scala&gt; println&#40;s.pop&#40;&#41;&#41;
3
scala&gt; println&#40;s.pop&#40;&#41;&#41;
2
scala&gt; println&#40;s.pop&#40;&#41;&#41;
1

// for method
def sample&#91;K&#93;&#40;key:K&#41; {
  println&#40;key&#41;
}

def sample2 = sample&#91;String&#93; &#95;

scala&gt; sample2&#40;&quot;Hello&quot;&#41;
Hello
</code></pre><p>트레잇(Trait)</p><ul><li>자바나 nodejs의 interface와 유사함.</li></ul><pre class="language-scala"><code class="lang-scala language-scala">// Machine 트레잇 
trait Machine {
  val serialNumber: Int = 1
  def work&#40;message: String&#41;
}

// KrMachine 트레잇 
trait KrMachine {
  var conturyCode: String = &quot;kr&quot;
  def print&#40;&#41; = println&#40;&quot;한글 출력&quot;&#41;    // krprint 기본 구현 
}

// Computer 클래스는 Machine, KrMachine를 둘다 구현합니다. 
class Computer&#40;location: String&#41; extends Machine with KrMachine {
  this.conturyCode = &quot;us&quot;   // code 값 변경 
  def work&#40;message: String&#41; = println&#40;message&#41;
}

// Car 클래스는 Machine, KrMachine를 둘다 구현합니다.
class Car&#40;location: String&#41; extends Machine with KrMachine {
  def work&#40;message: String&#41; = println&#40;message&#41;
  override def print&#40;&#41; = println&#40;&quot;운전중입니다.&quot;&#41; // print 재정의
}

var machine = new Computer&#40;&quot;노트북&quot;&#41;
var car = new Car&#40;&quot;포르쉐&quot;&#41;

scala&gt; machine.work&#40;&quot;computing...&quot;&#41;
computing...

scala&gt; machine.print&#40;&#41;
한글 출력

scala&gt; println&#40;machine.conturyCode&#41;
us

scala&gt; car.work&#40;&quot;driving...&quot;&#41;
driving...

scala&gt; car.print&#40;&#41; 
운전중입니다.

scala&gt; println&#40;car.conturyCode&#41;
kr
</code></pre><p>클래스 - 그냥 클래스스럽다.</p><h2 id="spark">Spark</h2><p>인메모리 기반의 대용량 데이터 고속 처리 엔진, 범용 분산 클러스터 컴퓨팅 프레임워크. 스파크는 작업을 관리하는 드라이버와, 작업이 실행되는 노드를 관리하는 클러스터 매니저 - 크게 두가지로 구성되어있음.</p><p>스파크 app 구현 방법은 RDD를 이용하는 방법과, Dataset, DataFrame을 이용하는 방법이 있는데, 주로 DataFrame을 이용하는 방법을 알아보자.</p><h3 id="spark_sessin_init">Spark Sessin init</h3><p>Dataset, DataFrame은 스파크 세션을 이용하여 처리 함.</p><pre class="language-scala"><code class="lang-scala language-scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder&#40;&#41;
  .appName&#40;&quot;Spark SQL basic example&quot;&#41;
  .config&#40;&quot;spark.some.config.option&quot;, &quot;some-value&quot;&#41;
  .getOrCreate&#40;&#41;
</code></pre><h3 id="dataframe_init">DataFrame init</h3><p>DataFrame은 스파크세션의 <code>read</code> method로 생성할 수 있음. <code>read</code>는 json, parquet, orc, text 등 다양한 형식의 데이터를 읽을 수 있음. 자세한 건 <a href='read_dfreader_api'>API 문서</a>를 확인.</p><h3 id="dataframe_%EC%97%B0%EC%82%B0">DataFrame 연산</h3><h4 id="%EC%8A%A4%ED%82%A4%EB%A7%88_%ED%99%95%EC%9D%B8">스키마 확인</h4><p><code>printSchema</code>를 이용함.<pre class="language-scala"><code class="lang-scala language-scala">val df = spark.read.json&#40;&quot;/user/people.json&quot;&#41;

// 스키마 출력 
scala&gt; df.printSchema&#40;&#41;
root
 |-- age: long &#40;nullable = true&#41;
 |-- name: string &#40;nullable = true&#41;

scala&gt; df.show&#40;&#41;
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
</code></pre></p><h4 id="%EC%A1%B0%ED%9A%8C">조회</h4><p><code>select</code><pre class="language-scala"><code class="lang-scala language-scala">// name 칼럼만 조회 
scala&gt; df.select&#40;&quot;name&quot;&#41;.show&#40;&#41;
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+

// name, age 순으로 age에 값을 1더하여 조회 
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot; + 1&#41;.show&#40;&#41;
+-------+---------+
|   name|&#40;age + 1&#41;|
+-------+---------+
|Michael|     null|
|   Andy|       31|
| Justin|       20|
+-------+---------+
</code></pre></p><h4 id="show%28%29_%ED%95%A8%EC%88%98_%EC%84%A4%EC%A0%95">show() 함수 설정</h4><p>데이터의 길이와 칼럼의 사이즈를 제한하여 출력할 수 있음<pre class="language-scala"><code class="lang-scala language-scala">// show 함수 선언 
def show&#40;numRows: Int, truncate: Boolean&#41;: Unit = println&#40;showString&#40;numRows, truncate&#41;&#41;

// 사용방법 
scala&gt; show&#40;10, false&#41;
scala&gt; show&#40;100, true&#41;
</code></pre></p><h4 id="%ED%95%84%ED%84%B0%EB%A7%81">필터링</h4><p><code>filter</code><pre class="language-scala"><code class="lang-scala language-scala">// 필터링 처리 
scala&gt; df.filter&#40;$&quot;age&quot; &gt; 21&#41;.show&#40;&#41;
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

// select에 filter 조건 추가 
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot;&#41;.filter&#40;$&quot;age&quot; &gt; 20&#41;.show&#40;&#41;
scala&gt; df.select&#40;$&quot;name&quot;, $&quot;age&quot;&#41;.filter&#40;&quot;age &gt; 20&quot;&#41;.show&#40;&#41;
+----+---+
|name|age|
+----+---+
|Andy| 30|
+----+---+
</code></pre></p><h4 id="%EA%B7%B8%EB%A3%B9%ED%95%91">그룹핑</h4><p><code>groupBy</code><pre class="language-scala"><code class="lang-scala language-scala">// 그룹핑 처리 
scala&gt; df.groupBy&#40;&quot;age&quot;&#41;.count&#40;&#41;.show&#40;&#41;
+----+-----+
| age|count|
+----+-----+
|  19|    1|
|null|    1|
|  30|    1|
+----+-----+
</code></pre></p><h4 id="%EC%B9%BC%EB%9F%BC_%EC%B6%94%EA%B0%80">칼럼 추가</h4><p>새로운 칼럼을 추가할 때는 <code>withColumn</code>을 이용<pre class="language-scala"><code class="lang-scala language-scala">// age가 NULL일 때는 KKK, 값이 있을 때는 TTT를 출력 
scala&gt; df.withColumn&#40;&quot;xx&quot;, when&#40;$&quot;age&quot;.isNull, &quot;KKK&quot;&#41;.otherwise&#40;&quot;TTT&quot;&#41;&#41;.show&#40;&#41;
+----+-------+---+
| age|   name| xx|
+----+-------+---+
|null|Michael|KKK|
|  30|   Andy|TTT|
|  19| Justin|TTT|
+----+-------+---+
</code></pre></p><h4 id="dataframe%EC%9D%98_sql%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A1%B0%ED%9A%8C">DataFrame의 SQL을 이용한 데이터 조회</h4><p>데이터프레임은 SQL 쿼리를 이용해 데이터를 조회할 수 있음. 데이터프레임을 이용하여 뷰를 생성하고 SQL쿼리를 실행하면 됨.</p><pre class="language-scala"><code class="lang-scala language-scala">// 뷰생성
val df = spark.read.json&#40;&quot;/user/people.json&quot;&#41;

// DataFrame으로 뷰를 생성 
df.createOrReplaceTempView&#40;&quot;people&quot;&#41;

// 스파크세션을 이용하여 SQL 쿼리 작성 
scala&gt; spark.sql&#40;&quot;SELECT &#42; FROM people&quot;&#41;.show&#40;&#41;
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+

// SQL 사용
// 조회 조건 추가 
scala&gt; spark.sql&#40;&quot;SELECT &#42; FROM people WHERE age &gt; 20&quot;&#41;.show&#40;&#41;
+---+----+
|age|name|
+---+----+
| 30|Andy|
+---+----+

// 그룹핑 추가 
scala&gt; spark.sql&#40;&quot;SELECT age, count&#40;1&#41; FROM people GROUP BY age&quot;&#41;.show&#40;&#41;
+----+--------+
| age|count&#40;1&#41;|
+----+--------+
|  19|       1|
|null|       1|
|  30|       1|
+----+--------+
</code></pre><h3 id="dataframe_%EC%A0%80%EC%9E%A5%2F%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0">DataFrame 저장/불러오기</h3><p>Dataset이랑 DataFrame이랑 같음.</p><h4 id="%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%80%EC%9E%A5">데이터 저장</h4><p><code>save</code> 이용<pre class="language-scala"><code class="lang-scala language-scala">val peopleDF = spark.read.json&#40;&quot;/user/people.json&quot;&#41;
case class People&#40;name: String, age: Long&#41;
val peopleDS = peopleDF.as&#91;People&#93;


peopleDS.write.save&#40;&quot;/user/ds&quot;&#41;
peopleDF.write.save&#40;&quot;/user/df&quot;&#41;
</code></pre></p><h4 id="%EC%A0%80%EC%9E%A5_%ED%8F%AC%EB%A7%B7_%EC%A7%80%EC%A0%95">저장 포맷 지정</h4><p><code>format</code>을 이용해 json, csv 등 포맷을 설정할 수 있음<pre class="language-scala"><code class="lang-scala language-scala">// 데이터 저장 
peopleDS.select&#40;&quot;name&quot;&#41;.write.format&#40;&quot;json&quot;&#41;.save&#40;&quot;/user/ds&#95;1&quot;&#41;

// 저장 위치 확인 
$ hadoop fs -ls /user/ds&#95;1/
Found 2 items
-rw-r--r--   2 hadoop hadoop          0 2019-01-24 07:19 /user/ds&#95;1/&#95;SUCCESS
-rw-r--r--   2 hadoop hadoop         53 2019-01-24 07:19 /user/ds&#95;1/part-r-00000-88b715ad-1b5b-480c-8e17-7b0c0ea93e9f.json

// 저장 형식 확인 
$ hadoop fs -text /user/ds&#95;1/part-r-00000-88b715ad-1b5b-480c-8e17-7b0c0ea93e9f.json
{&quot;name&quot;:&quot;Michael&quot;}
{&quot;name&quot;:&quot;Andy&quot;}
{&quot;name&quot;:&quot;Justin&quot;}
</code></pre></p><h4 id="%EC%95%95%EC%B6%95_%ED%8F%AC%EB%A7%B7_%EC%A7%80%EC%A0%95">압축 포맷 지정</h4><p><code>option</code>을 이용하여 압축 포맷을 지정할 수 있음. gzip, snappy 등의 형식을 이용할 수 있음. <pre class="language-scala"><code class="lang-scala language-scala">// snappy 형식 압축 
peopleDS.select&#40;&quot;name&quot;&#41;.write.format&#40;&quot;json&quot;&#41;.option&#40;&quot;compression&quot;, &quot;snappy&quot;&#41;.save&#40;&quot;/user/ds&#95;1&quot;&#41;

// 저장 모드, SaveMode.ErrorIfExists, SaveMode.Append, SaveMode.Overwrite, SaveMode.Ignore 등의 모드가 있음
// SaveMode 사용을 위해 import
import org.apache.spark.sql.&#95;

val peopleDF = spark.read.json&#40;&quot;/user/people.json&quot;&#41;
peopleDF.select&#40;&quot;name&quot;, &quot;age&quot;&#41;.write.mode&#40;SaveMode.Overwrite&#41;.save&#40;&quot;/user/people/&quot;&#41;
</code></pre></p><h4 id="%ED%85%8C%EC%9D%B4%EB%B8%94_%EC%A0%80%EC%9E%A5">테이블 저장</h4><p><code>saveAsTable</code>을 이용합니다.<pre class="language-scala"><code class="lang-scala language-scala">peopleDF.select&#40;&quot;name&quot;, &quot;age&quot;&#41;.write.saveAsTable&#40;&quot;people&quot;&#41;
</code></pre></p><h4 id="%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0">데이터 불러오기</h4><pre class="language-scala"><code class="lang-scala language-scala">val peopleDF = spark.read.json&#40;&quot;/user/ds&#95;1/&quot;&#41;
scala&gt; peopleDF.show&#40;&#41;
+-------+
|   name|
+-------+
|Michael|
|   Andy|
| Justin|
+-------+
</code></pre><h2 id="spark_sql_and_hive">Spark SQL and Hive</h2><ul><li>Spark SQL</li><li>UDF(User Defined Function)</li></ul><h2 id="reference">Reference</h2><ul><li><a href='https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/sql/DataFrameReader.html'>read_dfreader_api</a></li><li><a href='https://wikidocs.net/book/2350'>book</a></li></ul>
<p>Discuss this post <a href="https://github.com/jungwookim/jungwookim.github.io/discussions">here</a>.</p>
<p><i>Published: 2021-05-25</i></p>

<p>
  <i>
  Tagged:
  
  <span class="tag">
    <a href="tags/Spark.html">Spark</a>
  </span>
  
  <span class="tag">
    <a href="tags/Scala.html">Scala</a>
  </span>
  
  </i>
</p>
<p>
  <script
    src="https://utteranc.es/client.js"
    repo="jungwookim/jungwookim.github.io"
    issue-term="pathname"
    theme="github-light"
    crossorigin="anonymous"
    async>
  </script>
</p>



      
      <div style="margin-bottom: 20px; float: right;">
        <a class="page-link" href="archive.html">Archive</a>
      </div>
      
    </div>
  </body>
</html>
